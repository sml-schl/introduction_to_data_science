{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Introduction to Machine Learning\n",
    "\n",
    "**Duration:** 90 minutes  \n",
    "**Dataset:** Titanic Passenger Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Define machine learning and understand it as an optimization task\n",
    "- Differentiate supervised, unsupervised, and reinforcement learning\n",
    "- Apply train-test split and cross-validation\n",
    "- Understand classification tasks (K-NN, Decision Trees, Logistic Regression)\n",
    "- Understand regression for continuous predictions\n",
    "- Apply K-means clustering\n",
    "- Understand neural network basics (layers, activation functions, hyperparameters)\n",
    "- Understand loss functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading (5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, \n",
    "    mean_squared_error, r2_score, silhouette_score\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: What is Machine Learning? (10 mins)\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Machine Learning** is a subset of artificial intelligence that enables computers to learn patterns from data without being explicitly programmed.\n",
    "\n",
    "### Machine Learning as an Optimization Task\n",
    "\n",
    "At its core, machine learning is about **optimization**:\n",
    "1. Define a **model** (function) with parameters\n",
    "2. Define a **loss function** (measures how wrong predictions are)\n",
    "3. Find parameters that **minimize the loss** (optimization)\n",
    "\n",
    "```\n",
    "minimize: Loss(predictions, actual_values)\n",
    "by adjusting: model parameters\n",
    "```\n",
    "\n",
    "### Three Types of Machine Learning\n",
    "\n",
    "#### 1. Supervised Learning\n",
    "- **Learn from labeled data** (we have both input features and correct answers)\n",
    "- **Goal:** Predict output for new, unseen inputs\n",
    "- **Examples:**\n",
    "  - Classification: Predict categories (survived/died, spam/not spam)\n",
    "  - Regression: Predict continuous values (house price, temperature)\n",
    "\n",
    "#### 2. Unsupervised Learning\n",
    "- **Learn from unlabeled data** (we only have input features, no correct answers)\n",
    "- **Goal:** Discover hidden patterns or structure in data\n",
    "- **Examples:**\n",
    "  - Clustering: Group similar items together\n",
    "  - Dimensionality reduction: Compress data while preserving important information\n",
    "\n",
    "#### 3. Reinforcement Learning\n",
    "- **Learn by trial and error** through interaction with an environment\n",
    "- **Goal:** Maximize cumulative reward\n",
    "- **Examples:** Game playing (AlphaGo), robotics, self-driving cars\n",
    "- *Note: We won't cover this today*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Identify the Learning Type\n",
    "\n",
    "For each scenario, identify whether it's **supervised** or **unsupervised** learning:\n",
    "\n",
    "1. Predicting whether a Titanic passenger survived (we have survival labels): _______________\n",
    "2. Grouping passengers into clusters based on age and fare (no labels): _______________\n",
    "3. Predicting the fare a passenger paid based on their class and age: _______________\n",
    "4. Finding natural groups in customer purchase behavior: _______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Data Preparation for Machine Learning (10 mins)\n",
    "\n",
    "Before we can train models, we need to prepare our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean copy for machine learning\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df_ml.isnull().sum())\n",
    "print(\"\\nTotal missing:\", df_ml.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Handle missing values\n",
    "# Fill missing 'age' with median age\n",
    "df_ml['age'] = df_ml['age'].fillna(df_ml['age'].median())\n",
    "\n",
    "# TODO: Fill missing 'embarked' with most common value (mode)\n",
    "df_ml['embarked'] = # YOUR CODE HERE (use fillna with mode)\n",
    "\n",
    "# TODO: Fill missing 'fare' with median fare\n",
    "df_ml['fare'] = # YOUR CODE HERE\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_ml[['age', 'embarked', 'fare']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "# TODO: Convert 'sex' to binary: 1 for male, 0 for female\n",
    "df_ml['sex_encoded'] = # YOUR CODE HERE (use map: {'male': 1, 'female': 0})\n",
    "\n",
    "# One-hot encode 'embarked'\n",
    "embarked_dummies = pd.get_dummies(df_ml['embarked'], prefix='embarked', drop_first=True)\n",
    "df_ml = pd.concat([df_ml, embarked_dummies], axis=1)\n",
    "\n",
    "print(\"\\nEncoded features:\")\n",
    "print(df_ml[['sex', 'sex_encoded', 'embarked', 'embarked_Q', 'embarked_S']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: create family_size\n",
    "# TODO: Create family_size = siblings/spouses + parents/children + 1 (the passenger)\n",
    "df_ml['family_size'] = # YOUR CODE HERE (sibsp + parch + 1)\n",
    "\n",
    "print(f\"Family size range: {df_ml['family_size'].min()} to {df_ml['family_size'].max()}\")\n",
    "print(f\"Average family size: {df_ml['family_size'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Train-Test Split (10 mins)\n",
    "\n",
    "### Why Split Data?\n",
    "\n",
    "We split data into **training** and **testing** sets to:\n",
    "1. **Train** the model on one portion of data\n",
    "2. **Evaluate** the model on unseen data (testing set)\n",
    "3. **Detect overfitting**: When a model memorizes training data but fails on new data\n",
    "\n",
    "```\n",
    "Original Data (100%)\n",
    "    |\n",
    "    ├── Training Set (70-80%): Used to train the model\n",
    "    └── Test Set (20-30%): Used to evaluate the model\n",
    "```\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "- **Training Set**: Data used to train (fit) the model\n",
    "- **Test Set**: Data used to evaluate model performance (simulates real-world predictions)\n",
    "- **Never** use test data for training!\n",
    "- Common split ratios: 80/20, 70/30, 75/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for our first model\n",
    "feature_columns = ['pclass', 'sex_encoded', 'age', 'fare', 'family_size', \n",
    "                   'embarked_Q', 'embarked_S']\n",
    "\n",
    "# TODO: Create X (features) and y (target)\n",
    "X = # YOUR CODE HERE (select feature_columns from df_ml)\n",
    "y = # YOUR CODE HERE (select 'survived' from df_ml)\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into training and testing sets (80/20 split)\n",
    "# Use random_state=42 for reproducibility\n",
    "X_train, X_test, y_train, y_test = # YOUR CODE HERE (use train_test_split)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining set survival rate: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"Test set survival rate: {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Many ML algorithms perform better when features are on the same scale.\n",
    "\n",
    "**Important**: Fit scaler on training data only, then transform both train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_scaled = # YOUR CODE HERE (fit_transform on X_train)\n",
    "X_test_scaled = # YOUR CODE HERE (transform on X_test - don't fit!)\n",
    "\n",
    "print(\"Original training data (first 3 rows):\")\n",
    "print(X_train.head(3))\n",
    "print(\"\\nScaled training data (first 3 rows):\")\n",
    "print(X_train_scaled[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Classification - Predicting Survival (25 mins)\n",
    "\n",
    "### What is Classification?\n",
    "\n",
    "**Classification** is a supervised learning task where we predict **discrete categories** (classes).\n",
    "\n",
    "- **Binary Classification**: 2 classes (e.g., survived/died, spam/not spam)\n",
    "- **Multi-class Classification**: 3+ classes (e.g., passenger class 1/2/3)\n",
    "\n",
    "Today we'll explore three popular classification algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: K-Nearest Neighbors (K-NN)\n",
    "\n",
    "**How it works:**\n",
    "1. Find the K nearest data points to the new observation\n",
    "2. Take a \"vote\" among those K neighbors\n",
    "3. Assign the most common class\n",
    "\n",
    "**Analogy:** \"You are the average of your 5 closest friends\"\n",
    "\n",
    "**Key hyperparameter:** K (number of neighbors)\n",
    "- Small K: More sensitive to noise (overfitting)\n",
    "- Large K: Smoother decision boundary (underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a K-NN classifier with K=5\n",
    "knn = # YOUR CODE HERE (create KNeighborsClassifier with n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"K-NN Accuracy: {accuracy_knn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different K values\n",
    "k_values = [1, 3, 5, 7, 9, 15, 25, 50]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    # YOUR CODE HERE\n",
    "    # Train KNN with n_neighbors=k\n",
    "    # Predict on test set\n",
    "    # Calculate accuracy and append to accuracies list\n",
    "    pass\n",
    "\n",
    "# Plot results\n",
    "fig = px.line(x=k_values, y=accuracies, markers=True,\n",
    "              title='K-NN Performance vs K Value',\n",
    "              labels={'x': 'K (Number of Neighbors)', 'y': 'Accuracy'})\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()\n",
    "\n",
    "print(f\"Best K value: {k_values[np.argmax(accuracies)]} with accuracy: {max(accuracies)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Decision Trees\n",
    "\n",
    "**How it works:**\n",
    "1. Split data based on features to create \"decision rules\"\n",
    "2. Keep splitting until reaching pure leaf nodes or stopping criteria\n",
    "3. Make predictions by following the decision path\n",
    "\n",
    "**Analogy:** A flowchart of yes/no questions\n",
    "\n",
    "```\n",
    "Is sex == female?\n",
    "  ├─ Yes → Is pclass <= 2?\n",
    "  │         ├─ Yes → Survived\n",
    "  │         └─ No → Check age...\n",
    "  └─ No → Is age < 16?\n",
    "            ├─ Yes → Survived\n",
    "            └─ No → Died\n",
    "```\n",
    "\n",
    "**Pros:** Easy to interpret, handles non-linear relationships  \n",
    "**Cons:** Can overfit easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = # YOUR CODE HERE\n",
    "\n",
    "# Evaluate\n",
    "accuracy_dt = # YOUR CODE HERE (calculate accuracy)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt, \n",
    "          feature_names=feature_columns,\n",
    "          class_names=['Died', 'Survived'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree for Titanic Survival Prediction', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': dt.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig = px.bar(feature_importance, x='importance', y='feature', orientation='h',\n",
    "             title='Feature Importance in Decision Tree')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nMost important features:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Logistic Regression\n",
    "\n",
    "**How it works:**\n",
    "1. Creates a linear combination of features\n",
    "2. Applies sigmoid function to get probability (0 to 1)\n",
    "3. Classifies based on threshold (typically 0.5)\n",
    "\n",
    "**Formula:** `P(survived=1) = 1 / (1 + e^(-z))` where `z = w₁x₁ + w₂x₂ + ... + b`\n",
    "\n",
    "**Note:** Despite the name, it's a **classification** algorithm, not regression!\n",
    "\n",
    "**Pros:** Fast, interpretable, provides probabilities  \n",
    "**Cons:** Assumes linear decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = # YOUR CODE HERE\n",
    "\n",
    "# Get probability predictions\n",
    "y_pred_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]  # Probability of survival\n",
    "\n",
    "# Evaluate\n",
    "accuracy_lr = # YOUR CODE HERE\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr*100:.2f}%\")\n",
    "print(f\"\\nFirst 5 probability predictions: {y_pred_proba_lr[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model coefficients (weights)\n",
    "coefficients = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'coefficient': lr.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "fig = px.bar(coefficients, x='coefficient', y='feature', orientation='h',\n",
    "             title='Logistic Regression Coefficients',\n",
    "             color='coefficient',\n",
    "             color_continuous_scale='RdBu_r')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nFeature coefficients:\")\n",
    "print(coefficients)\n",
    "print(\"\\nPositive coefficient = increases survival probability\")\n",
    "print(\"Negative coefficient = decreases survival probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4: Comparing Classification Models\n",
    "\n",
    "Let's compare all three models using multiple metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate metrics for all models\n",
    "models = {\n",
    "    'K-NN': y_pred_knn,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'Logistic Regression': y_pred_lr\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, y_pred in models.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "results_melted = results_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "fig = px.bar(results_melted, x='Model', y='Score', color='Metric', barmode='group',\n",
    "             title='Classification Model Comparison')\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Classification Metrics\n",
    "\n",
    "- **Accuracy**: % of correct predictions (both survived and died)\n",
    "- **Precision**: Of all predicted survivors, what % actually survived? (avoid false alarms)\n",
    "- **Recall**: Of all actual survivors, what % did we predict correctly? (avoid missing survivors)\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model (Logistic Regression)\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "fig = px.imshow(cm, \n",
    "                labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "                x=['Died', 'Survived'],\n",
    "                y=['Died', 'Survived'],\n",
    "                text_auto=True,\n",
    "                title='Confusion Matrix - Logistic Regression',\n",
    "                color_continuous_scale='Blues')\n",
    "fig.show()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"True Negatives (correctly predicted died): {cm[0, 0]}\")\n",
    "print(f\"False Positives (incorrectly predicted survived): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (incorrectly predicted died): {cm[1, 0]}\")\n",
    "print(f\"True Positives (correctly predicted survived): {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5: Support Vector Machines (SVM) - Conceptual Overview\n",
    "\n",
    "**How it works:**\n",
    "1. Find the hyperplane (decision boundary) that best separates classes\n",
    "2. Maximize the margin (distance) between the boundary and nearest points\n",
    "3. Support vectors are the critical points closest to the boundary\n",
    "\n",
    "**Key concept:** The \"kernel trick\" allows SVM to handle non-linear boundaries\n",
    "\n",
    "**Pros:** Effective in high dimensions, memory efficient  \n",
    "**Cons:** Slow for large datasets, requires feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train an SVM classifier (optional - can be slow)\n",
    "# Use a smaller dataset for demonstration\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_svm*100:.2f}%\")\n",
    "print(f\"Number of support vectors: {len(svm.support_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Cross-Validation (8 mins)\n",
    "\n",
    "### The Problem with Single Train-Test Split\n",
    "\n",
    "A single split might not be representative. What if test set is too easy or too hard?\n",
    "\n",
    "### Solution: K-Fold Cross-Validation\n",
    "\n",
    "1. Split data into K folds (e.g., 5 folds)\n",
    "2. Train K times, each time using a different fold as test set\n",
    "3. Average the results to get more reliable estimate\n",
    "\n",
    "```\n",
    "Fold 1: [Test][Train][Train][Train][Train]\n",
    "Fold 2: [Train][Test][Train][Train][Train]\n",
    "Fold 3: [Train][Train][Test][Train][Train]\n",
    "Fold 4: [Train][Train][Train][Test][Train]\n",
    "Fold 5: [Train][Train][Train][Train][Test]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform 5-fold cross-validation on Logistic Regression\n",
    "lr_cv = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use cross_val_score with cv=5\n",
    "cv_scores = cross_val_score(lr_cv, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-Validation Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i}: {score*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nMean CV Score: {cv_scores.mean()*100:.2f}%\")\n",
    "print(f\"Standard Deviation: {cv_scores.std()*100:.2f}%\")\n",
    "print(f\"95% Confidence Interval: [{(cv_scores.mean() - 2*cv_scores.std())*100:.2f}%, {(cv_scores.mean() + 2*cv_scores.std())*100:.2f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare CV scores for all models\n",
    "models_cv = {\n",
    "    'K-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=4, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "for model_name, model in models_cv.items():\n",
    "    # YOUR CODE HERE: perform cross-validation\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'Model': model_name,\n",
    "        'Mean CV Score': scores.mean(),\n",
    "        'Std Dev': scores.std()\n",
    "    })\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\nCross-Validation Comparison:\")\n",
    "print(cv_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Regression - Predicting Continuous Values (12 mins)\n",
    "\n",
    "### What is Regression?\n",
    "\n",
    "**Regression** is a supervised learning task where we predict **continuous numerical values**.\n",
    "\n",
    "Examples:\n",
    "- Predicting house prices\n",
    "- Forecasting temperature\n",
    "- Estimating ticket fare\n",
    "\n",
    "Today we'll predict the **fare** a passenger paid based on their characteristics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "# TODO: Create features (X) and target (y) for fare prediction\n",
    "regression_features = ['pclass', 'sex_encoded', 'age', 'family_size', \n",
    "                       'embarked_Q', 'embarked_S']\n",
    "\n",
    "X_reg = df_ml[regression_features]\n",
    "y_reg = df_ml['fare']  # Target is now 'fare' (continuous)\n",
    "\n",
    "print(f\"Target (fare) statistics:\")\n",
    "print(y_reg.describe())\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "**How it works:**\n",
    "- Finds the best-fit line (or hyperplane) through the data\n",
    "- Minimizes the sum of squared errors (distance from points to line)\n",
    "\n",
    "**Formula:** `y = w₁x₁ + w₂x₂ + ... + b`\n",
    "\n",
    "Where:\n",
    "- `y` = predicted fare\n",
    "- `x₁, x₂, ...` = features (pclass, age, etc.)\n",
    "- `w₁, w₂, ...` = weights (coefficients)\n",
    "- `b` = bias (intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = lin_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Evaluate using regression metrics\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"\\nInterpretation: On average, predictions are off by £{rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Regression Metrics\n",
    "\n",
    "- **MSE (Mean Squared Error)**: Average squared difference between predictions and actual values\n",
    "  - Lower is better\n",
    "  - Units are squared (hard to interpret)\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**: Square root of MSE\n",
    "  - Lower is better\n",
    "  - Same units as target variable (easier to interpret)\n",
    "\n",
    "- **R² (R-squared)**: Proportion of variance explained by the model\n",
    "  - Range: 0 to 1 (sometimes negative for very bad models)\n",
    "  - 1.0 = perfect predictions\n",
    "  - 0.0 = model is no better than predicting the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig = go.Figure()\n",
    "\n",
    "# Perfect predictions line\n",
    "max_fare = max(y_test_reg.max(), y_pred_reg.max())\n",
    "fig.add_trace(go.Scatter(x=[0, max_fare], y=[0, max_fare], \n",
    "                         mode='lines', name='Perfect Prediction',\n",
    "                         line=dict(color='red', dash='dash')))\n",
    "\n",
    "# Actual predictions\n",
    "fig.add_trace(go.Scatter(x=y_test_reg, y=y_pred_reg, \n",
    "                         mode='markers', name='Predictions',\n",
    "                         marker=dict(size=8, opacity=0.6)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Predicted vs Actual Fare (R² = {r2:.3f})',\n",
    "    xaxis_title='Actual Fare (£)',\n",
    "    yaxis_title='Predicted Fare (£)',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coefficients for regression\n",
    "reg_coefficients = pd.DataFrame({\n",
    "    'feature': regression_features,\n",
    "    'coefficient': lin_reg.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "fig = px.bar(reg_coefficients, x='coefficient', y='feature', orientation='h',\n",
    "             title='Linear Regression Coefficients for Fare Prediction',\n",
    "             color='coefficient',\n",
    "             color_continuous_scale='RdBu_r')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nFeature coefficients:\")\n",
    "print(reg_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Unsupervised Learning - K-Means Clustering (10 mins)\n",
    "\n",
    "### What is Clustering?\n",
    "\n",
    "**Clustering** is an unsupervised learning task where we group similar data points together **without labels**.\n",
    "\n",
    "Use cases:\n",
    "- Customer segmentation\n",
    "- Anomaly detection\n",
    "- Data exploration\n",
    "\n",
    "### K-Means Algorithm\n",
    "\n",
    "**How it works:**\n",
    "1. Choose K (number of clusters)\n",
    "2. Randomly initialize K cluster centers\n",
    "3. Assign each point to nearest center\n",
    "4. Update centers to mean of assigned points\n",
    "5. Repeat steps 3-4 until convergence\n",
    "\n",
    "**Key hyperparameter:** K (number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "# TODO: Select features for clustering (age and fare)\n",
    "cluster_features = ['age', 'fare', 'pclass']\n",
    "X_cluster = df_ml[cluster_features]\n",
    "\n",
    "# Scale features (important for K-Means!)\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "print(f\"Clustering data shape: {X_cluster_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply K-Means with K=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_ml['cluster'] = clusters\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "print(df_ml['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Cluster characteristics\n",
    "print(\"\\nCluster characteristics:\")\n",
    "print(df_ml.groupby('cluster')[cluster_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D (Age vs Fare)\n",
    "fig = px.scatter(df_ml, x='age', y='fare', color='cluster',\n",
    "                 title='K-Means Clustering of Titanic Passengers',\n",
    "                 labels={'cluster': 'Cluster'},\n",
    "                 hover_data=['pclass', 'sex'])\n",
    "\n",
    "# Add cluster centers\n",
    "centers = scaler_cluster.inverse_transform(kmeans.cluster_centers_)\n",
    "fig.add_trace(go.Scatter(x=centers[:, 0], y=centers[:, 1],\n",
    "                         mode='markers',\n",
    "                         marker=dict(symbol='x', size=15, color='black', line=dict(width=2)),\n",
    "                         name='Cluster Centers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using the Elbow Method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    # YOUR CODE HERE\n",
    "    # Fit K-Means with k clusters\n",
    "    # Store inertia_ and silhouette_score\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_cluster_scaled)\n",
    "    \n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans_temp.labels_))\n",
    "\n",
    "# Plot elbow curve\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(K_range), y=inertias, mode='lines+markers',\n",
    "                         name='Inertia'))\n",
    "fig.update_layout(title='Elbow Method for Optimal K',\n",
    "                  xaxis_title='Number of Clusters (K)',\n",
    "                  yaxis_title='Inertia (Within-Cluster Sum of Squares)')\n",
    "fig.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "fig2 = px.line(x=list(K_range), y=silhouette_scores, markers=True,\n",
    "               title='Silhouette Score vs K',\n",
    "               labels={'x': 'Number of Clusters (K)', 'y': 'Silhouette Score'})\n",
    "fig2.show()\n",
    "\n",
    "print(f\"Best K based on silhouette score: {K_range[np.argmax(silhouette_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Evaluation Metrics\n",
    "\n",
    "- **Inertia**: Sum of squared distances to nearest cluster center\n",
    "  - Lower is better\n",
    "  - Always decreases as K increases\n",
    "  - Look for \"elbow\" in the curve\n",
    "\n",
    "- **Silhouette Score**: Measures how similar points are to their own cluster vs other clusters\n",
    "  - Range: -1 to 1\n",
    "  - Higher is better\n",
    "  - >0.5 = good clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Introduction to Neural Networks (10 mins)\n",
    "\n",
    "### What are Neural Networks?\n",
    "\n",
    "**Neural networks** are computing systems inspired by biological neural networks in animal brains.\n",
    "\n",
    "### Basic Structure\n",
    "\n",
    "```\n",
    "Input Layer → Hidden Layer(s) → Output Layer\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Neurons (Nodes)**: Processing units that receive inputs, apply weights, and produce outputs\n",
    "2. **Layers**: Groups of neurons\n",
    "   - Input Layer: Receives raw data\n",
    "   - Hidden Layer(s): Performs computations\n",
    "   - Output Layer: Produces final prediction\n",
    "3. **Weights**: Learned parameters that connect neurons\n",
    "4. **Biases**: Offset values for each neuron\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "**Activation functions** introduce non-linearity, allowing networks to learn complex patterns.\n",
    "\n",
    "**Common activation functions:**\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**\n",
    "   - Formula: `f(x) = max(0, x)`\n",
    "   - Most popular for hidden layers\n",
    "   - Fast and effective\n",
    "\n",
    "2. **Sigmoid**\n",
    "   - Formula: `f(x) = 1 / (1 + e^(-x))`\n",
    "   - Output: 0 to 1\n",
    "   - Used for binary classification output\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**\n",
    "   - Formula: `f(x) = (e^x - e^(-x)) / (e^x + e^(-x))`\n",
    "   - Output: -1 to 1\n",
    "   - Similar to sigmoid but centered at 0\n",
    "\n",
    "4. **Softmax**\n",
    "   - Converts outputs to probabilities (sum to 1)\n",
    "   - Used for multi-class classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Tanh\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=relu, mode='lines', name='ReLU'))\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid, mode='lines', name='Sigmoid'))\n",
    "fig.add_trace(go.Scatter(x=x, y=tanh, mode='lines', name='Tanh'))\n",
    "\n",
    "fig.update_layout(title='Common Activation Functions',\n",
    "                  xaxis_title='Input',\n",
    "                  yaxis_title='Output',\n",
    "                  showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Hyperparameters\n",
    "\n",
    "**Hyperparameters** are settings we choose before training (not learned from data).\n",
    "\n",
    "**Key hyperparameters:**\n",
    "\n",
    "1. **Number of layers**: How deep is the network?\n",
    "   - More layers = can learn more complex patterns\n",
    "   - But: harder to train, risk of overfitting\n",
    "\n",
    "2. **Number of neurons per layer**: How wide is each layer?\n",
    "   - More neurons = more capacity to learn\n",
    "   - But: more computation, risk of overfitting\n",
    "\n",
    "3. **Learning rate**: How big are the update steps during training?\n",
    "   - Too high: training unstable, might not converge\n",
    "   - Too low: training very slow\n",
    "   - Typical values: 0.001 to 0.1\n",
    "\n",
    "4. **Batch size**: How many examples to process before updating weights?\n",
    "   - Small batch: noisy updates, but more frequent\n",
    "   - Large batch: stable updates, but less frequent\n",
    "   - Common values: 16, 32, 64, 128\n",
    "\n",
    "5. **Epochs**: How many times to go through entire dataset?\n",
    "   - More epochs = more training\n",
    "   - But: risk of overfitting if too many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Neural Network Architecture\n",
    "\n",
    "For Titanic survival prediction:\n",
    "\n",
    "```\n",
    "Input Layer (7 neurons)\n",
    "    ↓\n",
    "Hidden Layer 1 (16 neurons, ReLU activation)\n",
    "    ↓\n",
    "Hidden Layer 2 (8 neurons, ReLU activation)\n",
    "    ↓\n",
    "Output Layer (1 neuron, Sigmoid activation)\n",
    "```\n",
    "\n",
    "**Total parameters to learn:**\n",
    "- Layer 1: 7 × 16 + 16 = 128\n",
    "- Layer 2: 16 × 8 + 8 = 136\n",
    "- Output: 8 × 1 + 1 = 9\n",
    "- **Total: 273 parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation (Conceptual)\n",
    "\n",
    "How a neural network makes a prediction:\n",
    "\n",
    "1. **Input**: Start with features (age, sex, pclass, etc.)\n",
    "2. **Layer 1**: \n",
    "   - Multiply inputs by weights\n",
    "   - Add biases\n",
    "   - Apply activation function (ReLU)\n",
    "3. **Layer 2**: \n",
    "   - Multiply layer 1 outputs by weights\n",
    "   - Add biases\n",
    "   - Apply activation function (ReLU)\n",
    "4. **Output Layer**: \n",
    "   - Multiply layer 2 outputs by weights\n",
    "   - Add bias\n",
    "   - Apply activation function (Sigmoid)\n",
    "   - Get probability of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "**Loss function** measures how wrong our predictions are. Goal: minimize loss!\n",
    "\n",
    "#### For Classification:\n",
    "\n",
    "**1. Binary Cross-Entropy Loss** (for binary classification)\n",
    "- Formula: `-[y log(ŷ) + (1-y) log(1-ŷ)]`\n",
    "- Where y = actual (0 or 1), ŷ = predicted probability\n",
    "- Penalizes confident wrong predictions heavily\n",
    "\n",
    "**2. Categorical Cross-Entropy Loss** (for multi-class)\n",
    "- Extension of binary cross-entropy for multiple classes\n",
    "\n",
    "#### For Regression:\n",
    "\n",
    "**1. Mean Squared Error (MSE)**\n",
    "- Formula: `average of (actual - predicted)²`\n",
    "- Penalizes large errors more heavily\n",
    "\n",
    "**2. Mean Absolute Error (MAE)**\n",
    "- Formula: `average of |actual - predicted|`\n",
    "- Less sensitive to outliers than MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Binary Cross-Entropy Loss\n",
    "# When actual value is 1 (survived)\n",
    "y_pred_prob = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Loss when actual = 1\n",
    "loss_when_1 = -np.log(y_pred_prob)\n",
    "\n",
    "# Loss when actual = 0\n",
    "loss_when_0 = -np.log(1 - y_pred_prob)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y_pred_prob, y=loss_when_1, \n",
    "                         mode='lines', name='Actual = 1 (Survived)'))\n",
    "fig.add_trace(go.Scatter(x=y_pred_prob, y=loss_when_0, \n",
    "                         mode='lines', name='Actual = 0 (Died)'))\n",
    "\n",
    "fig.update_layout(title='Binary Cross-Entropy Loss',\n",
    "                  xaxis_title='Predicted Probability',\n",
    "                  yaxis_title='Loss',\n",
    "                  showlegend=True)\n",
    "fig.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- If actual = 1, we want predicted probability close to 1 (low loss)\")\n",
    "print(\"- If actual = 0, we want predicted probability close to 0 (low loss)\")\n",
    "print(\"- Being confident and wrong results in very high loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks (Conceptual)\n",
    "\n",
    "**Backpropagation** is the algorithm for training neural networks:\n",
    "\n",
    "1. **Forward pass**: Make predictions\n",
    "2. **Calculate loss**: How wrong are we?\n",
    "3. **Backward pass**: Calculate gradient of loss with respect to each weight\n",
    "4. **Update weights**: Adjust weights in direction that reduces loss\n",
    "5. **Repeat**: Go through dataset multiple times (epochs)\n",
    "\n",
    "**Gradient Descent** optimization:\n",
    "```\n",
    "new_weight = old_weight - learning_rate × gradient\n",
    "```\n",
    "\n",
    "**Note**: We won't implement neural networks from scratch today, but libraries like TensorFlow and PyTorch handle this for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Exercises and Practice (5 mins)\n",
    "\n",
    "### Exercise 10.1: Improve Classification Performance\n",
    "\n",
    "Try to improve model accuracy by:\n",
    "1. Adding more features\n",
    "2. Engineering new features\n",
    "3. Trying different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your turn! Try to improve the model\n",
    "# Ideas:\n",
    "# - Add 'cabin_known' feature (binary: 1 if cabin is not null)\n",
    "# - Add 'is_alone' feature (1 if family_size == 1)\n",
    "# - Extract title from name (Mr, Mrs, Miss, Master)\n",
    "# - Try different train-test split ratios\n",
    "# - Experiment with different K values for K-NN\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2: Predict Passenger Class\n",
    "\n",
    "Change the problem: Predict `pclass` (1, 2, or 3) instead of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a multi-class classifier\n",
    "# 1. Select appropriate features (don't include pclass!)\n",
    "# 2. Split data\n",
    "# 3. Train a model (Logistic Regression works for multi-class)\n",
    "# 4. Evaluate using accuracy\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3: Clustering Analysis\n",
    "\n",
    "Analyze the clusters you created:\n",
    "1. What are the characteristics of each cluster?\n",
    "2. How do survival rates differ across clusters?\n",
    "3. Can you give meaningful names to each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze clusters\n",
    "# Calculate survival rate for each cluster\n",
    "# Look at other characteristics (sex, embarked, etc.)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Reflection\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "Today we learned:\n",
    "\n",
    "#### 1. Machine Learning Fundamentals\n",
    "- Machine learning is an optimization task: minimize loss by adjusting parameters\n",
    "- Three types: supervised (labeled data), unsupervised (unlabeled), reinforcement (rewards)\n",
    "\n",
    "#### 2. Supervised Learning: Classification\n",
    "- **K-NN**: Classify based on nearest neighbors (simple, intuitive)\n",
    "- **Decision Trees**: Create decision rules (interpretable)\n",
    "- **Logistic Regression**: Linear model with sigmoid activation (fast, probabilistic)\n",
    "- **SVM**: Find optimal decision boundary (effective in high dimensions)\n",
    "\n",
    "#### 3. Supervised Learning: Regression\n",
    "- **Linear Regression**: Predict continuous values with linear relationship\n",
    "- Metrics: MSE, RMSE (lower is better), R² (higher is better)\n",
    "\n",
    "#### 4. Model Evaluation\n",
    "- **Train-test split**: Essential for detecting overfitting\n",
    "- **Cross-validation**: More robust evaluation using multiple splits\n",
    "- **Metrics**: Accuracy, precision, recall, F1-score (classification); MSE, RMSE, R² (regression)\n",
    "\n",
    "#### 5. Unsupervised Learning: Clustering\n",
    "- **K-Means**: Group similar data points without labels\n",
    "- **Elbow method**: Find optimal number of clusters\n",
    "- **Silhouette score**: Measure clustering quality\n",
    "\n",
    "#### 6. Neural Networks (Conceptual)\n",
    "- **Architecture**: Input → Hidden Layer(s) → Output\n",
    "- **Activation functions**: ReLU, sigmoid, tanh, softmax\n",
    "- **Hyperparameters**: layers, neurons, learning rate, batch size, epochs\n",
    "- **Loss functions**: Cross-entropy (classification), MSE/MAE (regression)\n",
    "- **Training**: Forward pass + backpropagation + gradient descent\n",
    "\n",
    "### Machine Learning Workflow\n",
    "\n",
    "```\n",
    "1. Define Problem (classification, regression, clustering)\n",
    "   ↓\n",
    "2. Prepare Data (cleaning, encoding, feature engineering)\n",
    "   ↓\n",
    "3. Split Data (train/test or cross-validation)\n",
    "   ↓\n",
    "4. Choose Algorithm (based on problem type)\n",
    "   ↓\n",
    "5. Train Model (fit on training data)\n",
    "   ↓\n",
    "6. Evaluate Model (test on unseen data)\n",
    "   ↓\n",
    "7. Tune Hyperparameters (improve performance)\n",
    "   ↓\n",
    "8. Deploy Model (use for predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **What's the difference between classification and regression?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "2. **Why do we need to split data into training and testing sets?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "3. **Which classification model performed best on the Titanic dataset? Why do you think that is?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "4. **What are the three most important features for predicting survival?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "5. **When would you use unsupervised learning instead of supervised learning?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "6. **What is the purpose of activation functions in neural networks?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "7. **What's the relationship between loss functions and optimization?**\n",
    "\n",
    "   Your answer: ___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus Challenges (Optional)\n",
    "\n",
    "### Challenge 1: Feature Engineering\n",
    "Create a new feature called `fare_category` that bins fare into 4 categories (cheap, medium, expensive, luxury). Does this improve model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Ensemble Methods\n",
    "Research and try a Random Forest classifier (ensemble of decision trees). How does it compare to individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: from sklearn.ensemble import RandomForestClassifier\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Hyperparameter Tuning\n",
    "Use GridSearchCV to find the best hyperparameters for your favorite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: from sklearn.model_selection import GridSearchCV\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources for Further Learning\n",
    "\n",
    "### Documentation\n",
    "- **Scikit-learn User Guide**: https://scikit-learn.org/stable/user_guide.html\n",
    "- **Scikit-learn Cheat Sheet**: https://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "\n",
    "### Tutorials\n",
    "- **Machine Learning Crash Course (Google)**: https://developers.google.com/machine-learning/crash-course\n",
    "- **Kaggle Learn**: https://www.kaggle.com/learn\n",
    "- **Neural Networks Explained**: https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "### Books\n",
    "- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Aurélien Géron\n",
    "- *Introduction to Statistical Learning* (free PDF): https://www.statlearning.com/\n",
    "\n",
    "### Practice\n",
    "- **Kaggle Competitions**: https://www.kaggle.com/competitions\n",
    "- **UCI ML Repository**: https://archive.ics.uci.edu/ml/\n",
    "\n",
    "---\n",
    "\n",
    "**Great job today! You've taken your first steps into machine learning!**\n",
    "\n",
    "Next session: Advanced machine learning techniques and deep learning with neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
