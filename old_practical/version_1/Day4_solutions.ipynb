{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4033394c",
   "metadata": {},
   "source": [
    "# Day 4 — Solutions Notebook\n",
    "\n",
    "*Auto-generated notebook based on provided lecture slides.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372654c",
   "metadata": {},
   "source": [
    "## Solutions — Day 4: Data Prep & Feature Engineering\n",
    "Worked solutions and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: installs (uncomment the !pip lines if needed) and imports\n",
    "# If running in a managed environment (e.g. Google Colab), uncomment the pip installs below.\n",
    "# !pip install pandas numpy seaborn plotly scikit-learn matplotlib\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Load dataset (seaborn's titanic dataset) - we'll use this across all notebooks\n",
    "df = sns.load_dataset('titanic')\n",
    "df_original = df.copy()  # keep a pristine copy\n",
    "print('Loaded titanic dataset with shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57447e",
   "metadata": {},
   "source": [
    "### Data types & conversions (solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "# Convert survived and pclass (if needed)\n",
    "df['survived'] = df['survived'].astype('int')\n",
    "if 'pclass' in df.columns:\n",
    "    df['pclass'] = df['pclass'].astype('category')\n",
    "print('\\nConverted types:\\n', df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1471422",
   "metadata": {},
   "source": [
    "### Missing values (solution)\n",
    "Two approaches shown: drop rows vs median imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows example\n",
    "df_drop = df.dropna(subset=['age','fare'])\n",
    "print('After dropping rows with missing age/fare:', df_drop.shape)\n",
    "\n",
    "# Median imputation example\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='median')\n",
    "df_imputed = df.copy()\n",
    "df_imputed['age'] = imp.fit_transform(df[['age']])\n",
    "print('Missing age after impute:', df_imputed['age'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4070e",
   "metadata": {},
   "source": [
    "### Outlier handling (solution)\n",
    "Two options: remove extreme fares, or cap them (winsorize). We'll show both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e02e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR remove\n",
    "q1 = df['fare'].quantile(0.25)\n",
    "q3 = df['fare'].quantile(0.75)\n",
    "iqr = q3-q1\n",
    "lower = q1 - 1.5*iqr\n",
    "upper = q3 + 1.5*iqr\n",
    "df_iqr_removed = df[~((df['fare']<lower) | (df['fare']>upper))]\n",
    "print('After IQR removal:', df_iqr_removed.shape)\n",
    "\n",
    "# Capping (winsorizing)\n",
    "df_capped = df.copy()\n",
    "df_capped['fare'] = np.clip(df_capped['fare'], lower, upper)\n",
    "print('Capped fares min/max:', df_capped['fare'].min(), df_capped['fare'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699de64",
   "metadata": {},
   "source": [
    "### Normalization & Encoding (solution)\n",
    "We show Min-Max and Standard scaling. Also one-hot encoding with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ff1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataFrame: median impute age\n",
    "X = df_imputed.copy()\n",
    "X['fare'] = X['fare'].fillna(X['fare'].median())\n",
    "num_cols = ['age','fare']\n",
    "# Min-Max\n",
    "mm = MinMaxScaler()\n",
    "X_mm = X.copy()\n",
    "X_mm[num_cols] = mm.fit_transform(X_mm[num_cols])\n",
    "# Standard\n",
    "ss = StandardScaler()\n",
    "X_ss = X.copy()\n",
    "X_ss[num_cols] = ss.fit_transform(X_ss[num_cols])\n",
    "\n",
    "# One-hot encoding\n",
    "X_encoded = pd.get_dummies(X_mm, columns=['sex','embarked'], drop_first=True)\n",
    "print('Encoded cols sample:', [c for c in X_encoded.columns if 'sex_' in c or 'embarked_' in c])\n",
    "X_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73f952",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Normalization is important for distance-based models (KNN) or gradient-based optimization.\n",
    "- One-hot encoding is necessary for categorical variables when algorithms expect numeric inputs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
