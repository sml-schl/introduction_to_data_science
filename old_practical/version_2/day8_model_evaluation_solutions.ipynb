{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 8: Model Evaluation & Assessment - SOLUTIONS\n\n**Duration:** 90 minutes\n**Dataset:** Titanic Passenger Data\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Setup and Data Loading"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import libraries\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n    mean_squared_error, mean_absolute_error, r2_score\n)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"\u2713 Libraries imported successfully!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load and prepare data\ndf = sns.load_dataset('titanic')\nprint(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\nprint(f\"Survival rate: {df['survived'].mean()*100:.1f}%\")\n\ndf_clean = df.copy()\ndf_clean['age'].fillna(df_clean['age'].median(), inplace=True)\ndf_clean['embarked'].fillna(df_clean['embarked'].mode()[0], inplace=True)\ndf_clean['fare'].fillna(df_clean['fare'].median(), inplace=True)\ndf_clean['sex_encoded'] = df_clean['sex'].map({'male': 1, 'female': 0})\ndf_clean['family_size'] = df_clean['sibsp'] + df_clean['parch'] + 1\ndf_clean['is_alone'] = (df_clean['family_size'] == 1).astype(int)\n\nfeatures = ['pclass', 'sex_encoded', 'age', 'fare', 'family_size', 'is_alone']\nX = df_clean[features]\ny = df_clean['survived']\n\nprint(\"\\nFeatures prepared!\")\ndf_clean.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 2: Train-Test Split - SOLUTION"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SOLUTION: Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"\\nTraining set survival rate: {y_train.mean()*100:.1f}%\")\nprint(f\"Test set survival rate: {y_test.mean()*100:.1f}%\")\n\nprint(\"\\n**Answer:** We use random_state=42 for reproducibility - so we get the same split every time.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 3: Bias-Variance Tradeoff - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train models with different complexities\nmodel_simple = LogisticRegression(max_iter=1000, random_state=42)\nmodel_simple.fit(X_train, y_train)\n\nmodel_medium = DecisionTreeClassifier(max_depth=3, random_state=42)\nmodel_medium.fit(X_train, y_train)\n\n# SOLUTION: Complex model\nmodel_complex = DecisionTreeClassifier(random_state=42)\nmodel_complex.fit(X_train, y_train)\n\nprint(\"\u2713 All models trained!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Compare performance\nmodels = {\n    'Simple (Logistic)': model_simple,\n    'Medium (Tree depth=3)': model_medium,\n    'Complex (Tree unlimited)': model_complex\n}\n\nresults = []\nfor name, model in models.items():\n    train_acc = model.score(X_train, y_train)\n    test_acc = model.score(X_test, y_test)\n    gap = train_acc - test_acc\n    \n    results.append({\n        'Model': name,\n        'Train Accuracy': train_acc,\n        'Test Accuracy': test_acc,\n        'Gap': gap\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"Model Performance Comparison:\")\nprint(results_df.round(4))\n\nprint(\"\\n**Analysis:**\")\nprint(\"- Simple model: Small gap (low variance), decent test performance\")\nprint(\"- Medium model: Good balance, best test performance\")\nprint(\"- Complex model: Large gap indicates OVERFITTING (memorizing training data)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize\nfig = go.Figure()\nfig.add_trace(go.Bar(name='Training', x=results_df['Model'], y=results_df['Train Accuracy'], marker_color='lightblue'))\nfig.add_trace(go.Bar(name='Test', x=results_df['Model'], y=results_df['Test Accuracy'], marker_color='darkblue'))\nfig.update_layout(title='Bias-Variance Tradeoff', xaxis_title='Model', yaxis_title='Accuracy', barmode='group', yaxis_tickformat='.0%')\nfig.show()\n\nprint(\"\\n**Question Answer:** Complex model shows overfitting - high training accuracy but lower test accuracy.\")\nprint(\"**Best choice:** Medium complexity model has best test performance and smallest train-test gap.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Learning curves\ntrain_sizes, train_scores, test_scores = learning_curve(\n    model_medium, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, random_state=42\n)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_mean, name='Training', mode='lines+markers', line=dict(color='lightblue')))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_mean, name='CV', mode='lines+markers', line=dict(color='darkblue')))\nfig.update_layout(title='Learning Curve', xaxis_title='Training Size', yaxis_title='Accuracy', yaxis_tickformat='.0%')\nfig.show()\n\nprint(\"\\n**Answer:** As we add more data, the gap between training and test scores DECREASES.\")\nprint(\"More data helps the model generalize better!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 4: Regression Metrics - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Regression example\nX_reg = df_clean[['pclass', 'sex_encoded', 'age', 'family_size']].copy()\ny_reg = df_clean['fare'].copy()\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n\nreg_model = LinearRegression()\nreg_model.fit(X_train_reg, y_train_reg)\ny_pred_reg = reg_model.predict(X_test_reg)\n\n# SOLUTION: Calculate all metrics\nmae = mean_absolute_error(y_test_reg, y_pred_reg)\nmse = mean_squared_error(y_test_reg, y_pred_reg)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_reg, y_pred_reg)\n\nprint(\"Regression Metrics:\")\nprint(f\"MAE:  \u00a3{mae:.2f} (on average, off by this much)\")\nprint(f\"MSE:  {mse:.2f}\")\nprint(f\"RMSE: \u00a3{rmse:.2f} (similar to MAE but penalizes large errors)\")\nprint(f\"R\u00b2:   {r2:.3f} (model explains {r2*100:.1f}% of variance)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize predictions\ncomparison_df = pd.DataFrame({'Actual': y_test_reg, 'Predicted': y_pred_reg})\nfig = px.scatter(comparison_df, x='Actual', y='Predicted', title='Actual vs Predicted Fare')\nfig.add_trace(go.Scatter(x=[0, comparison_df['Actual'].max()], y=[0, comparison_df['Actual'].max()], \n                         mode='lines', name='Perfect', line=dict(color='red', dash='dash')))\nfig.show()\n\nprint(\"\\n**Answer:** Points far from red line = large prediction errors. Perfect predictions would lie on the line.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 5: Classification Metrics - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train classifier\nclf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_model.fit(X_train, y_train)\ny_pred = clf_model.predict(X_test)\n\n# SOLUTION: Calculate all metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Classification Metrics:\")\nprint(f\"Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}% correct overall)\")\nprint(f\"Precision: {precision:.3f} (when we predict survival, we're right {precision*100:.1f}% of the time)\")\nprint(f\"Recall:    {recall:.3f} (we catch {recall*100:.1f}% of actual survivors)\")\nprint(f\"F1-Score:  {f1:.3f} (balanced metric)\")\n\nprint(\"\\n**Iceberg Detection Answer:** Prioritize RECALL! Better to have false alarms than miss an iceberg.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 6: Confusion Matrix - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SOLUTION: Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nInterpretation:\")\nprint(f\"True Negatives (TN):  {cm[0,0]} - Correctly predicted died\")\nprint(f\"False Positives (FP): {cm[0,1]} - Predicted survived but died (Type I)\")\nprint(f\"False Negatives (FN): {cm[1,0]} - Predicted died but survived (Type II)\")\nprint(f\"True Positives (TP):  {cm[1,1]} - Correctly predicted survived\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize\nfig = px.imshow(cm, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n                x=['Died (0)', 'Survived (1)'], y=['Died (0)', 'Survived (1)'],\n                text_auto=True, color_continuous_scale='Blues', title='Confusion Matrix')\nfig.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SOLUTION: Manual calculation\nTN, FP, FN, TP = cm.ravel()\n\nmanual_accuracy = (TP + TN) / (TP + TN + FP + FN)\nmanual_precision = TP / (TP + FP)\nmanual_recall = TP / (TP + FN)\n\nprint(\"Manual Calculations:\")\nprint(f\"Accuracy:  {manual_accuracy:.3f}\")\nprint(f\"Precision: {manual_precision:.3f}\")\nprint(f\"Recall:    {manual_recall:.3f}\")\n\nprint(\"\\nVerification:\")\nprint(f\"Accuracy matches:  {np.isclose(manual_accuracy, accuracy)}\")\nprint(f\"Precision matches: {np.isclose(manual_precision, precision)}\")\nprint(f\"Recall matches:    {np.isclose(manual_recall, recall)}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SOLUTION: Classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 7: Type I vs Type II Errors - SOLUTIONS"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Answers to scenarios:**\n\n1. **Cancer Screening:** Type II is worse (missing cancer is more dangerous than false alarm)\n2. **Credit Card Fraud:** Depends on context, but usually Type II is worse (losing money to fraud)\n3. **Spam Filter:** Type I is worse (missing important emails)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Error rates\ntype1_error_rate = FP / (FP + TN)\ntype2_error_rate = FN / (FN + TP)\n\nprint(\"Error Analysis:\")\nprint(f\"\\nType I Error Rate:  {type1_error_rate:.3f} - {FP} false positives\")\nprint(f\"Type II Error Rate: {type2_error_rate:.3f} - {FN} false negatives\")\nprint(\"\\n**Answer:** In disaster scenario, Type II is worse - predicting death when they could survive\")\nprint(\"means we might not allocate resources to save them!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 8: ROC Curve & AUC - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SOLUTION: ROC curve\ny_pred_proba = clf_model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\n\nprint(f\"AUC Score: {auc:.3f}\")\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={auc:.3f})', line=dict(color='blue', width=2)))\nfig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Random (0.5)', line=dict(color='red', dash='dash')))\nfig.update_layout(title='ROC Curve', xaxis_title='False Positive Rate', yaxis_title='True Positive Rate', \n                  width=700, height=700)\nfig.show()\n\nprint(\"\\n**Answer 1:** Close to top-left = HIGH true positive rate, LOW false positive rate = EXCELLENT!\")\nprint(\"**Answer 2:** Following diagonal = random guessing = USELESS model\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Compare multiple models\nmodels_to_compare = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n}\n\nfig = go.Figure()\nfor name, model in models_to_compare.items():\n    model.fit(X_train, y_train)\n    y_proba = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    auc_score = roc_auc_score(y_test, y_proba)\n    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'{name} (AUC={auc_score:.3f})'))\n\nfig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Random', line=dict(color='red', dash='dash')))\nfig.update_layout(title='ROC Comparison', xaxis_title='FPR', yaxis_title='TPR', width=700, height=700)\nfig.show()\n\nprint(\"\\n**Answer:** Best model = highest AUC score (curve most towards top-left)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 9: Cross-Validation - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SOLUTION: Cross-validation\ncv_scores = cross_val_score(clf_model, X, y, cv=5)\n\nprint(\"Cross-Validation Results:\")\nprint(f\"Scores: {cv_scores}\")\nprint(f\"\\nMean: {cv_scores.mean():.3f}\")\nprint(f\"Std Dev: {cv_scores.std():.3f}\")\nprint(f\"95% CI: {cv_scores.mean():.3f} \u00b1 {1.96*cv_scores.std():.3f}\")\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=[f'Fold {i+1}' for i in range(5)], y=cv_scores, marker_color='lightblue'))\nfig.add_hline(y=cv_scores.mean(), line_dash=\"dash\", line_color=\"red\", annotation_text=f\"Mean: {cv_scores.mean():.3f}\")\nfig.update_layout(title='CV Scores', xaxis_title='Fold', yaxis_title='Accuracy', yaxis_tickformat='.0%')\nfig.show()\n\nprint(\"\\n**Answer:** CV is more reliable because it tests on multiple different splits,\")\nprint(\"reducing the chance that results are due to a lucky/unlucky single split.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Part 10: Ethical AI - SOLUTIONS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Bias detection\nresults_by_gender = []\nfor gender in [0, 1]:\n    mask = X_test['sex_encoded'] == gender\n    X_test_gender = X_test[mask]\n    y_test_gender = y_test[mask]\n    \n    if len(y_test_gender) > 0:\n        y_pred_gender = clf_model.predict(X_test_gender)\n        results_by_gender.append({\n            'Gender': 'Female' if gender == 0 else 'Male',\n            'Count': len(y_test_gender),\n            'Accuracy': accuracy_score(y_test_gender, y_pred_gender),\n            'Precision': precision_score(y_test_gender, y_pred_gender, zero_division=0),\n            'Recall': recall_score(y_test_gender, y_pred_gender, zero_division=0)\n        })\n\nbias_df = pd.DataFrame(results_by_gender)\nprint(\"Performance by Gender:\")\nprint(bias_df.round(3))\n\nfig = go.Figure()\nfor metric in ['Accuracy', 'Precision', 'Recall']:\n    fig.add_trace(go.Bar(name=metric, x=bias_df['Gender'], y=bias_df[metric]))\nfig.update_layout(title='Bias Analysis', barmode='group', yaxis_tickformat='.0%')\nfig.show()\n\nprint(\"\\n**Answer 1:** Model performs differently for men vs women, reflecting historical bias.\")\nprint(\"**Answer 2:** Yes, it reflects Titanic's 'women and children first' policy. In this HISTORICAL\")\nprint(\"context, it's accurate. But using sex for modern survival predictions would be problematic!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Trustworthy AI Scenario Answers:**\n\n1. **Black box hospital AI:** Violates **Transparency** (doctors can't explain decisions)\n2. **Biased facial recognition:** Violates **Diversity & Fairness** (unequal accuracy across groups)\n3. **Discriminatory loan AI:** Violates **Fairness** and **Privacy/Data Governance** (perpetuates bias)\n4. **No-appeal hiring AI:** Violates **Human Agency**, **Transparency**, and **Accountability**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': features,\n    'Importance': clf_model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"Feature Importance:\")\nprint(feature_importance)\n\nfig = px.bar(feature_importance, x='Importance', y='Feature', orientation='h',\n             title='Model Feature Importance')\nfig.show()\n\nprint(\"\\n**Ethics Answer:** For HISTORICAL analysis (Titanic), using sex is appropriate - it reflects\")\nprint(\"the actual evacuation policy. For MODERN applications, using sex/gender would be discriminatory.\")\nprint(\"\\n**Explanation:** 'Our model found that being female, in 1st class, with a higher fare increased\")\nprint(\"survival odds, reflecting the evacuation priority given to women and upper-class passengers.'\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\\n## Summary\n\n**Key Metrics:**\n- Our Random Forest achieved {:.1f}% accuracy with AUC of {:.3f}\n- Always evaluate multiple metrics, not just accuracy\n- Cross-validation showed consistent performance across folds\n- Detected performance differences between genders\n\n**Best Practices:**\n1. Always use train-test split\n2. Evaluate multiple metrics appropriate for your problem\n3. Check for bias across sensitive attributes\n4. Use cross-validation for robust estimates\n5. Prioritize interpretability and fairness\n\n**Congratulations on completing the course!** \ud83c\udf89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}