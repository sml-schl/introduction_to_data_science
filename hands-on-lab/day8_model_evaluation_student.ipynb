{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tag 8: Model Evaluation & Ethisches AI üéØ\n",
        "\n",
        "**Dauer:** 90 Minuten  \n",
        "**Datensatz:** Titanic Passenger Data\n",
        "\n",
        "## Lernziele\n",
        "- ‚úÖ Bias vs. Variance Tradeoff verstehen\n",
        "- ‚úÖ Overfitting und Underfitting erkennen\n",
        "- ‚úÖ Klassifikationsmetriken anwenden (Accuracy, Precision, Recall, F1)\n",
        "- ‚úÖ Confusion Matrix interpretieren\n",
        "- ‚úÖ ROC/AUC Kurven verstehen\n",
        "- ‚úÖ Type I vs. Type II Errors unterscheiden\n",
        "- ‚úÖ Ethisches AI & Trustworthy AI Prinzipien kennen\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Setup und Datenvorbereitung (5 min)\n",
        "\n",
        "### Warum Model Evaluation?\n",
        "\n",
        "Ein Machine Learning Modell zu bauen ist nur der erste Schritt. Die wichtigste Frage ist: **Wie gut ist unser Modell wirklich?**\n",
        "\n",
        "**Heute lernen wir:**\n",
        "- üìä Modell-Performance objektiv messen\n",
        "- üéØ Erkennen wann Modelle zu simpel oder zu komplex sind\n",
        "- üìà Die richtigen Metriken f√ºr verschiedene Probleme w√§hlen\n",
        "- ü§ñ Ethische AI-Systeme entwickeln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(f\"Survival rate: {df['survived'].mean()*100:.1f}%\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daten vorbereiten\n",
        "df_clean = df.copy()\n",
        "df_clean['age'].fillna(df_clean['age'].median(), inplace=True)\n",
        "df_clean['embarked'].fillna(df_clean['embarked'].mode()[0], inplace=True)\n",
        "df_clean['fare'].fillna(df_clean['fare'].median(), inplace=True)\n",
        "\n",
        "# Features erstellen\n",
        "df_clean['sex_encoded'] = df_clean['sex'].map({'male': 1, 'female': 0})\n",
        "df_clean['family_size'] = df_clean['sibsp'] + df_clean['parch'] + 1\n",
        "df_clean['is_alone'] = (df_clean['family_size'] == 1).astype(int)\n",
        "\n",
        "# Finale Features\n",
        "features = ['pclass', 'sex_encoded', 'age', 'fare', 'family_size', 'is_alone']\n",
        "X = df_clean[features]\n",
        "y = df_clean['survived']\n",
        "\n",
        "# Train-Test Split (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"‚úì Daten vorbereitet!\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2Ô∏è‚É£ Bias-Variance Tradeoff (7 min)\n",
        "\n",
        "### Die Goldene Regel des Machine Learning\n",
        "\n",
        "**NIEMALS auf Trainingsdaten testen!**\n",
        "\n",
        "**Analogie - Pr√ºfungsvorbereitung:**\n",
        "- **Training Set:** √úbungsaufgaben zum Lernen\n",
        "- **Test Set:** Die echte Pr√ºfung mit neuen Fragen\n",
        "\n",
        "**Bias (Verzerrung):** Wie weit liegen Vorhersagen im Durchschnitt daneben?\n",
        "- ‚¨ÜÔ∏è Hoher Bias = **Underfitting** (Modell zu simpel)\n",
        "- Modell √ºbersieht wichtige Muster\n",
        "\n",
        "**Variance (Varianz):** Wie stark variieren Vorhersagen bei unterschiedlichen Trainingsdaten?\n",
        "- ‚¨ÜÔ∏è Hohe Variance = **Overfitting** (Modell zu komplex)\n",
        "- Modell merkt sich Trainingsdaten inkl. Rauschen\n",
        "\n",
        "```\n",
        "High Bias          Sweet Spot          High Variance\n",
        "(Underfitting)                         (Overfitting)\n",
        "Zu simpel          ‚úÖ Balance           Zu komplex\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ In-Lecture √úbung 2.1: Bias vs. Variance Identifizieren (5 min)\n",
        "\n",
        "**Szenario:** Drei ML-Modelle wurden trainiert:\n",
        "\n",
        "**Modell A:**\n",
        "- Training Accuracy: 65%\n",
        "- Test Accuracy: 64%\n",
        "\n",
        "**Modell B:**\n",
        "- Training Accuracy: 99%\n",
        "- Test Accuracy: 72%\n",
        "\n",
        "**Modell C:**\n",
        "- Training Accuracy: 82%\n",
        "- Test Accuracy: 79%\n",
        "\n",
        "**Aufgaben:**\n",
        "1. Welches Modell hat **Underfitting** (High Bias)?\n",
        "2. Welches Modell hat **Overfitting** (High Variance)?\n",
        "3. Welches Modell ist am besten balanciert?\n",
        "\n",
        "```python\n",
        "# TODO: Schreibe deine Antworten hier\n",
        "# 1. Underfitting: Modell ___\n",
        "# 2. Overfitting: Modell ___\n",
        "# 3. Beste Balance: Modell ___\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Teste deine Antworten\n",
        "# Antworten:\n",
        "# 1. Modell A (beide niedrig = zu simpel)\n",
        "# 2. Modell B (riesiger Gap = merkt sich Trainingsdaten)\n",
        "# 3. Modell C (kleiner Gap, gute Performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3Ô∏è‚É£ Klassifikationsmetriken (10 min)\n",
        "\n",
        "### Confusion Matrix - Die Grundlage aller Metriken\n",
        "\n",
        "Eine **Confusion Matrix** zeigt, wie oft ein Klassifikationsmodell richtig vs. falsch liegt:\n",
        "\n",
        "```\n",
        "                 Predicted\n",
        "                 0    1\n",
        "Actual  0       TN   FP\n",
        "        1       FN   TP\n",
        "```\n",
        "\n",
        "**Vier Quadranten:**\n",
        "- **True Positive (TP):** Richtig als positiv erkannt (z.B. √úberlebende korrekt vorhergesagt)\n",
        "- **True Negative (TN):** Richtig als negativ erkannt (z.B. Nicht-√úberlebende korrekt vorhergesagt)\n",
        "- **False Positive (FP):** F√§lschlicherweise als positiv erkannt (**Type I Error**)\n",
        "- **False Negative (FN):** F√§lschlicherweise als negativ erkannt (**Type II Error**)\n",
        "\n",
        "### Die wichtigsten Metriken:\n",
        "\n",
        "**1. Accuracy (Genauigkeit):**\n",
        "```\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "```\n",
        "Wie viele Vorhersagen waren insgesamt richtig?\n",
        "\n",
        "**2. Precision (Pr√§zision):**\n",
        "```\n",
        "Precision = TP / (TP + FP)\n",
        "```\n",
        "Von allen als \"positiv\" vorhergesagten, wie viele waren wirklich positiv?\n",
        "\n",
        "**3. Recall (Sensitivit√§t):**\n",
        "```\n",
        "Recall = TP / (TP + FN)\n",
        "```\n",
        "Von allen echten Positiven, wie viele haben wir gefunden?\n",
        "\n",
        "**4. F1-Score:**\n",
        "```\n",
        "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
        "```\n",
        "Harmonisches Mittel von Precision und Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ In-Lecture √úbung 3.1: Confusion Matrix interpretieren (7 min)\n",
        "\n",
        "**Gegeben:** Ein Modell zur Titanic-√úberlebensvorhersage\n",
        "\n",
        "```\n",
        "Confusion Matrix:\n",
        "                Predicted\n",
        "                Dead  Survived\n",
        "Actual Dead      90      15\n",
        "       Survived  10      64\n",
        "```\n",
        "\n",
        "**Aufgaben:**\n",
        "1. Berechne **Accuracy**\n",
        "2. Berechne **Precision** (f√ºr \"Survived\")\n",
        "3. Berechne **Recall** (f√ºr \"Survived\")\n",
        "4. Was bedeutet diese Confusion Matrix f√ºr unser Modell?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Berechne die Metriken\n",
        "TP = 64  # Survived richtig vorhergesagt\n",
        "TN = 90  # Dead richtig vorhergesagt\n",
        "FP = 15  # Dead als Survived vorhergesagt\n",
        "FN = 10  # Survived als Dead vorhergesagt\n",
        "\n",
        "# Aufgabe 1: Accuracy\n",
        "accuracy = # TODO: Deine Formel hier\n",
        "\n",
        "# Aufgabe 2: Precision\n",
        "precision = # TODO: Deine Formel hier\n",
        "\n",
        "# Aufgabe 3: Recall\n",
        "recall = # TODO: Deine Formel hier\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Precision: {precision:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ Type I vs. Type II Errors (5 min)\n",
        "\n",
        "### Medizinisches Beispiel - COVID-19 Test:\n",
        "\n",
        "**Type I Error (False Positive, Œ±):**\n",
        "- Fehler: Gesunde Person wird als \"positiv\" diagnostiziert\n",
        "- Konsequenz: Unn√∂tige Quarant√§ne, Angst\n",
        "- **\"False Alarm\"**\n",
        "\n",
        "**Type II Error (False Negative, Œ≤):**\n",
        "- Fehler: Kranke Person wird als \"negativ\" diagnostiziert\n",
        "- Konsequenz: Keine Behandlung, Verbreitung der Krankheit\n",
        "- **\"Missed Detection\"**\n",
        "\n",
        "**Welcher Fehler ist schlimmer?** ‚Üí H√§ngt vom Kontext ab!\n",
        "\n",
        "- **Spam Filter:** FP schlimmer (wichtige E-Mail geht verloren)\n",
        "- **Krebs-Diagnose:** FN schlimmer (Krankheit wird √ºbersehen)\n",
        "- **Titanic:** FN schlimmer (√úberlebenschance wird falsch eingesch√§tzt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ In-Lecture √úbung 4.1: Fehlertypen Identifizieren (3 min)\n",
        "\n",
        "**F√ºr folgende Szenarien:** Welcher Fehler ist gef√§hrlicher - Type I oder Type II?\n",
        "\n",
        "1. **Betrugserkennung bei Kreditkarten**\n",
        "   - Type I: Legitime Transaktion wird blockiert\n",
        "   - Type II: Betr√ºgerische Transaktion wird durchgelassen\n",
        "   - Schlimmer? ___________\n",
        "\n",
        "2. **Autopilot Fu√üg√§ngererkennung**\n",
        "   - Type I: Objekt wird f√§lschlicherweise als Fu√üg√§nger erkannt\n",
        "   - Type II: Echter Fu√üg√§nger wird nicht erkannt\n",
        "   - Schlimmer? ___________\n",
        "\n",
        "3. **Job-Bewerbung Vorauswahl (AI)**\n",
        "   - Type I: Ungeeigneter Kandidat wird eingeladen\n",
        "   - Type II: Geeigneter Kandidat wird abgelehnt\n",
        "   - Schlimmer? ___________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Diskutiere mit deinem Nachbarn\n",
        "# Antworten:\n",
        "# 1. Type II (Betrug wird nicht erkannt ‚Üí Geldverlust)\n",
        "# 2. Type II (Fu√üg√§nger wird √ºbersehen ‚Üí Unfall!)\n",
        "# 3. Kontext-abh√§ngig (oft Type II ‚Üí Talente gehen verloren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5Ô∏è‚É£ ROC Curve & AUC (10 min)\n",
        "\n",
        "### ROC Curve - Receiver Operating Characteristic\n",
        "\n",
        "Die **ROC Curve** visualisiert die Performance eines Klassifikators √ºber alle m√∂glichen Schwellenwerte.\n",
        "\n",
        "**Achsen:**\n",
        "- **X-Achse:** False Positive Rate (FPR) = FP / (FP + TN)\n",
        "- **Y-Achse:** True Positive Rate (TPR) = TP / (TP + FN) = Recall\n",
        "\n",
        "**AUC - Area Under the Curve:**\n",
        "- **AUC = 1.0:** Perfekter Klassifikator\n",
        "- **AUC = 0.5:** Zuf√§lliges Raten (Diagonale)\n",
        "- **AUC < 0.5:** Schlechter als Zufall\n",
        "\n",
        "**Interpretation:**\n",
        "- Je n√§her die Kurve an der linken oberen Ecke, desto besser\n",
        "- AUC gibt eine einzelne Zahl f√ºr die Gesamtperformance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ In-Lecture √úbung 5.1: ROC Curve erstellen (7 min)\n",
        "\n",
        "**Aufgabe:** Erstelle eine ROC Curve f√ºr ein Logistic Regression Modell auf dem Titanic-Datensatz.\n",
        "\n",
        "**Schritte:**\n",
        "1. Trainiere ein Logistic Regression Modell\n",
        "2. Berechne Vorhersage-Wahrscheinlichkeiten\n",
        "3. Erstelle ROC Curve\n",
        "4. Berechne AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Erstelle ROC Curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "\n",
        "# Modell trainieren\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "# TODO: Trainiere das Modell\n",
        "\n",
        "# Vorhersage-Wahrscheinlichkeiten\n",
        "# TODO: Berechne y_pred_proba f√ºr die positive Klasse\n",
        "\n",
        "# ROC Curve berechnen\n",
        "# TODO: Verwende roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# AUC berechnen\n",
        "# TODO: Berechne auc(fpr, tpr)\n",
        "\n",
        "# Visualisierung\n",
        "# TODO: Plotte die ROC Curve mit matplotlib oder RocCurveDisplay\n",
        "\n",
        "print(f\"AUC Score: {roc_auc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6Ô∏è‚É£ Ethisches AI & Trustworthy AI (10 min)\n",
        "\n",
        "### Warum Ethik in AI wichtig ist\n",
        "\n",
        "**Real-World Probleme:**\n",
        "- üè• **Healthcare:** Algorithmen mit Bias gegen bestimmte Ethnien\n",
        "- üëÆ **Polizei:** Gesichtserkennung mit h√∂heren Fehlerraten bei dunkler Hautfarbe\n",
        "- üíº **Recruiting:** AI-Systeme bevorzugen m√§nnliche Kandidaten\n",
        "- üí∞ **Kredite:** Diskriminierung basierend auf Postleitzahl\n",
        "\n",
        "### EU AI Act - Trustworthy AI Prinzipien:\n",
        "\n",
        "1. **Human Agency & Oversight** - Menschliche Kontrolle\n",
        "2. **Technical Robustness & Safety** - Zuverl√§ssigkeit\n",
        "3. **Privacy & Data Governance** - Datenschutz\n",
        "4. **Transparency** - Nachvollziehbarkeit\n",
        "5. **Diversity & Fairness** - Keine Diskriminierung\n",
        "6. **Societal & Environmental Wellbeing** - Gesellschaftlicher Nutzen\n",
        "7. **Accountability** - Verantwortlichkeit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ In-Lecture √úbung 6.1: Ethik-Szenario Analyse (8 min)\n",
        "\n",
        "**Szenario:** Eine Universit√§t entwickelt ein AI-System zur Bewertung von Studierenden-Bewerbungen.\n",
        "\n",
        "**Gegebene Informationen:**\n",
        "- Training Data: Letzte 10 Jahre akzeptierte Studierende\n",
        "- Features: Noten, Alter, Geschlecht, Postleitzahl, Hobbies\n",
        "- Accuracy: 89% auf Test-Daten\n",
        "\n",
        "**Diskussionsfragen:**\n",
        "\n",
        "1. **Welche Bias-Risiken siehst du in diesem System?**\n",
        "   \n",
        "   ___________________________________\n",
        "\n",
        "2. **Welche EU AI Act Prinzipien k√∂nnten verletzt werden?**\n",
        "   \n",
        "   ___________________________________\n",
        "\n",
        "3. **Wie k√∂nntest du das System fairer gestalten?**\n",
        "   \n",
        "   ___________________________________\n",
        "\n",
        "4. **Sollte Geschlecht als Feature verwendet werden? Warum/nicht?**\n",
        "   \n",
        "   ___________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Diskutiere in Gruppen (4-5 Minuten)\n",
        "# M√∂gliche Probleme:\n",
        "# - Historical Bias: System lernt alte Diskriminierungsmuster\n",
        "# - Gender Bias: Geschlecht sollte irrelevant sein\n",
        "# - Geographic Bias: Postleitzahl als Proxy f√ºr Einkommen\n",
        "# - Fehlende Diversity & Fairness\n",
        "# - Mangelnde Transparency (Black Box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "# üìù Wiederholungsblock - F√ºr Zuhause\n",
        "\n",
        "**Ziel:** Vertiefe dein Verst√§ndnis von Model Evaluation durch praktische √úbungen.\n",
        "\n",
        "**Zeitaufwand:** 60-90 Minuten  \n",
        "**Empfehlung:** Arbeite alle Bl√∂cke durch, bevor du mit dem Abschlussprojekt startest!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù W1: Regression Metriken Deep Dive\n",
        "\n",
        "### Aufgabe: Titanic Fare Vorhersage mit Regression Metrics\n",
        "\n",
        "**Ziel:** Verstehe MAE, MSE, RMSE, R¬≤ durch praktische Anwendung.\n",
        "\n",
        "**Regression Metriken √úberblick:**\n",
        "\n",
        "1. **MAE (Mean Absolute Error):**\n",
        "   - Durchschnittlicher absoluter Fehler\n",
        "   - Einheit: Gleiche wie Zielvariable\n",
        "   - Robust gegen Ausrei√üer\n",
        "\n",
        "2. **MSE (Mean Squared Error):**\n",
        "   - Durchschnittlicher quadratischer Fehler\n",
        "   - Bestraft gro√üe Fehler st√§rker\n",
        "   - Empfindlich gegen Ausrei√üer\n",
        "\n",
        "3. **RMSE (Root Mean Squared Error):**\n",
        "   - Wurzel aus MSE\n",
        "   - Gleiche Einheit wie Zielvariable\n",
        "   - Interpretierbar\n",
        "\n",
        "4. **R¬≤ (Coefficient of Determination):**\n",
        "   - Anteil erkl√§rter Varianz (0 bis 1)\n",
        "   - 1.0 = perfekte Vorhersage\n",
        "   - 0.0 = Mittelwert-Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Regression Metriken praktisch anwenden\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Regression Problem: Fare Vorhersage\n",
        "X_fare = df_clean[['pclass', 'age', 'family_size']].dropna()\n",
        "y_fare = df_clean.loc[X_fare.index, 'fare']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_fare, y_fare, test_size=0.2, random_state=42)\n",
        "\n",
        "# Modell trainieren\n",
        "lr_model = LinearRegression()\n",
        "# TODO: Trainiere das Modell\n",
        "\n",
        "# Vorhersagen\n",
        "# TODO: Berechne y_pred\n",
        "\n",
        "# Metriken berechnen\n",
        "mae = # TODO: mean_absolute_error(y_test_r, y_pred)\n",
        "mse = # TODO: mean_squared_error(y_test_r, y_pred)\n",
        "rmse = # TODO: np.sqrt(mse)\n",
        "r2 = # TODO: r2_score(y_test_r, y_pred)\n",
        "\n",
        "print(f\"MAE: ${mae:.2f}\")\n",
        "print(f\"MSE: ${mse:.2f}\")\n",
        "print(f\"RMSE: ${rmse:.2f}\")\n",
        "print(f\"R¬≤: {r2:.3f}\")\n",
        "\n",
        "# Interpretation:\n",
        "# - MAE: Durchschnittlich ${mae:.2f} Abweichung\n",
        "# - RMSE: Bestraft gro√üe Fehler st√§rker\n",
        "# - R¬≤: {r2:.1%} der Varianz wird erkl√§rt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù W2: Learning Curves & Model Complexity\n",
        "\n",
        "### Aufgabe: Visualisiere Overfitting mit Learning Curves\n",
        "\n",
        "**Learning Curves** zeigen wie sich Training- und Test-Performance mit mehr Daten entwickeln.\n",
        "\n",
        "**Interpretation:**\n",
        "- **Hoher Bias (Underfitting):** Beide Kurven konvergieren auf niedriger Performance\n",
        "- **Hohe Variance (Overfitting):** Gro√üer Gap zwischen Training und Test\n",
        "- **Optimal:** Beide Kurven konvergieren auf hoher Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Erstelle Learning Curves\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Teste verschiedene max_depth Werte\n",
        "depths = [2, 5, 10, 20]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, depth in enumerate(depths):\n",
        "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    \n",
        "    # Learning Curve berechnen\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        # TODO: F√ºlle die Parameter aus\n",
        "        # model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "    )\n",
        "    \n",
        "    # Durchschnitte berechnen\n",
        "    # TODO: train_mean = np.mean(train_scores, axis=1)\n",
        "    # TODO: test_mean = np.mean(test_scores, axis=1)\n",
        "    \n",
        "    # Plotten\n",
        "    axes[idx].plot(train_sizes, train_mean, label='Training Score', marker='o')\n",
        "    axes[idx].plot(train_sizes, test_mean, label='Test Score', marker='s')\n",
        "    axes[idx].set_title(f'max_depth={depth}')\n",
        "    axes[idx].set_xlabel('Training Set Size')\n",
        "    axes[idx].set_ylabel('Accuracy')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Frage: Welches max_depth ist optimal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù W3: Multi-Model Comparison mit allen Metriken\n",
        "\n",
        "### Aufgabe: Vergleiche 5 verschiedene Modelle umfassend\n",
        "\n",
        "**Ziel:** Lerne wie man Modelle systematisch vergleicht und das Beste ausw√§hlt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Vergleiche mehrere Modelle systematisch\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# Modelle definieren\n",
        "models = {\n",
        "    'K-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "# Ergebnisse sammeln\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # TODO: Trainiere jedes Modell\n",
        "    # TODO: Mache Vorhersagen\n",
        "    # TODO: Berechne alle Metriken\n",
        "    \n",
        "    # Metriken berechnen\n",
        "    acc = # TODO\n",
        "    prec = # TODO\n",
        "    rec = # TODO\n",
        "    f1 = # TODO\n",
        "    auc = # TODO\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': acc,\n",
        "        'Precision': prec,\n",
        "        'Recall': rec,\n",
        "        'F1-Score': f1,\n",
        "        'AUC': auc\n",
        "    })\n",
        "\n",
        "# Ergebnisse als DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Frage: Welches Modell ist am besten? Warum?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù W4: Hyperparameter Tuning mit GridSearchCV\n",
        "\n",
        "### Aufgabe: Finde optimale Hyperparameter f√ºr Random Forest\n",
        "\n",
        "**Grid Search:** Systematisches Testen aller Hyperparameter-Kombinationen\n",
        "\n",
        "**Cross-Validation:** Mehrfache Train-Test Splits f√ºr robuste Bewertung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Hyperparameter Tuning mit GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    # TODO: F√ºlle die Parameter aus\n",
        "    # estimator=rf_model,\n",
        "    # param_grid=param_grid,\n",
        "    # cv=5,\n",
        "    # scoring='f1',\n",
        "    # n_jobs=-1\n",
        ")\n",
        "\n",
        "# Training\n",
        "# TODO: grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Beste Parameter\n",
        "print(\"Beste Parameter:\", grid_search.best_params_)\n",
        "print(f\"Beste F1-Score (CV): {grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Test auf Test-Set\n",
        "best_model = grid_search.best_estimator_\n",
        "# TODO: y_pred = best_model.predict(X_test)\n",
        "# TODO: Berechne Test-Metriken\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù W5: Fairness & Bias Detection im Titanic-Modell\n",
        "\n",
        "### Aufgabe: Analysiere ob dein Modell fair gegen√ºber verschiedenen Gruppen ist\n",
        "\n",
        "**Fairness Metrics:**\n",
        "- **Demographic Parity:** Vorhersagerate sollte √ºber Gruppen gleich sein\n",
        "- **Equal Opportunity:** TPR sollte √ºber Gruppen gleich sein\n",
        "- **Equalized Odds:** TPR und FPR sollten √ºber Gruppen gleich sein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Fairness-Analyse nach Geschlecht\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Trainiere ein Modell (z.B. Logistic Regression)\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "# TODO: Trainiere das Modell\n",
        "# TODO: Mache Vorhersagen auf Test-Set\n",
        "\n",
        "# Analysiere nach Geschlecht\n",
        "genders = ['male', 'female']\n",
        "\n",
        "for gender in genders:\n",
        "    # Filter Test-Daten nach Geschlecht\n",
        "    gender_mask = df_clean.loc[X_test.index, 'sex'] == gender\n",
        "    y_test_gender = y_test[gender_mask]\n",
        "    y_pred_gender = y_pred[gender_mask]\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test_gender, y_pred_gender)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # Metriken berechnen\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{gender.upper()} Group:\")\n",
        "    print(f\"  Sample Size: {len(y_test_gender)}\")\n",
        "    print(f\"  True Positive Rate (Recall): {tpr:.3f}\")\n",
        "    print(f\"  False Positive Rate: {fpr:.3f}\")\n",
        "    print(f\"  Precision: {precision:.3f}\")\n",
        "    print(f\"  Predicted Survival Rate: {y_pred_gender.mean():.3f}\")\n",
        "\n",
        "# Diskussion: Ist das Modell fair? Welche Unterschiede siehst du?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Zusammenfassung & Reflexion\n",
        "\n",
        "### Haupterkenntnisse von Tag 8:\n",
        "\n",
        "‚úÖ **Bias-Variance Tradeoff:** Balance zwischen Underfitting und Overfitting  \n",
        "‚úÖ **Klassifikationsmetriken:** Accuracy, Precision, Recall, F1-Score, AUC  \n",
        "‚úÖ **Regression Metriken:** MAE, MSE, RMSE, R¬≤  \n",
        "‚úÖ **Confusion Matrix:** Basis f√ºr alle Klassifikationsmetriken  \n",
        "‚úÖ **Type I vs II Errors:** Kontext bestimmt welcher Fehler schlimmer ist  \n",
        "‚úÖ **ROC/AUC:** Visualisierung der Modellperformance  \n",
        "‚úÖ **Ethisches AI:** Fairness, Transparency, Accountability\n",
        "\n",
        "### Wichtig f√ºrs Abschlussprojekt:\n",
        "\n",
        "- **W√§hle Metriken** die zu deinem Problem passen\n",
        "- **Cross-Validation** f√ºr robuste Evaluierung\n",
        "- **Teste auf Fairness** √ºber verschiedene Gruppen\n",
        "- **Dokumentiere** warum du bestimmte Modelle gew√§hlt hast\n",
        "- **Erkl√§re** deine Ergebnisse verst√§ndlich\n",
        "\n",
        "### Reflexionsfragen:\n",
        "\n",
        "1. **Welche Metrik ist f√ºr dein Projekt am wichtigsten? Warum?**\n",
        "   \n",
        "   ___________________________________\n",
        "\n",
        "2. **Wie k√∂nntest du Bias in deinem Projekt-Modell erkennen?**\n",
        "   \n",
        "   ___________________________________\n",
        "\n",
        "3. **Was w√ºrdest du bei Type I vs. Type II Errors priorisieren?**\n",
        "   \n",
        "   ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìö Weiterf√ºhrende Ressourcen\n",
        "\n",
        "### Empfohlene Materialien:\n",
        "\n",
        "**Scikit-Learn Dokumentation:**\n",
        "- [Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "- [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "- [Hyperparameter Tuning](https://scikit-learn.org/stable/modules/grid_search.html)\n",
        "\n",
        "**Videos & Tutorials:**\n",
        "- StatQuest: ROC and AUC Explained\n",
        "- Google's ML Fairness Course\n",
        "- EU AI Act Overview\n",
        "\n",
        "**Tools:**\n",
        "- [Fairlearn](https://fairlearn.org/) - Fairness Assessment & Mitigation\n",
        "- [SHAP](https://shap.readthedocs.io/) - Model Explainability\n",
        "- [What-If Tool](https://pair-code.github.io/what-if-tool/) - Interactive Model Analysis\n",
        "\n",
        "**Ethical AI:**\n",
        "- [EU Ethics Guidelines for Trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n",
        "- [Google's AI Principles](https://ai.google/principles/)\n",
        "- [Microsoft's Responsible AI](https://www.microsoft.com/en-us/ai/responsible-ai)\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ Herzlichen Gl√ºckwunsch!\n",
        "\n",
        "Du hast alle 4 Vorlesungstage abgeschlossen:\n",
        "- ‚úÖ Tag 2: Intro to Data Science\n",
        "- ‚úÖ Tag 4: Data Preparation\n",
        "- ‚úÖ Tag 6: Machine Learning\n",
        "- ‚úÖ Tag 8: Model Evaluation & Ethical AI\n",
        "\n",
        "**Du bist jetzt bereit f√ºr das Abschlussprojekt! üöÄ**\n",
        "\n",
        "**Tipps f√ºr dein Projekt:**\n",
        "1. W√§hle ein Dataset das dich interessiert\n",
        "2. Wende alle gelernten Techniken an (Preparation ‚Üí ML ‚Üí Evaluation)\n",
        "3. Achte auf Fairness und ethische Aspekte\n",
        "4. Dokumentiere deine Entscheidungen\n",
        "5. Erz√§hle eine Geschichte mit deinen Daten\n",
        "\n",
        "**Viel Erfolg! üí™**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
