{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 8: Model Evaluation & Assessment\n",
        "\n",
        "**Duration:** 90 minutes  \n",
        "**Dataset:** Titanic Passenger Data\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand bias vs variance tradeoff\n",
        "- Identify overfitting and underfitting\n",
        "- Apply regression metrics (MSE, RMSE, MAE, R²)\n",
        "- Apply classification metrics (Precision, Recall, Accuracy, F1-Score)\n",
        "- Interpret confusion matrices\n",
        "- Understand ROC/AUC curves\n",
        "- Differentiate Type 1 vs Type 2 errors\n",
        "- Understand ethical AI considerations\n",
        "- Recognize Trustworthy AI principles\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Data Loading (5 mins)\n",
        "\n",
        "### Introduction to Model Evaluation\n",
        "\n",
        "Building a machine learning model is only half the battle. The real question is: **How good is our model?**\n",
        "\n",
        "Today we'll learn how to:\n",
        "- Measure model performance objectively\n",
        "- Identify when models are too simple or too complex\n",
        "- Choose the right metrics for different problems\n",
        "- Ensure our AI systems are trustworthy and ethical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(f\"Survival rate: {df['survived'].mean()*100:.1f}%\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "# Select features and handle missing values\n",
        "df_clean = df.copy()\n",
        "df_clean['age'].fillna(df_clean['age'].median(), inplace=True)\n",
        "df_clean['embarked'].fillna(df_clean['embarked'].mode()[0], inplace=True)\n",
        "df_clean['fare'].fillna(df_clean['fare'].median(), inplace=True)\n",
        "\n",
        "# Create features\n",
        "df_clean['sex_encoded'] = df_clean['sex'].map({'male': 1, 'female': 0})\n",
        "df_clean['family_size'] = df_clean['sibsp'] + df_clean['parch'] + 1\n",
        "df_clean['is_alone'] = (df_clean['family_size'] == 1).astype(int)\n",
        "\n",
        "# Select final features\n",
        "features = ['pclass', 'sex_encoded', 'age', 'fare', 'family_size', 'is_alone']\n",
        "X = df_clean[features]\n",
        "y = df_clean['survived']\n",
        "\n",
        "print(\"Features prepared:\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Train-Test Split & The Golden Rule (8 mins)\n",
        "\n",
        "### The Golden Rule of Machine Learning\n",
        "\n",
        "**Never test on your training data!** \n",
        "\n",
        "Think of it like studying for an exam:\n",
        "- **Training Set:** Practice problems you study from\n",
        "- **Test Set:** The actual exam with new questions\n",
        "\n",
        "If you memorize the practice problems without understanding, you'll fail the real exam!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Split data into training and testing sets (80/20 split)\n",
        "# Hint: Use train_test_split with test_size=0.2 and random_state=42\n",
        "\n",
        "X_train, X_test, y_train, y_test = # YOUR CODE HERE\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining set survival rate: {y_train.mean()*100:.1f}%\")\n",
        "print(f\"Test set survival rate: {y_test.mean()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Why do we use `random_state=42`?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Bias-Variance Tradeoff (12 mins)\n",
        "\n",
        "### Understanding Bias and Variance\n",
        "\n",
        "**Bias:** How far off are our predictions on average?\n",
        "- High bias = Underfitting (model too simple)\n",
        "- Model misses important patterns\n",
        "\n",
        "**Variance:** How much do predictions vary with different training data?\n",
        "- High variance = Overfitting (model too complex)\n",
        "- Model memorizes training data, including noise\n",
        "\n",
        "**The Sweet Spot:** Balance between bias and variance!\n",
        "\n",
        "```\n",
        "High Bias (Underfitting)  →  Sweet Spot  →  High Variance (Overfitting)\n",
        "Too Simple                                   Too Complex\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.1: Training Models with Different Complexities\n",
        "\n",
        "Let's train 3 models with varying complexity:\n",
        "1. **Simple:** Logistic Regression (linear, low complexity)\n",
        "2. **Medium:** Decision Tree with limited depth\n",
        "3. **Complex:** Decision Tree with no depth limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Simple - Logistic Regression\n",
        "model_simple = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_simple.fit(X_train, y_train)\n",
        "\n",
        "# Model 2: Medium - Decision Tree (max_depth=3)\n",
        "model_medium = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_medium.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Model 3: Complex - Decision Tree (no depth limit)\n",
        "# Hint: Use DecisionTreeClassifier with no max_depth parameter\n",
        "model_complex = # YOUR CODE HERE\n",
        "model_complex.fit(X_train, y_train)\n",
        "\n",
        "print(\"✓ All models trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare training vs test accuracy\n",
        "models = {\n",
        "    'Simple (Logistic)': model_simple,\n",
        "    'Medium (Tree depth=3)': model_medium,\n",
        "    'Complex (Tree unlimited)': model_complex\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    gap = train_acc - test_acc\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train Accuracy': train_acc,\n",
        "        'Test Accuracy': test_acc,\n",
        "        'Gap': gap\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(results_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the bias-variance tradeoff\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    name='Training Accuracy',\n",
        "    x=results_df['Model'],\n",
        "    y=results_df['Train Accuracy'],\n",
        "    marker_color='lightblue'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    name='Test Accuracy',\n",
        "    x=results_df['Model'],\n",
        "    y=results_df['Test Accuracy'],\n",
        "    marker_color='darkblue'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Bias-Variance Tradeoff: Training vs Test Performance',\n",
        "    xaxis_title='Model Complexity',\n",
        "    yaxis_title='Accuracy',\n",
        "    barmode='group',\n",
        "    yaxis_tickformat='.0%'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Which model shows signs of overfitting? How can you tell?\n",
        "\n",
        "Your answer: ___________________________________\n",
        "\n",
        "**Question:** Which model would you choose and why?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.2: Learning Curves\n",
        "\n",
        "Learning curves show how model performance changes with training set size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate learning curve for the medium complexity model\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    model_medium, X_train, y_train, \n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    cv=5, random_state=42\n",
        ")\n",
        "\n",
        "# Calculate mean and std\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=train_sizes, y=train_mean,\n",
        "    name='Training Score',\n",
        "    mode='lines+markers',\n",
        "    line=dict(color='lightblue')\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=train_sizes, y=test_mean,\n",
        "    name='Cross-validation Score',\n",
        "    mode='lines+markers',\n",
        "    line=dict(color='darkblue')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Learning Curve: How Performance Changes with Training Data Size',\n",
        "    xaxis_title='Training Set Size',\n",
        "    yaxis_title='Accuracy',\n",
        "    yaxis_tickformat='.0%'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** What happens to the gap between training and test scores as we add more data?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Regression Metrics (10 mins)\n",
        "\n",
        "While our main task is classification (survived or not), let's quickly understand regression metrics by predicting a continuous variable: **fare**.\n",
        "\n",
        "### Common Regression Metrics\n",
        "\n",
        "**1. MAE (Mean Absolute Error)**\n",
        "- Average absolute difference between predictions and actual values\n",
        "- Easy to interpret: \"On average, we're off by X units\"\n",
        "- Formula: `MAE = mean(|actual - predicted|)`\n",
        "\n",
        "**2. MSE (Mean Squared Error)**\n",
        "- Average of squared errors\n",
        "- Penalizes large errors more heavily\n",
        "- Formula: `MSE = mean((actual - predicted)²)`\n",
        "\n",
        "**3. RMSE (Root Mean Squared Error)**\n",
        "- Square root of MSE\n",
        "- Same units as the target variable\n",
        "- Formula: `RMSE = √MSE`\n",
        "\n",
        "**4. R² (R-Squared / Coefficient of Determination)**\n",
        "- Percentage of variance explained by the model\n",
        "- Range: 0 to 1 (higher is better)\n",
        "- 0.8 = model explains 80% of variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a regression problem: predict fare from other features\n",
        "X_reg = df_clean[['pclass', 'sex_encoded', 'age', 'family_size']].copy()\n",
        "y_reg = df_clean['fare'].copy()\n",
        "\n",
        "# Split the data\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a linear regression model\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_reg = reg_model.predict(X_test_reg)\n",
        "\n",
        "print(\"✓ Regression model trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Calculate regression metrics\n",
        "# Hint: Use mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "mae = # YOUR CODE HERE\n",
        "mse = # YOUR CODE HERE\n",
        "rmse = # YOUR CODE HERE (np.sqrt(mse))\n",
        "r2 = # YOUR CODE HERE\n",
        "\n",
        "print(\"Regression Metrics:\")\n",
        "print(f\"MAE:  £{mae:.2f} (on average, off by this much)\")\n",
        "print(f\"MSE:  {mse:.2f}\")\n",
        "print(f\"RMSE: £{rmse:.2f} (similar to MAE but penalizes large errors)\")\n",
        "print(f\"R²:   {r2:.3f} (model explains {r2*100:.1f}% of variance)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual values\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Actual Fare': y_test_reg,\n",
        "    'Predicted Fare': y_pred_reg,\n",
        "    'Error': y_test_reg - y_pred_reg\n",
        "})\n",
        "\n",
        "fig = px.scatter(comparison_df, x='Actual Fare', y='Predicted Fare',\n",
        "                 title='Actual vs Predicted Fare',\n",
        "                 labels={'Actual Fare': 'Actual Fare (£)', 'Predicted Fare': 'Predicted Fare (£)'})\n",
        "\n",
        "# Add perfect prediction line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, comparison_df['Actual Fare'].max()],\n",
        "    y=[0, comparison_df['Actual Fare'].max()],\n",
        "    mode='lines',\n",
        "    name='Perfect Prediction',\n",
        "    line=dict(color='red', dash='dash')\n",
        "))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** What does it mean if points are far from the red line?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Classification Metrics (15 mins)\n",
        "\n",
        "Now back to our main task: predicting survival (classification).\n",
        "\n",
        "### The Four Outcomes of Binary Classification\n",
        "\n",
        "```\n",
        "                    Predicted\n",
        "                 No  |  Yes\n",
        "          No    TN  |  FP   (Type I Error)\n",
        "Actual    \n",
        "          Yes   FN  |  TP   (Type II Error)\n",
        "```\n",
        "\n",
        "- **TP (True Positive):** Correctly predicted survival ✓\n",
        "- **TN (True Negative):** Correctly predicted death ✓\n",
        "- **FP (False Positive):** Predicted survival, but died ✗ (Type I Error)\n",
        "- **FN (False Negative):** Predicted death, but survived ✗ (Type II Error)\n",
        "\n",
        "### Key Metrics\n",
        "\n",
        "**Accuracy:** Overall correctness\n",
        "- Formula: `(TP + TN) / Total`\n",
        "- Good when classes are balanced\n",
        "\n",
        "**Precision:** Of all positive predictions, how many were correct?\n",
        "- Formula: `TP / (TP + FP)`\n",
        "- \"When I predict survival, how often am I right?\"\n",
        "\n",
        "**Recall (Sensitivity):** Of all actual positives, how many did we catch?\n",
        "- Formula: `TP / (TP + FN)`\n",
        "- \"Of all survivors, how many did I identify?\"\n",
        "\n",
        "**F1-Score:** Harmonic mean of Precision and Recall\n",
        "- Formula: `2 × (Precision × Recall) / (Precision + Recall)`\n",
        "- Balanced metric when you care about both Precision and Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a model for classification\n",
        "clf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf_model.predict(X_test)\n",
        "\n",
        "print(\"✓ Classification model trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Calculate classification metrics\n",
        "# Hint: Use accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = # YOUR CODE HERE\n",
        "precision = # YOUR CODE HERE\n",
        "recall = # YOUR CODE HERE\n",
        "f1 = # YOUR CODE HERE\n",
        "\n",
        "print(\"Classification Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}% correct overall)\")\n",
        "print(f\"Precision: {precision:.3f} (when we predict survival, we're right {precision*100:.1f}% of the time)\")\n",
        "print(f\"Recall:    {recall:.3f} (we catch {recall*100:.1f}% of actual survivors)\")\n",
        "print(f\"F1-Score:  {f1:.3f} (balanced metric)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5.1: Understanding the Tradeoff\n",
        "\n",
        "**Scenario:** You're designing an alarm system for detecting icebergs.\n",
        "\n",
        "- **High Precision:** Alarm rarely goes off falsely, but might miss some icebergs\n",
        "- **High Recall:** Catches all icebergs, but many false alarms\n",
        "\n",
        "**Question:** For iceberg detection, would you prioritize Precision or Recall? Why?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Confusion Matrix (10 mins)\n",
        "\n",
        "A confusion matrix shows all four outcomes (TP, TN, FP, FN) in a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create confusion matrix\n",
        "# Hint: Use confusion_matrix(y_test, y_pred)\n",
        "\n",
        "cm = # YOUR CODE HERE\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"True Negatives (TN):  {cm[0,0]} - Correctly predicted died\")\n",
        "print(f\"False Positives (FP): {cm[0,1]} - Predicted survived but died (Type I Error)\")\n",
        "print(f\"False Negatives (FN): {cm[1,0]} - Predicted died but survived (Type II Error)\")\n",
        "print(f\"True Positives (TP):  {cm[1,1]} - Correctly predicted survived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrix\n",
        "fig = px.imshow(cm, \n",
        "                labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
        "                x=['Died (0)', 'Survived (1)'],\n",
        "                y=['Died (0)', 'Survived (1)'],\n",
        "                text_auto=True,\n",
        "                color_continuous_scale='Blues',\n",
        "                title='Confusion Matrix')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.1: Manual Calculation\n",
        "\n",
        "Let's verify our metrics using the confusion matrix values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Calculate metrics manually from confusion matrix\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "# Calculate accuracy\n",
        "manual_accuracy = # YOUR CODE HERE (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Calculate precision\n",
        "manual_precision = # YOUR CODE HERE TP / (TP + FP)\n",
        "\n",
        "# Calculate recall\n",
        "manual_recall = # YOUR CODE HERE TP / (TP + FN)\n",
        "\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"Accuracy:  {manual_accuracy:.3f}\")\n",
        "print(f\"Precision: {manual_precision:.3f}\")\n",
        "print(f\"Recall:    {manual_recall:.3f}\")\n",
        "\n",
        "print(\"\\nVerification (should match earlier calculations):\")\n",
        "print(f\"Accuracy matches:  {np.isclose(manual_accuracy, accuracy)}\")\n",
        "print(f\"Precision matches: {np.isclose(manual_precision, precision)}\")\n",
        "print(f\"Recall matches:    {np.isclose(manual_recall, recall)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.2: Classification Report\n",
        "\n",
        "Scikit-learn provides a convenient summary of all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Generate classification report\n",
        "# Hint: Use classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(# YOUR CODE HERE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Type I vs Type II Errors (8 mins)\n",
        "\n",
        "### Understanding Error Types\n",
        "\n",
        "**Type I Error (False Positive):**\n",
        "- Predicting something that isn't true\n",
        "- Example: Diagnosing a healthy person as sick\n",
        "- Example: Spam filter marking important email as spam\n",
        "\n",
        "**Type II Error (False Negative):**\n",
        "- Missing something that is true\n",
        "- Example: Failing to diagnose a sick person\n",
        "- Example: Spam filter letting spam through\n",
        "\n",
        "### Medical Testing Example\n",
        "\n",
        "```\n",
        "Disease Testing:\n",
        "- Type I Error (FP):  Telling healthy person they're sick\n",
        "- Type II Error (FN): Telling sick person they're healthy\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7.1: Real-World Scenarios\n",
        "\n",
        "For each scenario, identify which error type is worse:\n",
        "\n",
        "**Scenario 1: Cancer Screening**\n",
        "- Type I Error: Healthy person told they have cancer (leads to stress, more tests)\n",
        "- Type II Error: Cancer patient told they're healthy (cancer goes untreated)\n",
        "\n",
        "Which is worse? ___________________________________\n",
        "\n",
        "**Scenario 2: Credit Card Fraud Detection**\n",
        "- Type I Error: Legitimate transaction blocked (customer inconvenience)\n",
        "- Type II Error: Fraudulent transaction approved (financial loss)\n",
        "\n",
        "Which is worse? ___________________________________\n",
        "\n",
        "**Scenario 3: Spam Email Filter**\n",
        "- Type I Error: Important email goes to spam (might miss it)\n",
        "- Type II Error: Spam email in inbox (minor annoyance)\n",
        "\n",
        "Which is worse? ___________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate error rates from our Titanic model\n",
        "type1_error_rate = FP / (FP + TN)  # False Positive Rate\n",
        "type2_error_rate = FN / (FN + TP)  # False Negative Rate\n",
        "\n",
        "print(\"Error Analysis for Titanic Survival Prediction:\")\n",
        "print(f\"\\nType I Error Rate (False Positive):  {type1_error_rate:.3f}\")\n",
        "print(f\"  - We predicted {FP} people would survive but they didn't\")\n",
        "print(f\"  - That's {type1_error_rate*100:.1f}% of actual non-survivors\")\n",
        "\n",
        "print(f\"\\nType II Error Rate (False Negative): {type2_error_rate:.3f}\")\n",
        "print(f\"  - We predicted {FN} people would die but they survived\")\n",
        "print(f\"  - That's {type2_error_rate*100:.1f}% of actual survivors\")\n",
        "\n",
        "print(\"\\nQuestion: In a real disaster scenario, which error would be worse?\")\n",
        "print(\"Type I (predicting survival when they don't) or Type II (predicting death when they survive)?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: ROC Curve & AUC (12 mins)\n",
        "\n",
        "### Understanding ROC Curves\n",
        "\n",
        "**ROC (Receiver Operating Characteristic) Curve:**\n",
        "- Shows tradeoff between True Positive Rate and False Positive Rate\n",
        "- X-axis: False Positive Rate (Type I Error)\n",
        "- Y-axis: True Positive Rate (Recall)\n",
        "\n",
        "**AUC (Area Under Curve):**\n",
        "- Summary metric: area under the ROC curve\n",
        "- Range: 0.5 to 1.0\n",
        "- 0.5 = Random guessing (coin flip)\n",
        "- 1.0 = Perfect classifier\n",
        "- 0.7-0.8 = Acceptable\n",
        "- 0.8-0.9 = Excellent\n",
        "- 0.9+ = Outstanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get probability predictions (not just 0/1)\n",
        "y_pred_proba = clf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# TODO: Calculate ROC curve\n",
        "# Hint: Use roc_curve(y_test, y_pred_proba)\n",
        "fpr, tpr, thresholds = # YOUR CODE HERE\n",
        "\n",
        "# TODO: Calculate AUC\n",
        "# Hint: Use roc_auc_score(y_test, y_pred_proba)\n",
        "auc = # YOUR CODE HERE\n",
        "\n",
        "print(f\"AUC Score: {auc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC Curve\n",
        "fig = go.Figure()\n",
        "\n",
        "# ROC curve\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=fpr, y=tpr,\n",
        "    mode='lines',\n",
        "    name=f'ROC Curve (AUC = {auc:.3f})',\n",
        "    line=dict(color='blue', width=2)\n",
        "))\n",
        "\n",
        "# Random classifier line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1], y=[0, 1],\n",
        "    mode='lines',\n",
        "    name='Random Classifier (AUC = 0.5)',\n",
        "    line=dict(color='red', dash='dash')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='ROC Curve: True Positive Rate vs False Positive Rate',\n",
        "    xaxis_title='False Positive Rate (Type I Error)',\n",
        "    yaxis_title='True Positive Rate (Recall)',\n",
        "    xaxis=dict(range=[0, 1]),\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    width=700, height=700\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** What does it mean if the ROC curve is close to the top-left corner?\n",
        "\n",
        "Your answer: ___________________________________\n",
        "\n",
        "**Question:** What does it mean if the ROC curve follows the diagonal red line?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 8.1: Comparing Multiple Models\n",
        "\n",
        "Let's compare ROC curves for different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multiple models\n",
        "models_to_compare = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for name, model in models_to_compare.items():\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Get predictions\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc_score = roc_auc_score(y_test, y_proba)\n",
        "    \n",
        "    # Add to plot\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=fpr, y=tpr,\n",
        "        mode='lines',\n",
        "        name=f'{name} (AUC = {auc_score:.3f})'\n",
        "    ))\n",
        "\n",
        "# Add random classifier\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1], y=[0, 1],\n",
        "    mode='lines',\n",
        "    name='Random (AUC = 0.5)',\n",
        "    line=dict(color='red', dash='dash')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='ROC Curves: Model Comparison',\n",
        "    xaxis_title='False Positive Rate',\n",
        "    yaxis_title='True Positive Rate',\n",
        "    width=700, height=700\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Which model performs best? How do you know?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 9: Cross-Validation (8 mins)\n",
        "\n",
        "### The Problem with Single Train-Test Split\n",
        "\n",
        "What if our test set happened to be particularly easy or hard? Our results might be misleading!\n",
        "\n",
        "### Solution: K-Fold Cross-Validation\n",
        "\n",
        "1. Split data into K parts (folds)\n",
        "2. Train K times, each time using a different fold as test set\n",
        "3. Average the results\n",
        "\n",
        "```\n",
        "Fold 1: [Test][Train][Train][Train][Train]\n",
        "Fold 2: [Train][Test][Train][Train][Train]\n",
        "Fold 3: [Train][Train][Test][Train][Train]\n",
        "Fold 4: [Train][Train][Train][Test][Train]\n",
        "Fold 5: [Train][Train][Train][Train][Test]\n",
        "```\n",
        "\n",
        "This gives us more reliable performance estimates!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Perform 5-fold cross-validation\n",
        "# Hint: Use cross_val_score with cv=5\n",
        "\n",
        "cv_scores = # YOUR CODE HERE (cross_val_score(clf_model, X, y, cv=5))\n",
        "\n",
        "print(\"Cross-Validation Results (5 folds):\")\n",
        "print(f\"Scores: {cv_scores}\")\n",
        "print(f\"\\nMean Accuracy: {cv_scores.mean():.3f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.3f}\")\n",
        "print(f\"95% Confidence Interval: {cv_scores.mean():.3f} ± {1.96*cv_scores.std():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    x=[f'Fold {i+1}' for i in range(len(cv_scores))],\n",
        "    y=cv_scores,\n",
        "    marker_color='lightblue'\n",
        "))\n",
        "\n",
        "fig.add_hline(y=cv_scores.mean(), line_dash=\"dash\", line_color=\"red\",\n",
        "              annotation_text=f\"Mean: {cv_scores.mean():.3f}\")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Cross-Validation Scores Across 5 Folds',\n",
        "    xaxis_title='Fold',\n",
        "    yaxis_title='Accuracy',\n",
        "    yaxis_tickformat='.0%'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Why is cross-validation more reliable than a single train-test split?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 10: Ethical AI & Trustworthy AI (10 mins)\n",
        "\n",
        "### Why Ethics Matter in Machine Learning\n",
        "\n",
        "Our Titanic model makes life-or-death predictions. In real applications:\n",
        "- **Healthcare:** Diagnosis and treatment decisions\n",
        "- **Criminal Justice:** Risk assessment, sentencing\n",
        "- **Finance:** Loan approvals, credit scores\n",
        "- **Hiring:** Resume screening, candidate selection\n",
        "\n",
        "**Models can perpetuate and amplify human biases!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10.1: Detecting Bias in Our Model\n",
        "\n",
        "Let's check if our model treats men and women fairly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze model performance by gender\n",
        "results_by_gender = []\n",
        "\n",
        "for gender in [0, 1]:  # 0=female, 1=male\n",
        "    # Filter test set by gender\n",
        "    mask = X_test['sex_encoded'] == gender\n",
        "    X_test_gender = X_test[mask]\n",
        "    y_test_gender = y_test[mask]\n",
        "    \n",
        "    if len(y_test_gender) > 0:\n",
        "        # Make predictions\n",
        "        y_pred_gender = clf_model.predict(X_test_gender)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test_gender, y_pred_gender)\n",
        "        prec = precision_score(y_test_gender, y_pred_gender, zero_division=0)\n",
        "        rec = recall_score(y_test_gender, y_pred_gender, zero_division=0)\n",
        "        \n",
        "        results_by_gender.append({\n",
        "            'Gender': 'Female' if gender == 0 else 'Male',\n",
        "            'Count': len(y_test_gender),\n",
        "            'Accuracy': acc,\n",
        "            'Precision': prec,\n",
        "            'Recall': rec\n",
        "        })\n",
        "\n",
        "bias_df = pd.DataFrame(results_by_gender)\n",
        "print(\"Model Performance by Gender:\")\n",
        "print(bias_df.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize performance differences\n",
        "fig = go.Figure()\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall']\n",
        "for metric in metrics:\n",
        "    fig.add_trace(go.Bar(\n",
        "        name=metric,\n",
        "        x=bias_df['Gender'],\n",
        "        y=bias_df[metric]\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Model Performance by Gender',\n",
        "    xaxis_title='Gender',\n",
        "    yaxis_title='Score',\n",
        "    barmode='group',\n",
        "    yaxis_tickformat='.0%'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Does the model perform equally well for both genders? If not, what might be the reason?\n",
        "\n",
        "Your answer: ___________________________________\n",
        "\n",
        "**Question:** In the actual Titanic disaster, women and children were prioritized for lifeboats. Is this historical bias reflected in our model? Is that a problem?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10.2: Trustworthy AI Principles\n",
        "\n",
        "The EU's **Trustworthy AI Framework** outlines 7 key requirements:\n",
        "\n",
        "1. **Human Agency & Oversight**\n",
        "   - Humans should remain in control\n",
        "   - AI should augment, not replace, human decision-making\n",
        "\n",
        "2. **Technical Robustness & Safety**\n",
        "   - Models should be reliable and secure\n",
        "   - Should handle errors gracefully\n",
        "\n",
        "3. **Privacy & Data Governance**\n",
        "   - Respect user privacy\n",
        "   - Secure data handling\n",
        "\n",
        "4. **Transparency**\n",
        "   - Decisions should be explainable\n",
        "   - Users should understand how AI works\n",
        "\n",
        "5. **Diversity, Non-discrimination & Fairness**\n",
        "   - Avoid bias\n",
        "   - Ensure equal treatment\n",
        "\n",
        "6. **Societal & Environmental Wellbeing**\n",
        "   - Consider broader impact\n",
        "   - Sustainability\n",
        "\n",
        "7. **Accountability**\n",
        "   - Clear responsibility for AI decisions\n",
        "   - Mechanisms for redress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Exercise: Applying Trustworthy AI\n",
        "\n",
        "For each scenario, identify which Trustworthy AI principle(s) are most relevant:\n",
        "\n",
        "**Scenario 1:** A hospital uses an AI system to prioritize patients in the emergency room. The system is a \"black box\" - doctors don't understand why it makes certain recommendations.\n",
        "\n",
        "Which principles are violated? ___________________________________\n",
        "\n",
        "**Scenario 2:** A facial recognition system has 99% accuracy for white males but only 65% accuracy for dark-skinned females.\n",
        "\n",
        "Which principles are violated? ___________________________________\n",
        "\n",
        "**Scenario 3:** A loan approval AI system was trained on historical data that reflected past discriminatory lending practices.\n",
        "\n",
        "Which principles are violated? ___________________________________\n",
        "\n",
        "**Scenario 4:** An AI hiring tool automatically rejects candidates without giving them a chance to appeal or understand why.\n",
        "\n",
        "Which principles are violated? ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10.3: Feature Importance & Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Examine feature importance from our Random Forest model\n",
        "# This helps with transparency!\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': clf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Visualize\n",
        "fig = px.bar(feature_importance, x='Importance', y='Feature', \n",
        "             orientation='h',\n",
        "             title='Which Features Does Our Model Rely On Most?')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Is it ethical for a survival prediction model to heavily weight 'sex'? Why or why not?\n",
        "\n",
        "Your answer: ___________________________________\n",
        "\n",
        "**Question:** How would you explain this model's predictions to someone who's not a data scientist?\n",
        "\n",
        "Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary & Reflection (5 mins)\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "Today we learned:\n",
        "\n",
        "**Model Evaluation Fundamentals:**\n",
        "- ✓ Always split data into train/test sets\n",
        "- ✓ Bias-variance tradeoff: balance between underfitting and overfitting\n",
        "- ✓ Cross-validation provides more reliable estimates\n",
        "\n",
        "**Regression Metrics:**\n",
        "- ✓ MAE, MSE, RMSE, R² for continuous predictions\n",
        "\n",
        "**Classification Metrics:**\n",
        "- ✓ Accuracy, Precision, Recall, F1-Score\n",
        "- ✓ Confusion matrix shows all four outcomes\n",
        "- ✓ ROC/AUC curves visualize performance tradeoffs\n",
        "\n",
        "**Error Types:**\n",
        "- ✓ Type I Error (False Positive): Predicting yes when it's no\n",
        "- ✓ Type II Error (False Negative): Predicting no when it's yes\n",
        "- ✓ Different applications prioritize different error types\n",
        "\n",
        "**Ethics & Trust:**\n",
        "- ✓ Models can perpetuate bias\n",
        "- ✓ Trustworthy AI requires transparency, fairness, and accountability\n",
        "- ✓ Feature importance helps explain model decisions\n",
        "\n",
        "### Metric Selection Guide\n",
        "\n",
        "```\n",
        "Choose Accuracy when:\n",
        "  - Classes are balanced\n",
        "  - All errors are equally bad\n",
        "\n",
        "Choose Precision when:\n",
        "  - False positives are costly\n",
        "  - Example: Spam detection (don't want false alarms)\n",
        "\n",
        "Choose Recall when:\n",
        "  - False negatives are costly\n",
        "  - Example: Disease screening (don't miss cases)\n",
        "\n",
        "Choose F1-Score when:\n",
        "  - Need balance between Precision and Recall\n",
        "  - Classes are imbalanced\n",
        "\n",
        "Choose AUC when:\n",
        "  - Want a single metric for overall performance\n",
        "  - Comparing multiple models\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions\n",
        "\n",
        "1. **Understanding Metrics:** Which metric do you think is most important for the Titanic survival prediction task? Why?\n",
        "\n",
        "   Your answer: ___________________________________\n",
        "\n",
        "2. **Overfitting:** Describe in your own words why overfitting is a problem and how to detect it.\n",
        "\n",
        "   Your answer: ___________________________________\n",
        "\n",
        "3. **Real-World Application:** Think of a real-world ML application. What would be the cost of Type I vs Type II errors?\n",
        "\n",
        "   Your answer: ___________________________________\n",
        "\n",
        "4. **Ethics:** What's one thing you'll keep in mind about ethical AI when building models in the future?\n",
        "\n",
        "   Your answer: ___________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Bonus Challenges (Optional)\n",
        "\n",
        "If you finish early, try these additional exercises!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bonus 1: Precision-Recall Curve\n",
        "\n",
        "Similar to ROC curve, but plots Precision vs Recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a Precision-Recall curve\n",
        "# Hint: from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bonus 2: Stratified K-Fold\n",
        "\n",
        "Regular K-Fold might create imbalanced folds. Stratified K-Fold ensures each fold has the same class distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Compare regular K-Fold with Stratified K-Fold\n",
        "# Hint: from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bonus 3: Cost-Sensitive Learning\n",
        "\n",
        "What if false negatives are 10x worse than false positives? Adjust your model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train a model with class weights\n",
        "# Hint: Use class_weight parameter in LogisticRegression or RandomForestClassifier\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bonus 4: Fairness Metrics\n",
        "\n",
        "Calculate demographic parity and equalized odds to assess fairness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Calculate fairness metrics comparing male vs female\n",
        "# Demographic Parity: P(predicted=1 | male) should equal P(predicted=1 | female)\n",
        "# Equalized Odds: TPR and FPR should be equal across groups\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Resources for Further Learning\n",
        "\n",
        "### Documentation\n",
        "- **Scikit-learn Metrics:** https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "- **Confusion Matrix Guide:** https://en.wikipedia.org/wiki/Confusion_matrix\n",
        "- **ROC Curves Explained:** https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
        "\n",
        "### Ethics & Fairness\n",
        "- **EU Trustworthy AI:** https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\n",
        "- **Fairness in ML:** https://fairmlbook.org/\n",
        "- **Google's Responsible AI:** https://ai.google/responsibility/responsible-ai-practices/\n",
        "\n",
        "### Interactive Learning\n",
        "- **Confusion Matrix Calculator:** https://www.machinelearningplus.com/statistics/confusion-matrix-calculator/\n",
        "- **ROC Curve Demo:** https://arogozhnikov.github.io/2015/10/05/roc-curve.html\n",
        "\n",
        "**Congratulations on completing Day 8!** 🎉\n",
        "\n",
        "You now have the tools to rigorously evaluate machine learning models and ensure they're trustworthy and ethical!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
