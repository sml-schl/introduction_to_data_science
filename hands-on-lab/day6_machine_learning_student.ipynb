{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Introduction to Machine Learning\n",
    "\n",
    "**Duration:** 90 minutes  \n",
    "**Dataset:** Titanic Passenger Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Define machine learning and understand it as an optimization task\n",
    "- Differentiate supervised, unsupervised, and reinforcement learning\n",
    "- Apply train-test split and cross-validation\n",
    "- Understand classification tasks (K-NN, Decision Trees, Logistic Regression)\n",
    "- Understand regression for continuous predictions\n",
    "- Apply K-means clustering\n",
    "- Understand neural network basics (layers, activation functions, hyperparameters)\n",
    "- Understand loss functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading (5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, \n",
    "    mean_squared_error, r2_score, silhouette_score\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Was ist Machine Learning? (10 mins)\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Machine Learning** ist ein Teilgebiet der KI, das Computern ermÃ¶glicht, aus Daten zu lernen, ohne explizit programmiert zu werden.\n",
    "\n",
    "### ML als Optimierungsproblem\n",
    "\n",
    "Machine Learning basiert auf **Optimierung**:\n",
    "1. Definiere ein **Modell** (Funktion) mit Parametern\n",
    "2. Definiere eine **Loss-Funktion** (misst Fehler der Vorhersagen)\n",
    "3. Finde Parameter, die den **Loss minimieren** (Optimierung)\n",
    "\n",
    "### Die 3 Arten von Machine Learning\n",
    "\n",
    "**1. Supervised Learning (Ãœberwachtes Lernen)**\n",
    "- Lernen aus gelabelten Daten (Features + korrekte Antworten)\n",
    "- Ziel: Vorhersagen fÃ¼r neue Daten\n",
    "- Beispiele: Klassifikation (Ã¼berlebt/gestorben), Regression (Hauspreis)\n",
    "\n",
    "**2. Unsupervised Learning (UnÃ¼berwachtes Lernen)**\n",
    "- Lernen aus unlabeled Daten (nur Features, keine Antworten)\n",
    "- Ziel: Muster und Strukturen entdecken\n",
    "- Beispiele: Clustering (Gruppierung)\n",
    "\n",
    "**3. Reinforcement Learning**\n",
    "- Lernen durch Trial-and-Error\n",
    "- Heute nicht behandelt\n",
    "\n",
    "### ðŸŽ¯ VorlesungsÃ¼bung 2.1: Lerntypen identifizieren (3 mins)\n",
    "\n",
    "**Aufgabe:** Ordne jedes Szenario zu (Supervised oder Unsupervised):\n",
    "1. Titanic-Ãœberleben vorhersagen (mit Labels): _______________\n",
    "2. Passagiere nach Alter/Fahrpreis gruppieren (ohne Labels): _______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Datenvorbereitung fÃ¼r ML (10 mins)\n",
    "\n",
    "### ðŸŽ¯ VorlesungsÃ¼bung 3.1: Daten vorbereiten (7 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine saubere Kopie fÃ¼r ML\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Fehlende Werte behandeln\n",
    "df_ml['age'] = df_ml['age'].fillna(df_ml['age'].median())\n",
    "df_ml['embarked'] = df_ml['embarked'].fillna(df_ml['embarked'].mode()[0])\n",
    "df_ml['fare'] = df_ml['fare'].fillna(df_ml['fare'].median())\n",
    "\n",
    "print(\"âœ… Fehlende Werte behandelt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorische Variablen kodieren\n",
    "df_ml['sex_encoded'] = df_ml['sex'].map({'male': 1, 'female': 0})\n",
    "embarked_dummies = pd.get_dummies(df_ml['embarked'], prefix='embarked', drop_first=True)\n",
    "df_ml = pd.concat([df_ml, embarked_dummies], axis=1)\n",
    "\n",
    "# Feature Engineering\n",
    "df_ml['family_size'] = df_ml['sibsp'] + df_ml['parch'] + 1\n",
    "\n",
    "print(\"âœ… Features kodiert und erstellt!\")\n",
    "print(f\"ðŸ“Š FamiliengrÃ¶ÃŸen-Bereich: {df_ml['family_size'].min()} bis {df_ml['family_size'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Train-Test Split (10 mins)\n",
    "\n",
    "### Warum Daten aufteilen?\n",
    "\n",
    "```\n",
    "Originaldaten (100%)\n",
    "    |\n",
    "    â”œâ”€â”€ Trainings-Set (70-80%): Zum Trainieren des Modells\n",
    "    â””â”€â”€ Test-Set (20-30%): Zum Evaluieren des Modells\n",
    "```\n",
    "\n",
    "**Wichtig:** **NIEMALS** Test-Daten zum Training verwenden!\n",
    "\n",
    "### ðŸŽ¯ VorlesungsÃ¼bung 4.1: Daten aufteilen (5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WÃ¤hle Features fÃ¼r unser Modell\n",
    "feature_columns = ['pclass', 'sex_encoded', 'age', 'fare', 'family_size', \n",
    "                   'embarked_Q', 'embarked_S']\n",
    "\n",
    "X = df_ml[feature_columns]\n",
    "y = df_ml['survived']\n",
    "\n",
    "print(f\"ðŸ“Š Features (X): {X.shape}\")\n",
    "print(f\"ðŸ“Š Target (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teile Daten auf: 80% Training, 20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"ðŸ“Š Trainings-Set: {len(X_train)} Samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"ðŸ“Š Test-Set: {len(X_test)} Samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nâœ… Ãœberlebensrate Training: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"âœ… Ãœberlebensrate Test: {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "**Wichtig:** Scaler nur auf Trainings-Daten fitten, dann beide transformieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skaliere Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Nur transform, NICHT fit!\n",
    "\n",
    "print(\"âœ… Features skaliert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Klassifikation - K-NN (10 mins)\n",
    "\n",
    "### K-Nearest Neighbors (K-NN)\n",
    "\n",
    "**Funktionsweise:**\n",
    "1. Finde die K nÃ¤chsten Datenpunkte\n",
    "2. \"Abstimmung\" unter den K Nachbarn\n",
    "3. Zuordnung zur hÃ¤ufigsten Klasse\n",
    "\n",
    "**Analogie:** \"Du bist der Durchschnitt deiner 5 engsten Freunde\"\n",
    "\n",
    "### ðŸŽ¯ VorlesungsÃ¼bung 5.1: K-NN Modell trainieren (7 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainiere K-NN mit K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluieren\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"ðŸ“Š K-NN Genauigkeit: {accuracy_knn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Decision Trees (10 mins)\n",
    "\n",
    "### EntscheidungsbÃ¤ume\n",
    "\n",
    "**Funktionsweise:** Erstelle eine Serie von Ja/Nein-Fragen basierend auf Features\n",
    "\n",
    "```\n",
    "Ist Geschlecht == weiblich?\n",
    "  â”œâ”€ Ja â†’ Ist Klasse <= 2?\n",
    "  â”‚         â”œâ”€ Ja â†’ Ãœberlebt\n",
    "  â”‚         â””â”€ Nein â†’ PrÃ¼fe Alter...\n",
    "  â””â”€ Nein â†’ Ist Alter < 16?\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ VorlesungsÃ¼bung 6.1: Decision Tree (7 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainiere einen Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "# Evaluieren\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"ðŸ“Š Decision Tree Genauigkeit: {accuracy_dt*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere den Decision Tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt, \n",
    "          feature_names=feature_columns,\n",
    "          class_names=['Gestorben', 'Ãœberlebt'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Entscheidungsbaum fÃ¼r Titanic-Ãœberlebenvorhersage', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': dt.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig = px.bar(feature_importance, x='importance', y='feature', orientation='h',\n",
    "             title='Feature Importance im Decision Tree')\n",
    "fig.show()\n",
    "\n",
    "print(\"ðŸ“Š Wichtigste Features:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Modellvergleich (10 mins)\n",
    "\n",
    "### ðŸŽ¯ VorlesungsÃ¼bung 7.1: Logistic Regression & Vergleich (7 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainiere Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluieren\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"ðŸ“Š Logistic Regression Genauigkeit: {accuracy_lr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell-Vergleich\n",
    "models = {\n",
    "    'K-NN': y_pred_knn,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'Logistic Regression': y_pred_lr\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, y_pred in models.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append({'Modell': model_name, 'Genauigkeit': accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"ðŸ“Š Modell-Vergleich:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualisierung\n",
    "fig = px.bar(results_df, x='Modell', y='Genauigkeit',\n",
    "             title='Vergleich der Klassifikationsmodelle')\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# ðŸ  WIEDERHOLUNG & VERTIEFUNG (FÃ¼r zu Hause oder Nachbereitung)\n",
    "\n",
    "---\n",
    "## Wiederholungsblock 1: K-NN mit verschiedenen K-Werten (20 mins)\n",
    "\n",
    "### ðŸ“ Ãœbung W1.1: Optimales K finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Teste verschiedene K-Werte und finde das Optimum\n",
    "k_values = [1, 3, 5, 7, 9, 15, 25, 50]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    # DEIN CODE HIER\n",
    "    # Trainiere KNN mit n_neighbors=k\n",
    "    # Mache Vorhersagen auf Test-Set\n",
    "    # Berechne Accuracy und fÃ¼ge zu accuracies hinzu\n",
    "    pass\n",
    "\n",
    "# Visualisiere Ergebnisse\n",
    "fig = px.line(x=k_values, y=accuracies, markers=True,\n",
    "              title='K-NN Performance vs K-Wert',\n",
    "              labels={'x': 'K (Anzahl Nachbarn)', 'y': 'Genauigkeit'})\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()\n",
    "\n",
    "print(f\"Bestes K: {k_values[np.argmax(accuracies)]} mit Genauigkeit: {max(accuracies)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Wiederholungsblock 2: Erweiterte Metriken (25 mins)\n",
    "\n",
    "### ðŸ“ Ãœbung W2.1: Confusion Matrix erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Erstelle eine Confusion Matrix fÃ¼r Logistic Regression\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "fig = px.imshow(cm, \n",
    "                labels=dict(x=\"Vorhergesagt\", y=\"TatsÃ¤chlich\", color=\"Anzahl\"),\n",
    "                x=['Gestorben', 'Ãœberlebt'],\n",
    "                y=['Gestorben', 'Ãœberlebt'],\n",
    "                text_auto=True,\n",
    "                title='Confusion Matrix - Logistic Regression',\n",
    "                color_continuous_scale='Blues')\n",
    "fig.show()\n",
    "\n",
    "print(\"ðŸ“Š Confusion Matrix:\")\n",
    "print(f\"True Negatives (korrekt 'gestorben' vorhergesagt): {cm[0, 0]}\")\n",
    "print(f\"False Positives (fÃ¤lschlich 'Ã¼berlebt' vorhergesagt): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (fÃ¤lschlich 'gestorben' vorhergesagt): {cm[1, 0]}\")\n",
    "print(f\"True Positives (korrekt 'Ã¼berlebt' vorhergesagt): {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ Ãœbung W2.2: Precision, Recall, F1-Score berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Berechne alle Metriken fÃ¼r alle drei Modelle\n",
    "models = {\n",
    "    'K-NN': y_pred_knn,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'Logistic Regression': y_pred_lr\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, y_pred in models.items():\n",
    "    # DEIN CODE HIER\n",
    "    # Berechne: accuracy, precision, recall, f1\n",
    "    pass\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"ðŸ“Š Detaillierter Modellvergleich:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualisierung\n",
    "# DEIN CODE HIER (erstelle ein grouped bar chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Wiederholungsblock 3: Cross-Validation (20 mins)\n",
    "\n",
    "### ðŸ“ Ãœbung W3.1: 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FÃ¼hre 5-Fold Cross-Validation fÃ¼r Logistic Regression durch\n",
    "# Tipp: Verwende cross_val_score()\n",
    "\n",
    "lr_cv = LogisticRegression(max_iter=1000, random_state=42)\n",
    "cv_scores = # DEIN CODE HIER\n",
    "\n",
    "print(\"ðŸ“Š Cross-Validation Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i}: {score*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nâœ… Durchschnitt: {cv_scores.mean()*100:.2f}%\")\n",
    "print(f\"ðŸ“Š Standardabweichung: {cv_scores.std()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Wiederholungsblock 4: Regression (25 mins)\n",
    "\n",
    "### ðŸ“ Ãœbung W4.1: Linear Regression fÃ¼r Fahrpreisvorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Trainiere ein Linear Regression Modell zur Fahrpreisvorhersage\n",
    "# Features: pclass, sex_encoded, age, family_size\n",
    "# Target: fare\n",
    "\n",
    "regression_features = ['pclass', 'sex_encoded', 'age', 'family_size']\n",
    "X_reg = df_ml[regression_features]\n",
    "y_reg = df_ml['fare']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = # DEIN CODE HIER\n",
    "\n",
    "# Skalierung\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = # DEIN CODE HIER\n",
    "X_test_reg_scaled = # DEIN CODE HIER\n",
    "\n",
    "# Modell trainieren\n",
    "lin_reg = LinearRegression()\n",
    "# DEIN CODE HIER\n",
    "\n",
    "# Vorhersagen und Evaluierung\n",
    "y_pred_reg = # DEIN CODE HIER\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"ðŸ“Š MSE: {mse:.2f}\")\n",
    "print(f\"ðŸ“Š RMSE: {rmse:.2f} Â£\")\n",
    "print(f\"ðŸ“Š RÂ² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Wiederholungsblock 5: Clustering mit K-Means (25 mins)\n",
    "\n",
    "### ðŸ“ Ãœbung W5.1: K-Means Clustering anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wende K-Means Clustering an\n",
    "# Features: age, fare, pclass\n",
    "\n",
    "cluster_features = ['age', 'fare', 'pclass']\n",
    "X_cluster = df_ml[cluster_features]\n",
    "\n",
    "# Skalierung\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = # DEIN CODE HIER\n",
    "\n",
    "# K-Means mit K=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = # DEIN CODE HIER (fit_predict)\n",
    "\n",
    "df_ml['cluster'] = clusters\n",
    "\n",
    "print(\"ðŸ“Š Cluster-Verteilung:\")\n",
    "print(df_ml['cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nðŸ“Š Cluster-Charakteristika:\")\n",
    "print(df_ml.groupby('cluster')[cluster_features].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ Ãœbung W5.2: Optimales K mit Elbow-Methode finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finde optimales K mit Elbow-Methode\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    # DEIN CODE HIER\n",
    "    # Trainiere K-Means mit k Clustern\n",
    "    # Speichere inertia_ und silhouette_score\n",
    "    pass\n",
    "\n",
    "# Visualisiere Elbow Curve\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(K_range), y=inertias, mode='lines+markers',\n",
    "                         name='Inertia'))\n",
    "fig.update_layout(title='Elbow-Methode fÃ¼r optimales K',\n",
    "                  xaxis_title='Anzahl Cluster (K)',\n",
    "                  yaxis_title='Inertia')\n",
    "fig.show()\n",
    "\n",
    "print(f\"ðŸ’¡ Suche nach dem 'Ellbogen' in der Kurve!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Zusammenfassung & Reflexion\n",
    "\n",
    "### Haupterkenntnisse von Tag 6:\n",
    "\n",
    "âœ… **Machine Learning Typen:** Supervised, Unsupervised, Reinforcement Learning  \n",
    "âœ… **Train-Test Split:** NIEMALS auf Test-Daten trainieren!  \n",
    "âœ… **Klassifikations-Algorithmen:** K-NN, Decision Trees, Logistic Regression  \n",
    "âœ… **Regression:** Vorhersage kontinuierlicher Werte (Linear Regression)  \n",
    "âœ… **Clustering:** Gruppierung ohne Labels (K-Means)  \n",
    "âœ… **Cross-Validation:** Robustere Modell-Evaluierung  \n",
    "âœ… **Metriken:** Accuracy, Precision, Recall, F1, MSE, RMSE, RÂ²\n",
    "\n",
    "### Wichtig fÃ¼rs Projekt:\n",
    "- **Datenvorbereitung** ist entscheidend fÃ¼r gute Modelle\n",
    "- **Feature Engineering** aus Tag 4 verbessert ML-Performance\n",
    "- **Mehrere Modelle vergleichen** um das Beste zu finden\n",
    "- **Cross-Validation** fÃ¼r zuverlÃ¤ssige Bewertung\n",
    "\n",
    "### Reflexionsfragen:\n",
    "\n",
    "1. **Welcher Algorithmus hat dich am meisten Ã¼berrascht? Warum?**\n",
    "   \n",
    "   ___________________________________\n",
    "\n",
    "2. **Wann wÃ¼rdest du K-NN vs. Decision Trees vs. Logistic Regression verwenden?**\n",
    "   \n",
    "   ___________________________________\n",
    "\n",
    "3. **Wie kÃ¶nntest du ML im Abschlussprojekt anwenden?**\n",
    "   \n",
    "   ___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š WeiterfÃ¼hrende Ressourcen\n",
    "\n",
    "### Empfohlene Materialien fÃ¼r Zuhause:\n",
    "\n",
    "**Scikit-Learn Dokumentation:**\n",
    "- [K-Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "- [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [K-Means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "**Tutorials & Videos:**\n",
    "- StatQuest: Machine Learning Basics (YouTube)\n",
    "- Scikit-Learn Cheat Sheet\n",
    "- Kaggle Learn: Intro to Machine Learning\n",
    "\n",
    "**NÃ¤chste Schritte:**\n",
    "- âœ… Tag 8: Model Evaluation & Ethical AI\n",
    "- ðŸ“Š Abschlussprojekt: Wende ML auf eigene Daten an!\n",
    "\n",
    "---\n",
    "**Gut gemacht! ðŸŽ‰ Du hast die Grundlagen von Machine Learning gemeistert!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5: Support Vector Machines (SVM) - Conceptual Overview\n",
    "\n",
    "**How it works:**\n",
    "1. Find the hyperplane (decision boundary) that best separates classes\n",
    "2. Maximize the margin (distance) between the boundary and nearest points\n",
    "3. Support vectors are the critical points closest to the boundary\n",
    "\n",
    "**Key concept:** The \"kernel trick\" allows SVM to handle non-linear boundaries\n",
    "\n",
    "**Pros:** Effective in high dimensions, memory efficient  \n",
    "**Cons:** Slow for large datasets, requires feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train an SVM classifier (optional - can be slow)\n",
    "# Use a smaller dataset for demonstration\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_svm*100:.2f}%\")\n",
    "print(f\"Number of support vectors: {len(svm.support_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Cross-Validation (8 mins)\n",
    "\n",
    "### The Problem with Single Train-Test Split\n",
    "\n",
    "A single split might not be representative. What if test set is too easy or too hard?\n",
    "\n",
    "### Solution: K-Fold Cross-Validation\n",
    "\n",
    "1. Split data into K folds (e.g., 5 folds)\n",
    "2. Train K times, each time using a different fold as test set\n",
    "3. Average the results to get more reliable estimate\n",
    "\n",
    "```\n",
    "Fold 1: [Test][Train][Train][Train][Train]\n",
    "Fold 2: [Train][Test][Train][Train][Train]\n",
    "Fold 3: [Train][Train][Test][Train][Train]\n",
    "Fold 4: [Train][Train][Train][Test][Train]\n",
    "Fold 5: [Train][Train][Train][Train][Test]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform 5-fold cross-validation on Logistic Regression\n",
    "lr_cv = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use cross_val_score with cv=5\n",
    "cv_scores = cross_val_score(lr_cv, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-Validation Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i}: {score*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nMean CV Score: {cv_scores.mean()*100:.2f}%\")\n",
    "print(f\"Standard Deviation: {cv_scores.std()*100:.2f}%\")\n",
    "print(f\"95% Confidence Interval: [{(cv_scores.mean() - 2*cv_scores.std())*100:.2f}%, {(cv_scores.mean() + 2*cv_scores.std())*100:.2f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare CV scores for all models\n",
    "models_cv = {\n",
    "    'K-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=4, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "for model_name, model in models_cv.items():\n",
    "    # YOUR CODE HERE: perform cross-validation\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'Model': model_name,\n",
    "        'Mean CV Score': scores.mean(),\n",
    "        'Std Dev': scores.std()\n",
    "    })\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\nCross-Validation Comparison:\")\n",
    "print(cv_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Regression - Predicting Continuous Values (12 mins)\n",
    "\n",
    "### What is Regression?\n",
    "\n",
    "**Regression** is a supervised learning task where we predict **continuous numerical values**.\n",
    "\n",
    "Examples:\n",
    "- Predicting house prices\n",
    "- Forecasting temperature\n",
    "- Estimating ticket fare\n",
    "\n",
    "Today we'll predict the **fare** a passenger paid based on their characteristics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "# TODO: Create features (X) and target (y) for fare prediction\n",
    "regression_features = ['pclass', 'sex_encoded', 'age', 'family_size', \n",
    "                       'embarked_Q', 'embarked_S']\n",
    "\n",
    "X_reg = df_ml[regression_features]\n",
    "y_reg = df_ml['fare']  # Target is now 'fare' (continuous)\n",
    "\n",
    "print(f\"Target (fare) statistics:\")\n",
    "print(y_reg.describe())\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "**How it works:**\n",
    "- Finds the best-fit line (or hyperplane) through the data\n",
    "- Minimizes the sum of squared errors (distance from points to line)\n",
    "\n",
    "**Formula:** `y = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + b`\n",
    "\n",
    "Where:\n",
    "- `y` = predicted fare\n",
    "- `xâ‚, xâ‚‚, ...` = features (pclass, age, etc.)\n",
    "- `wâ‚, wâ‚‚, ...` = weights (coefficients)\n",
    "- `b` = bias (intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = lin_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Evaluate using regression metrics\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"\\nInterpretation: On average, predictions are off by Â£{rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Regression Metrics\n",
    "\n",
    "- **MSE (Mean Squared Error)**: Average squared difference between predictions and actual values\n",
    "  - Lower is better\n",
    "  - Units are squared (hard to interpret)\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**: Square root of MSE\n",
    "  - Lower is better\n",
    "  - Same units as target variable (easier to interpret)\n",
    "\n",
    "- **RÂ² (R-squared)**: Proportion of variance explained by the model\n",
    "  - Range: 0 to 1 (sometimes negative for very bad models)\n",
    "  - 1.0 = perfect predictions\n",
    "  - 0.0 = model is no better than predicting the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig = go.Figure()\n",
    "\n",
    "# Perfect predictions line\n",
    "max_fare = max(y_test_reg.max(), y_pred_reg.max())\n",
    "fig.add_trace(go.Scatter(x=[0, max_fare], y=[0, max_fare], \n",
    "                         mode='lines', name='Perfect Prediction',\n",
    "                         line=dict(color='red', dash='dash')))\n",
    "\n",
    "# Actual predictions\n",
    "fig.add_trace(go.Scatter(x=y_test_reg, y=y_pred_reg, \n",
    "                         mode='markers', name='Predictions',\n",
    "                         marker=dict(size=8, opacity=0.6)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Predicted vs Actual Fare (RÂ² = {r2:.3f})',\n",
    "    xaxis_title='Actual Fare (Â£)',\n",
    "    yaxis_title='Predicted Fare (Â£)',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coefficients for regression\n",
    "reg_coefficients = pd.DataFrame({\n",
    "    'feature': regression_features,\n",
    "    'coefficient': lin_reg.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "fig = px.bar(reg_coefficients, x='coefficient', y='feature', orientation='h',\n",
    "             title='Linear Regression Coefficients for Fare Prediction',\n",
    "             color='coefficient',\n",
    "             color_continuous_scale='RdBu_r')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nFeature coefficients:\")\n",
    "print(reg_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Unsupervised Learning - K-Means Clustering (10 mins)\n",
    "\n",
    "### What is Clustering?\n",
    "\n",
    "**Clustering** is an unsupervised learning task where we group similar data points together **without labels**.\n",
    "\n",
    "Use cases:\n",
    "- Customer segmentation\n",
    "- Anomaly detection\n",
    "- Data exploration\n",
    "\n",
    "### K-Means Algorithm\n",
    "\n",
    "**How it works:**\n",
    "1. Choose K (number of clusters)\n",
    "2. Randomly initialize K cluster centers\n",
    "3. Assign each point to nearest center\n",
    "4. Update centers to mean of assigned points\n",
    "5. Repeat steps 3-4 until convergence\n",
    "\n",
    "**Key hyperparameter:** K (number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "# TODO: Select features for clustering (age and fare)\n",
    "cluster_features = ['age', 'fare', 'pclass']\n",
    "X_cluster = df_ml[cluster_features]\n",
    "\n",
    "# Scale features (important for K-Means!)\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "print(f\"Clustering data shape: {X_cluster_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply K-Means with K=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_ml['cluster'] = clusters\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "print(df_ml['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Cluster characteristics\n",
    "print(\"\\nCluster characteristics:\")\n",
    "print(df_ml.groupby('cluster')[cluster_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D (Age vs Fare)\n",
    "fig = px.scatter(df_ml, x='age', y='fare', color='cluster',\n",
    "                 title='K-Means Clustering of Titanic Passengers',\n",
    "                 labels={'cluster': 'Cluster'},\n",
    "                 hover_data=['pclass', 'sex'])\n",
    "\n",
    "# Add cluster centers\n",
    "centers = scaler_cluster.inverse_transform(kmeans.cluster_centers_)\n",
    "fig.add_trace(go.Scatter(x=centers[:, 0], y=centers[:, 1],\n",
    "                         mode='markers',\n",
    "                         marker=dict(symbol='x', size=15, color='black', line=dict(width=2)),\n",
    "                         name='Cluster Centers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using the Elbow Method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    # YOUR CODE HERE\n",
    "    # Fit K-Means with k clusters\n",
    "    # Store inertia_ and silhouette_score\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_cluster_scaled)\n",
    "    \n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans_temp.labels_))\n",
    "\n",
    "# Plot elbow curve\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(K_range), y=inertias, mode='lines+markers',\n",
    "                         name='Inertia'))\n",
    "fig.update_layout(title='Elbow Method for Optimal K',\n",
    "                  xaxis_title='Number of Clusters (K)',\n",
    "                  yaxis_title='Inertia (Within-Cluster Sum of Squares)')\n",
    "fig.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "fig2 = px.line(x=list(K_range), y=silhouette_scores, markers=True,\n",
    "               title='Silhouette Score vs K',\n",
    "               labels={'x': 'Number of Clusters (K)', 'y': 'Silhouette Score'})\n",
    "fig2.show()\n",
    "\n",
    "print(f\"Best K based on silhouette score: {K_range[np.argmax(silhouette_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Evaluation Metrics\n",
    "\n",
    "- **Inertia**: Sum of squared distances to nearest cluster center\n",
    "  - Lower is better\n",
    "  - Always decreases as K increases\n",
    "  - Look for \"elbow\" in the curve\n",
    "\n",
    "- **Silhouette Score**: Measures how similar points are to their own cluster vs other clusters\n",
    "  - Range: -1 to 1\n",
    "  - Higher is better\n",
    "  - >0.5 = good clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Introduction to Neural Networks (10 mins)\n",
    "\n",
    "### What are Neural Networks?\n",
    "\n",
    "**Neural networks** are computing systems inspired by biological neural networks in animal brains.\n",
    "\n",
    "### Basic Structure\n",
    "\n",
    "```\n",
    "Input Layer â†’ Hidden Layer(s) â†’ Output Layer\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Neurons (Nodes)**: Processing units that receive inputs, apply weights, and produce outputs\n",
    "2. **Layers**: Groups of neurons\n",
    "   - Input Layer: Receives raw data\n",
    "   - Hidden Layer(s): Performs computations\n",
    "   - Output Layer: Produces final prediction\n",
    "3. **Weights**: Learned parameters that connect neurons\n",
    "4. **Biases**: Offset values for each neuron\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "**Activation functions** introduce non-linearity, allowing networks to learn complex patterns.\n",
    "\n",
    "**Common activation functions:**\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**\n",
    "   - Formula: `f(x) = max(0, x)`\n",
    "   - Most popular for hidden layers\n",
    "   - Fast and effective\n",
    "\n",
    "2. **Sigmoid**\n",
    "   - Formula: `f(x) = 1 / (1 + e^(-x))`\n",
    "   - Output: 0 to 1\n",
    "   - Used for binary classification output\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**\n",
    "   - Formula: `f(x) = (e^x - e^(-x)) / (e^x + e^(-x))`\n",
    "   - Output: -1 to 1\n",
    "   - Similar to sigmoid but centered at 0\n",
    "\n",
    "4. **Softmax**\n",
    "   - Converts outputs to probabilities (sum to 1)\n",
    "   - Used for multi-class classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Tanh\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=relu, mode='lines', name='ReLU'))\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid, mode='lines', name='Sigmoid'))\n",
    "fig.add_trace(go.Scatter(x=x, y=tanh, mode='lines', name='Tanh'))\n",
    "\n",
    "fig.update_layout(title='Common Activation Functions',\n",
    "                  xaxis_title='Input',\n",
    "                  yaxis_title='Output',\n",
    "                  showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Hyperparameters\n",
    "\n",
    "**Hyperparameters** are settings we choose before training (not learned from data).\n",
    "\n",
    "**Key hyperparameters:**\n",
    "\n",
    "1. **Number of layers**: How deep is the network?\n",
    "   - More layers = can learn more complex patterns\n",
    "   - But: harder to train, risk of overfitting\n",
    "\n",
    "2. **Number of neurons per layer**: How wide is each layer?\n",
    "   - More neurons = more capacity to learn\n",
    "   - But: more computation, risk of overfitting\n",
    "\n",
    "3. **Learning rate**: How big are the update steps during training?\n",
    "   - Too high: training unstable, might not converge\n",
    "   - Too low: training very slow\n",
    "   - Typical values: 0.001 to 0.1\n",
    "\n",
    "4. **Batch size**: How many examples to process before updating weights?\n",
    "   - Small batch: noisy updates, but more frequent\n",
    "   - Large batch: stable updates, but less frequent\n",
    "   - Common values: 16, 32, 64, 128\n",
    "\n",
    "5. **Epochs**: How many times to go through entire dataset?\n",
    "   - More epochs = more training\n",
    "   - But: risk of overfitting if too many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Neural Network Architecture\n",
    "\n",
    "For Titanic survival prediction:\n",
    "\n",
    "```\n",
    "Input Layer (7 neurons)\n",
    "    â†“\n",
    "Hidden Layer 1 (16 neurons, ReLU activation)\n",
    "    â†“\n",
    "Hidden Layer 2 (8 neurons, ReLU activation)\n",
    "    â†“\n",
    "Output Layer (1 neuron, Sigmoid activation)\n",
    "```\n",
    "\n",
    "**Total parameters to learn:**\n",
    "- Layer 1: 7 Ã— 16 + 16 = 128\n",
    "- Layer 2: 16 Ã— 8 + 8 = 136\n",
    "- Output: 8 Ã— 1 + 1 = 9\n",
    "- **Total: 273 parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation (Conceptual)\n",
    "\n",
    "How a neural network makes a prediction:\n",
    "\n",
    "1. **Input**: Start with features (age, sex, pclass, etc.)\n",
    "2. **Layer 1**: \n",
    "   - Multiply inputs by weights\n",
    "   - Add biases\n",
    "   - Apply activation function (ReLU)\n",
    "3. **Layer 2**: \n",
    "   - Multiply layer 1 outputs by weights\n",
    "   - Add biases\n",
    "   - Apply activation function (ReLU)\n",
    "4. **Output Layer**: \n",
    "   - Multiply layer 2 outputs by weights\n",
    "   - Add bias\n",
    "   - Apply activation function (Sigmoid)\n",
    "   - Get probability of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "**Loss function** measures how wrong our predictions are. Goal: minimize loss!\n",
    "\n",
    "#### For Classification:\n",
    "\n",
    "**1. Binary Cross-Entropy Loss** (for binary classification)\n",
    "- Formula: `-[y log(Å·) + (1-y) log(1-Å·)]`\n",
    "- Where y = actual (0 or 1), Å· = predicted probability\n",
    "- Penalizes confident wrong predictions heavily\n",
    "\n",
    "**2. Categorical Cross-Entropy Loss** (for multi-class)\n",
    "- Extension of binary cross-entropy for multiple classes\n",
    "\n",
    "#### For Regression:\n",
    "\n",
    "**1. Mean Squared Error (MSE)**\n",
    "- Formula: `average of (actual - predicted)Â²`\n",
    "- Penalizes large errors more heavily\n",
    "\n",
    "**2. Mean Absolute Error (MAE)**\n",
    "- Formula: `average of |actual - predicted|`\n",
    "- Less sensitive to outliers than MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Binary Cross-Entropy Loss\n",
    "# When actual value is 1 (survived)\n",
    "y_pred_prob = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Loss when actual = 1\n",
    "loss_when_1 = -np.log(y_pred_prob)\n",
    "\n",
    "# Loss when actual = 0\n",
    "loss_when_0 = -np.log(1 - y_pred_prob)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y_pred_prob, y=loss_when_1, \n",
    "                         mode='lines', name='Actual = 1 (Survived)'))\n",
    "fig.add_trace(go.Scatter(x=y_pred_prob, y=loss_when_0, \n",
    "                         mode='lines', name='Actual = 0 (Died)'))\n",
    "\n",
    "fig.update_layout(title='Binary Cross-Entropy Loss',\n",
    "                  xaxis_title='Predicted Probability',\n",
    "                  yaxis_title='Loss',\n",
    "                  showlegend=True)\n",
    "fig.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- If actual = 1, we want predicted probability close to 1 (low loss)\")\n",
    "print(\"- If actual = 0, we want predicted probability close to 0 (low loss)\")\n",
    "print(\"- Being confident and wrong results in very high loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks (Conceptual)\n",
    "\n",
    "**Backpropagation** is the algorithm for training neural networks:\n",
    "\n",
    "1. **Forward pass**: Make predictions\n",
    "2. **Calculate loss**: How wrong are we?\n",
    "3. **Backward pass**: Calculate gradient of loss with respect to each weight\n",
    "4. **Update weights**: Adjust weights in direction that reduces loss\n",
    "5. **Repeat**: Go through dataset multiple times (epochs)\n",
    "\n",
    "**Gradient Descent** optimization:\n",
    "```\n",
    "new_weight = old_weight - learning_rate Ã— gradient\n",
    "```\n",
    "\n",
    "**Note**: We won't implement neural networks from scratch today, but libraries like TensorFlow and PyTorch handle this for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Exercises and Practice (5 mins)\n",
    "\n",
    "### Exercise 10.1: Improve Classification Performance\n",
    "\n",
    "Try to improve model accuracy by:\n",
    "1. Adding more features\n",
    "2. Engineering new features\n",
    "3. Trying different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your turn! Try to improve the model\n",
    "# Ideas:\n",
    "# - Add 'cabin_known' feature (binary: 1 if cabin is not null)\n",
    "# - Add 'is_alone' feature (1 if family_size == 1)\n",
    "# - Extract title from name (Mr, Mrs, Miss, Master)\n",
    "# - Try different train-test split ratios\n",
    "# - Experiment with different K values for K-NN\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2: Predict Passenger Class\n",
    "\n",
    "Change the problem: Predict `pclass` (1, 2, or 3) instead of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a multi-class classifier\n",
    "# 1. Select appropriate features (don't include pclass!)\n",
    "# 2. Split data\n",
    "# 3. Train a model (Logistic Regression works for multi-class)\n",
    "# 4. Evaluate using accuracy\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3: Clustering Analysis\n",
    "\n",
    "Analyze the clusters you created:\n",
    "1. What are the characteristics of each cluster?\n",
    "2. How do survival rates differ across clusters?\n",
    "3. Can you give meaningful names to each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze clusters\n",
    "# Calculate survival rate for each cluster\n",
    "# Look at other characteristics (sex, embarked, etc.)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Reflection\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "Today we learned:\n",
    "\n",
    "#### 1. Machine Learning Fundamentals\n",
    "- Machine learning is an optimization task: minimize loss by adjusting parameters\n",
    "- Three types: supervised (labeled data), unsupervised (unlabeled), reinforcement (rewards)\n",
    "\n",
    "#### 2. Supervised Learning: Classification\n",
    "- **K-NN**: Classify based on nearest neighbors (simple, intuitive)\n",
    "- **Decision Trees**: Create decision rules (interpretable)\n",
    "- **Logistic Regression**: Linear model with sigmoid activation (fast, probabilistic)\n",
    "- **SVM**: Find optimal decision boundary (effective in high dimensions)\n",
    "\n",
    "#### 3. Supervised Learning: Regression\n",
    "- **Linear Regression**: Predict continuous values with linear relationship\n",
    "- Metrics: MSE, RMSE (lower is better), RÂ² (higher is better)\n",
    "\n",
    "#### 4. Model Evaluation\n",
    "- **Train-test split**: Essential for detecting overfitting\n",
    "- **Cross-validation**: More robust evaluation using multiple splits\n",
    "- **Metrics**: Accuracy, precision, recall, F1-score (classification); MSE, RMSE, RÂ² (regression)\n",
    "\n",
    "#### 5. Unsupervised Learning: Clustering\n",
    "- **K-Means**: Group similar data points without labels\n",
    "- **Elbow method**: Find optimal number of clusters\n",
    "- **Silhouette score**: Measure clustering quality\n",
    "\n",
    "#### 6. Neural Networks (Conceptual)\n",
    "- **Architecture**: Input â†’ Hidden Layer(s) â†’ Output\n",
    "- **Activation functions**: ReLU, sigmoid, tanh, softmax\n",
    "- **Hyperparameters**: layers, neurons, learning rate, batch size, epochs\n",
    "- **Loss functions**: Cross-entropy (classification), MSE/MAE (regression)\n",
    "- **Training**: Forward pass + backpropagation + gradient descent\n",
    "\n",
    "### Machine Learning Workflow\n",
    "\n",
    "```\n",
    "1. Define Problem (classification, regression, clustering)\n",
    "   â†“\n",
    "2. Prepare Data (cleaning, encoding, feature engineering)\n",
    "   â†“\n",
    "3. Split Data (train/test or cross-validation)\n",
    "   â†“\n",
    "4. Choose Algorithm (based on problem type)\n",
    "   â†“\n",
    "5. Train Model (fit on training data)\n",
    "   â†“\n",
    "6. Evaluate Model (test on unseen data)\n",
    "   â†“\n",
    "7. Tune Hyperparameters (improve performance)\n",
    "   â†“\n",
    "8. Deploy Model (use for predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **What's the difference between classification and regression?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "2. **Why do we need to split data into training and testing sets?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "3. **Which classification model performed best on the Titanic dataset? Why do you think that is?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "4. **What are the three most important features for predicting survival?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "5. **When would you use unsupervised learning instead of supervised learning?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "6. **What is the purpose of activation functions in neural networks?**\n",
    "\n",
    "   Your answer: ___________________________________\n",
    "\n",
    "7. **What's the relationship between loss functions and optimization?**\n",
    "\n",
    "   Your answer: ___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus Challenges (Optional)\n",
    "\n",
    "### Challenge 1: Feature Engineering\n",
    "Create a new feature called `fare_category` that bins fare into 4 categories (cheap, medium, expensive, luxury). Does this improve model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Ensemble Methods\n",
    "Research and try a Random Forest classifier (ensemble of decision trees). How does it compare to individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: from sklearn.ensemble import RandomForestClassifier\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Hyperparameter Tuning\n",
    "Use GridSearchCV to find the best hyperparameters for your favorite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: from sklearn.model_selection import GridSearchCV\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources for Further Learning\n",
    "\n",
    "### Documentation\n",
    "- **Scikit-learn User Guide**: https://scikit-learn.org/stable/user_guide.html\n",
    "- **Scikit-learn Cheat Sheet**: https://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "\n",
    "### Tutorials\n",
    "- **Machine Learning Crash Course (Google)**: https://developers.google.com/machine-learning/crash-course\n",
    "- **Kaggle Learn**: https://www.kaggle.com/learn\n",
    "- **Neural Networks Explained**: https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "### Books\n",
    "- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by AurÃ©lien GÃ©ron\n",
    "- *Introduction to Statistical Learning* (free PDF): https://www.statlearning.com/\n",
    "\n",
    "### Practice\n",
    "- **Kaggle Competitions**: https://www.kaggle.com/competitions\n",
    "- **UCI ML Repository**: https://archive.ics.uci.edu/ml/\n",
    "\n",
    "---\n",
    "\n",
    "**Great job today! You've taken your first steps into machine learning!**\n",
    "\n",
    "Next session: Advanced machine learning techniques and deep learning with neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
