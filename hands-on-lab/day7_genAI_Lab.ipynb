{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea87fecf",
   "metadata": {},
   "source": [
    "# Hands-on GenAI Lab with Formula 1 Context\n",
    "\n",
    "In this lab, we'll explore:\n",
    "1. Running Ollama with Podman\n",
    "2. Using a small LLM model for chat\n",
    "3. Connecting to Ollama via Python\n",
    "4. Using Langchain for enhanced prompts\n",
    "5. Setting up N8N workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9326df73",
   "metadata": {},
   "source": [
    "## 1. Setting up Ollama with Podman\n",
    "\n",
    "First, let's run Ollama using Podman. We'll use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab90718",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#Run Ollama container\n",
    "podman run -d --name ollama -p 11434:11434 ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19100d1e",
   "metadata": {},
   "source": [
    "### Command Explanation: Running Ollama Container\n",
    "\n",
    "The command `podman run -d --name ollama -p 11434:11434 ollama/ollama` does the following:\n",
    "- `-d`: Runs the container in detached mode (in the background)\n",
    "- `--name ollama`: Names the container \"ollama\" for easy reference\n",
    "- `-p 11434:11434`: Maps port 11434 from the container to your host machine\n",
    "- `ollama/ollama`: Specifies the container image to use\n",
    "\n",
    "This command starts the Ollama server that will handle our LLM requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9026e",
   "metadata": {},
   "source": [
    "## 2. Running a Small LLM Model\n",
    "\n",
    "We'll use the `tiny-llama` model, which is relatively small but capable. First, let's pull and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960eb3d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Access the container\n",
    "podman exec -it ollama /bin/bash\n",
    "\n",
    "# Pull the model\n",
    "ollama pull tinyllama\n",
    "\n",
    "# Chat with the model via terminal\n",
    "ollama run tinyllama \"Tell me about the 2023 F1 World Champion Max Verstappen\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e72854",
   "metadata": {},
   "source": [
    "## Command Explanation: Model Operations\n",
    "\n",
    "The commands do the following:\n",
    "1. `ollama pull tinyllama`: \n",
    "   - Downloads the TinyLlama model to your local system\n",
    "   - TinyLlama is a smaller, efficient version of Llama\n",
    "   - Requires about 3GB of disk space\n",
    "\n",
    "2. `ollama run tinyllama \"Tell me about...\"`: \n",
    "   - Loads the model into memory\n",
    "   - Sends a prompt about Max Verstappen\n",
    "   - Returns a generated response from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec1a65",
   "metadata": {},
   "source": [
    "## 3. Connecting to Ollama via Python\n",
    "\n",
    "Now let's interact with Ollama using Python. First, install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0319be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who won the most F1 championships in history?\"\n",
    "model_url = \"http://host.containers.internal:11434/v1/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230172d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama response: {'id': 'cmpl-544', 'object': 'text_completion', 'created': 1762004744, 'model': 'tinyllama', 'system_fingerprint': 'fp_ollama', 'choices': [{'text': 'Me: I\\'m doing amazing, thank you! How about you? Are you up for some online games? Hey, let\\'s start with a simple one called \"Let\\'s Go Fishing\"! It\\'s based on classic game that involves fishing and catching the right fish, I hope you\\'re ready to play alongside me. In this classic board game, it is your aim to be the first person back from fishing by picking up as many fruits', 'index': 0, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 100, 'total_tokens': 143}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example payload for generating text\n",
    "payload = {\n",
    "    \"model\": \"tinyllama\",  # e.g., \"llama2\"\n",
    "    \"prompt\": \"Hello Ollama! How are you?\",\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "response = requests.post(model_url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"Ollama response:\", data)\n",
    "else:\n",
    "    print(\"Request failed with status:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10990e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me: I\\'m doing amazing, thank you! How about you? Are you up for some online games? Hey, let\\'s start with a simple one called \"Let\\'s Go Fishing\"! It\\'s based on classic game that involves fishing and catching the right fish, I hope you\\'re ready to play alongside me. In this classic board game, it is your aim to be the first person back from fishing by picking up as many fruits'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ef791",
   "metadata": {},
   "source": [
    "## 4. Setting up N8N with Podman\n",
    "\n",
    "First, let's run N8N using Podman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcbfed",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "podman run -it --name n8n -p 5678:5678"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c47fe87",
   "metadata": {},
   "source": [
    "## Command Explanation: N8N Container\n",
    "\n",
    "The command `podman run -it --name n8n -p 5678:5678` does the following:\n",
    "- `-it`: Runs in interactive mode with a terminal\n",
    "- `--name n8n`: Names the container \"n8n\"\n",
    "- `-p 5678:5678`: Maps the N8N web interface port\n",
    "- `docker.io/n8nio/n8n`: Uses the official N8N container image\n",
    "\n",
    "This starts the N8N workflow automation platform where we can create our F1-themed automation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46a164",
   "metadata": {},
   "source": [
    "### N8N Workflows\n",
    "\n",
    "1. Basic F1 Chat Workflow:\n",
    "- HTTP Trigger node\n",
    "- Ollama node (configured to localhost:11434)\n",
    "- Respond to Webhook node\n",
    "\n",
    "2. Structured Output Workflow:\n",
    "- HTTP Trigger node\n",
    "- Function node (to format prompt for structured output)\n",
    "- Ollama node with this prompt template:\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Extract the following information about {driver}:\n",
    "    {\n",
    "        'name': 'Full name of driver',\n",
    "        'championships': 'Number of F1 championships',\n",
    "        'teams': 'List of F1 teams raced for',\n",
    "        'notable_achievements': 'Key career highlights'\n",
    "    }\n",
    "    Return only valid JSON.\"\n",
    "}\n",
    "```\n",
    "- JSON Parse node\n",
    "- Respond to Webhook node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba736a",
   "metadata": {},
   "source": [
    "To test the N8N workflows:\n",
    "1. Access N8N at http://localhost:5678\n",
    "2. Import and activate both workflows\n",
    "3. Use the webhook URLs to test the endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
