{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Data Preparation & Feature Engineering - SOLUTIONS\n",
    "\n",
    "**Duration:** 90 minutes  \n",
    "**Dataset:** Titanic Passenger Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand data quality properties and the knowledge hierarchy\n",
    "- Distinguish structured vs unstructured data\n",
    "- Handle missing data using different strategies\n",
    "- Detect and handle outliers\n",
    "- Apply normalization techniques\n",
    "- Perform categorical encoding\n",
    "- Create new features through feature engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading (5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Data Quality Assessment (15 mins)\n",
    "\n",
    "### The Knowledge Hierarchy\n",
    "- **Data:** Raw facts (e.g., \"Age: 22\")\n",
    "- **Information:** Processed data (e.g., \"Average age is 29.7\")\n",
    "- **Knowledge:** Understanding patterns (e.g., \"Younger passengers had better survival rates\")\n",
    "- **Wisdom:** Applying knowledge (e.g., \"Prioritize evacuating children in emergencies\")\n",
    "\n",
    "### Exercise 2.1: Data Quality Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Check for missing values\n",
    "# We use df.isnull().sum() to count missing values in each column\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values per Column:\")\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** \n",
    "- The `age` column has significant missing data (about 20%)\n",
    "- The `deck` and `cabin` columns have very high missing percentages (>70%)\n",
    "- The `embarked` column has minimal missing data (only 2 values)\n",
    "- Understanding missing data patterns is crucial for deciding how to handle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Visualize missing data\n",
    "# Create a heatmap showing where data is missing\n",
    "# This helps us see patterns in missing data across different passengers\n",
    "\n",
    "missing_data = df.isnull()\n",
    "fig = px.imshow(missing_data.T, \n",
    "                labels=dict(x=\"Passenger\", y=\"Feature\", color=\"Missing\"),\n",
    "                title=\"Missing Data Heatmap (White = Missing)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- The heatmap reveals that `cabin` and `deck` have systematic missing patterns\n",
    "- `age` has scattered missing values across passengers\n",
    "- This visualization helps identify if missing data is random or follows a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Structured vs Unstructured Data\n",
    "\n",
    "**Structured Data:** Organized in tables with rows and columns (like our Titanic dataset)  \n",
    "**Unstructured Data:** No predefined format (e.g., text, images, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Identify different data types in our dataset\n",
    "# Understanding data types helps us choose appropriate preprocessing methods\n",
    "\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "# Numerical features can be used in calculations; categorical need encoding\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features: {numerical_cols}\")\n",
    "print(f\"Categorical features: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Numerical features** (int64, float64): Can be directly used in mathematical operations\n",
    "  - Examples: age, fare, sibsp, parch\n",
    "- **Categorical features** (object, category): Need encoding before use in ML models\n",
    "  - Examples: sex, embarked, class, who\n",
    "- Different preprocessing techniques apply to each type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Handling Missing Data (20 mins)\n",
    "\n",
    "### Missing Data Mechanisms\n",
    "- **MCAR** (Missing Completely At Random): No pattern\n",
    "- **MAR** (Missing At Random): Related to other observed variables\n",
    "- **MNAR** (Missing Not At Random): Related to the missing value itself\n",
    "\n",
    "### Exercise 3.1: Analyze Missing Age Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Calculate percentage of missing age values\n",
    "# .isnull().mean() gives us the proportion of missing values\n",
    "age_missing_pct = df['age'].isnull().mean()\n",
    "print(f\"Missing age values: {age_missing_pct*100:.1f}%\")\n",
    "\n",
    "# Check if age is MCAR, MAR, or MNAR\n",
    "# Compare survival rates for passengers with/without age data\n",
    "# If there's a difference, the missing data is NOT completely random\n",
    "has_age = df[df['age'].notnull()]['survived'].mean()\n",
    "no_age = df[df['age'].isnull()]['survived'].mean()\n",
    "\n",
    "print(f\"\\nSurvival rate (age known): {has_age*100:.1f}%\")\n",
    "print(f\"Survival rate (age missing): {no_age*100:.1f}%\")\n",
    "print(f\"\\nDifference: {abs(has_age - no_age)*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Based on the survival rate difference, is the missing age data MCAR, MAR, or MNAR?\n",
    "\n",
    "**Answer:** The missing age data is most likely **MAR (Missing At Random)** or **MNAR (Missing Not At Random)**. The survival rate difference (~5-8 percentage points) suggests that whether age data is recorded is related to other factors. For example:\n",
    "- Third-class passengers may have had less thorough record-keeping (MAR - related to passenger class)\n",
    "- Crew members or passengers who boarded at certain ports may be less likely to have age recorded\n",
    "- Since there's a systematic difference in outcomes, we can rule out MCAR\n",
    "\n",
    "**Why this matters:** Understanding the missing data mechanism helps us choose appropriate imputation strategies. For MAR/MNAR, group-based imputation (using other features) is often better than simple mean imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Imputation Strategies\n",
    "\n",
    "Let's try different ways to fill in missing age values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Mean imputation\n",
    "# SOLUTION: Fill missing ages with the mean age\n",
    "# Simple but can distort the distribution by adding many values at the center\n",
    "df['age_mean_imputed'] = df['age'].fillna(df['age'].mean())\n",
    "\n",
    "print(f\"Original mean age: {df['age'].mean():.2f}\")\n",
    "print(f\"After mean imputation: {df['age_mean_imputed'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Mean imputation replaces all missing values with the average\n",
    "- **Pros:** Simple, preserves the mean\n",
    "- **Cons:** Reduces variance, creates artificial peak at the mean, ignores relationships with other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Median imputation\n",
    "# SOLUTION: Fill missing ages with the median age\n",
    "# More robust to outliers than mean imputation\n",
    "df['age_median_imputed'] = df['age'].fillna(df['age'].median())\n",
    "\n",
    "print(f\"Original median age: {df['age'].median():.2f}\")\n",
    "print(f\"After median imputation: {df['age_median_imputed'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Median imputation uses the middle value\n",
    "- **Pros:** Less affected by outliers than mean, still simple\n",
    "- **Cons:** Still reduces variance, ignores relationships with other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Group-based imputation (by passenger class and sex)\n",
    "# SOLUTION: Fill missing ages with the median age of the same class and gender\n",
    "# This preserves relationships between variables\n",
    "# For example: First-class females tend to be older than third-class males\n",
    "df['age_group_imputed'] = df.groupby(['pclass', 'sex'])['age'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "print(\"\\nMedian age by class and gender:\")\n",
    "print(df.groupby(['pclass', 'sex'])['age'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Group-based imputation considers that different groups have different age distributions\n",
    "- For example: 1st class male passengers had median age of ~40, while 3rd class females had median age of ~21\n",
    "- **Pros:** Preserves relationships between variables, more realistic values\n",
    "- **Cons:** More complex, requires identifying relevant grouping variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distributions\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=df['age'], name='Original (with missing)', opacity=0.7))\n",
    "fig.add_trace(go.Histogram(x=df['age_mean_imputed'], name='Mean Imputed', opacity=0.7))\n",
    "fig.add_trace(go.Histogram(x=df['age_group_imputed'], name='Group Imputed', opacity=0.7))\n",
    "fig.update_layout(title='Comparison of Imputation Strategies',\n",
    "                  xaxis_title='Age',\n",
    "                  yaxis_title='Count',\n",
    "                  barmode='overlay')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which imputation strategy preserves the distribution best? Why?\n",
    "\n",
    "**Answer:** **Group-based imputation** preserves the distribution best because:\n",
    "\n",
    "1. **Maintains variance:** Doesn't create artificial peaks like mean/median imputation\n",
    "2. **Preserves relationships:** Accounts for the fact that age distributions differ by passenger class and gender\n",
    "3. **More realistic:** Imputed values are drawn from similar passengers' ages\n",
    "4. **Avoids bias:** Mean imputation would create a spike at age ~30, which is unrealistic\n",
    "\n",
    "You can see in the histogram that:\n",
    "- Mean imputation creates an artificial spike at the mean age (~30)\n",
    "- Group imputation maintains the natural spread of ages\n",
    "- Group imputation better represents the underlying population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Handling Missing Cabin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Check missing cabin data\n",
    "cabin_missing_pct = df['cabin'].isnull().mean()\n",
    "print(f\"Missing cabin values: {cabin_missing_pct*100:.1f}%\")\n",
    "\n",
    "# Create a binary feature: cabin_known (1 if cabin is known, 0 otherwise)\n",
    "# When >70% of data is missing, imputation isn't useful\n",
    "# Instead, we create a feature that captures whether the information was recorded\n",
    "df['cabin_known'] = df['cabin'].notnull().astype(int)\n",
    "\n",
    "# Check if having cabin information correlates with survival\n",
    "print(\"\\nSurvival rate by cabin information:\")\n",
    "print(df.groupby('cabin_known')['survived'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- With 77% missing data, imputing cabin numbers would be unreliable\n",
    "- However, WHETHER cabin data exists is highly predictive!\n",
    "- Passengers with known cabins had ~67% survival vs ~30% for unknown\n",
    "- This makes sense: cabin assignments were more complete for first-class passengers\n",
    "- **Key insight:** Sometimes the presence/absence of data is more informative than the data itself!\n",
    "- This is an example of **feature engineering from missing data patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Outlier Detection and Handling (20 mins)\n",
    "\n",
    "### Exercise 4.1: Detect Outliers Using IQR Method\n",
    "\n",
    "**IQR (Interquartile Range) Method:**\n",
    "- Q1 = 25th percentile\n",
    "- Q3 = 75th percentile\n",
    "- IQR = Q3 - Q1\n",
    "- Outliers: values < Q1 - 1.5Ã—IQR or > Q3 + 1.5Ã—IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Detect outliers in 'fare' using IQR method\n",
    "# The IQR method is robust and commonly used for identifying outliers\n",
    "Q1 = df['fare'].quantile(0.25)\n",
    "Q3 = df['fare'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Q1: {Q1:.2f}\")\n",
    "print(f\"Q3: {Q3:.2f}\")\n",
    "print(f\"IQR: {IQR:.2f}\")\n",
    "print(f\"Lower bound: {lower_bound:.2f}\")\n",
    "print(f\"Upper bound: {upper_bound:.2f}\")\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['fare'] < lower_bound) | (df['fare'] > upper_bound)]\n",
    "print(f\"\\nNumber of outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Q1 (25th percentile):** 25% of passengers paid less than this\n",
    "- **Q3 (75th percentile):** 75% of passengers paid less than this\n",
    "- **IQR:** The range containing the middle 50% of data\n",
    "- **1.5Ã—IQR rule:** Standard threshold from Tukey's method\n",
    "- Values beyond Q1-1.5Ã—IQR or Q3+1.5Ã—IQR are considered outliers\n",
    "- This method is less sensitive to extreme values than mean-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create a box plot to visualize outliers\n",
    "# Box plots are specifically designed to show the IQR and outliers\n",
    "fig = px.box(df, y='fare', \n",
    "             title='Fare Distribution with Outliers',\n",
    "             labels={'fare': 'Fare (Â£)'})\n",
    "fig.update_layout(yaxis_title='Fare (Â£)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- The box shows the IQR (25th to 75th percentile)\n",
    "- The line inside the box is the median\n",
    "- The whiskers extend to 1.5Ã—IQR\n",
    "- Points beyond the whiskers are outliers\n",
    "- We can see several high-fare outliers (expensive first-class tickets)\n",
    "- These outliers are real data points, not errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Z-Score Method for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Calculate Z-scores for fare\n",
    "# Z-score measures how many standard deviations away from the mean a value is\n",
    "# Z-score = (value - mean) / standard deviation\n",
    "df['fare_zscore'] = stats.zscore(df['fare'], nan_policy='omit')\n",
    "\n",
    "# Identify outliers (|Z-score| > 3)\n",
    "# The \"3-sigma rule\": values beyond 3 standard deviations are rare (<0.3% in normal distribution)\n",
    "outliers_zscore = df[np.abs(df['fare_zscore']) > 3]\n",
    "print(f\"Number of outliers (Z-score > 3): {len(outliers_zscore)}\")\n",
    "\n",
    "# Show the outliers\n",
    "print(\"\\nOutliers:\")\n",
    "print(outliers_zscore[['name', 'fare', 'pclass', 'fare_zscore']].sort_values('fare', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Z-score formula:** (value - mean) / std\n",
    "- **Interpretation:** \n",
    "  - Z-score = 0: value is at the mean\n",
    "  - Z-score = 1: value is 1 standard deviation above mean\n",
    "  - Z-score = -2: value is 2 standard deviations below mean\n",
    "- **Threshold of 3:** In a normal distribution, 99.7% of data falls within Â±3 standard deviations\n",
    "- **Assumption:** Z-score method assumes data is approximately normally distributed\n",
    "- **Findings:** The outliers are mostly first-class passengers with expensive tickets (e.g., fare > Â£200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Should we remove these outliers? Why or why not?\n",
    "\n",
    "**Answer:** **No, we should NOT remove these outliers** for several reasons:\n",
    "\n",
    "1. **They are legitimate data points:** These are real passengers who paid expensive fares for first-class tickets\n",
    "2. **Domain knowledge:** In the Titanic context, expensive tickets for luxury suites are expected\n",
    "3. **Predictive value:** High fares are associated with higher survival rates (first-class passengers had priority for lifeboats)\n",
    "4. **Sample size:** We have a relatively small dataset (891 passengers), so removing data reduces our ability to learn patterns\n",
    "5. **Alternative approaches:**\n",
    "   - Use robust algorithms that handle outliers well (e.g., tree-based models)\n",
    "   - Apply log transformation to reduce the impact of extreme values\n",
    "   - Cap values at a reasonable threshold rather than removing them\n",
    "\n",
    "**When to remove outliers:**\n",
    "- Data entry errors (e.g., age = 999)\n",
    "- Measurement errors\n",
    "- Data from a different population\n",
    "\n",
    "**General rule:** Only remove outliers if you have strong evidence they are errors, not just because they are unusual!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Isolation Forest for Multivariate Outlier Detection\n",
    "\n",
    "**Isolation Forest:** Machine learning algorithm that detects anomalies by isolating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Use Isolation Forest to detect outliers\n",
    "# Select numerical features for analysis\n",
    "features_for_outliers = ['age_group_imputed', 'fare', 'sibsp', 'parch']\n",
    "X = df[features_for_outliers].copy()\n",
    "\n",
    "# Create and fit Isolation Forest\n",
    "# contamination=0.1 means we expect ~10% of data to be outliers\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "df['outlier'] = iso_forest.fit_predict(X)\n",
    "# -1 = outlier, 1 = normal\n",
    "\n",
    "print(f\"Number of outliers detected: {(df['outlier'] == -1).sum()}\")\n",
    "print(f\"Percentage: {(df['outlier'] == -1).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Isolation Forest** is an unsupervised learning algorithm for anomaly detection\n",
    "- **How it works:** \n",
    "  - Builds random decision trees\n",
    "  - Outliers are easier to \"isolate\" (require fewer splits) than normal points\n",
    "  - Points that are isolated quickly are classified as outliers\n",
    "- **Advantages over IQR/Z-score:**\n",
    "  - Works with multiple features simultaneously (multivariate)\n",
    "  - Doesn't assume normal distribution\n",
    "  - Can detect complex outlier patterns\n",
    "- **contamination parameter:** Expected proportion of outliers (we set to 10%)\n",
    "- **Use case:** Detecting passengers with unusual combinations of features (e.g., old age + large family + high fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers in 2D space (Age vs Fare)\n",
    "fig = px.scatter(df, x='age_group_imputed', y='fare', \n",
    "                 color=df['outlier'].map({1: 'Normal', -1: 'Outlier'}),\n",
    "                 title='Outlier Detection with Isolation Forest',\n",
    "                 labels={'color': 'Status'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Red points (outliers) are passengers with unusual combinations of features\n",
    "- Notice outliers are not just at extreme values of a single feature\n",
    "- They are points that are \"isolated\" from the main clusters\n",
    "- Examples: very old passengers with high fares, or unusual family configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Normalization & Standardization (15 mins)\n",
    "\n",
    "### Why Normalize?\n",
    "- Different features have different scales\n",
    "- Many ML algorithms perform better with normalized data\n",
    "- Prevents features with large values from dominating\n",
    "\n",
    "### Exercise 5.1: Min-Max Normalization (0-1 scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Apply Min-Max scaling to age and fare\n",
    "# Formula: (x - min) / (max - min)\n",
    "# This scales all values to the range [0, 1]\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "df['age_normalized'] = scaler_minmax.fit_transform(df[['age_group_imputed']])\n",
    "# Create a new scaler for fare to avoid mixing the min/max from age\n",
    "scaler_minmax_fare = MinMaxScaler()\n",
    "df['fare_normalized'] = scaler_minmax_fare.fit_transform(df[['fare']])\n",
    "\n",
    "print(\"Original values:\")\n",
    "print(df[['age_group_imputed', 'fare']].describe())\n",
    "print(\"\\nNormalized values (0-1 range):\")\n",
    "print(df[['age_normalized', 'fare_normalized']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Min-Max Scaling:** Transforms data to a fixed range [0, 1]\n",
    "- **Formula:** normalized_value = (value - min) / (max - min)\n",
    "- **Properties:**\n",
    "  - Minimum value â†’ 0\n",
    "  - Maximum value â†’ 1\n",
    "  - Preserves the shape of the distribution\n",
    "  - Preserves relationships between values\n",
    "- **When to use:**\n",
    "  - When you need bounded values (e.g., for neural networks)\n",
    "  - When features have different units but similar distributions\n",
    "- **Limitation:** Sensitive to outliers (they compress the rest of the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Z-Score Standardization (mean=0, std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Apply Z-score standardization\n",
    "# Formula: (x - mean) / std\n",
    "# This centers data at 0 with standard deviation of 1\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "df['age_standardized'] = scaler_standard.fit_transform(df[['age_group_imputed']])\n",
    "# Create a new scaler for fare\n",
    "scaler_standard_fare = StandardScaler()\n",
    "df['fare_standardized'] = scaler_standard_fare.fit_transform(df[['fare']])\n",
    "\n",
    "print(\"Standardized values (meanâ‰ˆ0, stdâ‰ˆ1):\")\n",
    "print(df[['age_standardized', 'fare_standardized']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Standardization:** Transforms data to have mean=0 and standard deviation=1\n",
    "- **Formula:** standardized_value = (value - mean) / std\n",
    "- **Properties:**\n",
    "  - Mean becomes 0 (or very close to 0)\n",
    "  - Standard deviation becomes 1\n",
    "  - No fixed minimum or maximum\n",
    "  - Preserves the shape of the distribution\n",
    "- **When to use:**\n",
    "  - Most common for ML algorithms (especially those using distance metrics)\n",
    "  - When features have different units and scales\n",
    "  - For algorithms like SVM, KNN, linear regression, PCA\n",
    "- **Advantage over Min-Max:** Less sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs normalized vs standardized\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=df['fare'], name='Original Fare'))\n",
    "fig.add_trace(go.Box(y=df['fare_normalized'], name='Normalized Fare'))\n",
    "fig.add_trace(go.Box(y=df['fare_standardized'], name='Standardized Fare'))\n",
    "fig.update_layout(title='Comparison of Scaling Methods',\n",
    "                  yaxis_title='Value')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Comparison:**\n",
    "- **Original:** Wide range, outliers clearly visible\n",
    "- **Normalized:** Compressed to [0, 1], outliers still present but less prominent\n",
    "- **Standardized:** Centered at 0, most values between -3 and 3\n",
    "\n",
    "**Key Insight:** All three transformations preserve the relative relationships between data points - they just change the scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Categorical Encoding (10 mins)\n",
    "\n",
    "### Exercise 6.1: One-Hot Encoding\n",
    "\n",
    "Machine learning models need numerical input. We need to convert categorical variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Apply one-hot encoding to 'embarked' column\n",
    "# One-hot encoding creates a binary column for each category\n",
    "embarked_encoded = pd.get_dummies(df['embarked'], prefix='embarked', drop_first=False)\n",
    "print(\"Original column:\")\n",
    "print(df['embarked'].value_counts())\n",
    "print(\"\\nOne-hot encoded:\")\n",
    "print(embarked_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **One-Hot Encoding:** Creates binary (0/1) columns for each category\n",
    "- **Example:** \n",
    "  - Original: embarked = 'S'\n",
    "  - Encoded: embarked_S=1, embarked_C=0, embarked_Q=0\n",
    "- **Why it works:** \n",
    "  - Doesn't assume any ordering between categories\n",
    "  - Each category gets its own feature\n",
    "- **drop_first parameter:**\n",
    "  - drop_first=True removes one column to avoid multicollinearity\n",
    "  - drop_first=False keeps all columns (easier to interpret)\n",
    "- **When to use:** \n",
    "  - Nominal categories (no inherent order): sex, embarked, cabin\n",
    "  - Works best with features that have few categories (<10)\n",
    "- **Limitation:** Creates many columns if there are many categories (\"curse of dimensionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Encode 'sex' column\n",
    "# Create a binary encoding: male=1, female=0\n",
    "# For binary categories, simple mapping is often clearer than one-hot encoding\n",
    "df['sex_encoded'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "print(\"Sex encoding:\")\n",
    "print(df[['sex', 'sex_encoded']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Binary Encoding:** For features with only 2 categories, use simple 0/1 encoding\n",
    "- **Advantages:**\n",
    "  - Uses only 1 column instead of 2 (more efficient)\n",
    "  - Easier to interpret coefficients in linear models\n",
    "  - Same information content as one-hot encoding for binary variables\n",
    "- **Convention:** Usually encode as 0/1, but the choice of which is 0 vs 1 is arbitrary\n",
    "- **Alternative:** Could also use -1/1 encoding for some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Feature Engineering (15 mins)\n",
    "\n",
    "### Creating New Features\n",
    "\n",
    "Feature engineering is the art of creating new features from existing data to improve model performance.\n",
    "\n",
    "### Exercise 7.1: Family Size Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create family_size feature\n",
    "# SibSp = number of siblings/spouses aboard\n",
    "# Parch = number of parents/children aboard\n",
    "# family_size = SibSp + Parch + 1 (the +1 is for the passenger themselves)\n",
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "\n",
    "# Create is_alone feature\n",
    "# A passenger is alone if family_size == 1\n",
    "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
    "\n",
    "print(\"Family size distribution:\")\n",
    "print(df['family_size'].value_counts().sort_index())\n",
    "print(f\"\\nPassengers traveling alone: {df['is_alone'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **family_size:** Combines sibsp and parch into a single meaningful feature\n",
    "  - sibsp: siblings and spouses aboard\n",
    "  - parch: parents and children aboard\n",
    "  - +1: counts the passenger themselves\n",
    "- **is_alone:** Binary feature for solo travelers\n",
    "  - Hypothesis: Solo travelers may have different survival patterns\n",
    "  - Easier to interpret than family_size in some contexts\n",
    "- **Why this helps:**\n",
    "  - Reduces 2 features (sibsp, parch) to 1 (family_size) - simpler model\n",
    "  - May have non-linear relationship: small families had better survival than very large or solo\n",
    "  - Captures domain knowledge: family groups may stick together during evacuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze survival by family size\n",
    "family_survival = df.groupby('family_size')['survived'].mean()\n",
    "fig = px.bar(x=family_survival.index, y=family_survival.values,\n",
    "             title='Survival Rate by Family Size',\n",
    "             labels={'x': 'Family Size', 'y': 'Survival Rate'})\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "- **Solo travelers (family_size=1):** ~30% survival - may have lacked help/coordination\n",
    "- **Small families (2-4):** ~50-70% survival - optimal for group coordination\n",
    "- **Large families (5+):** Lower survival - may have been harder to keep together during evacuation\n",
    "- **Non-linear relationship:** This is why combining sibsp and parch into family_size is valuable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Extract Title from Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Extract title from name (Mr., Mrs., Miss., etc.)\n",
    "# Names follow the pattern: \"Surname, Title. Firstname\"\n",
    "# We use regex to extract the title between space and period\n",
    "df['title'] = df['name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "print(\"Titles found:\")\n",
    "print(df['title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Regex pattern:** ` ([A-Za-z]+)\\.`\n",
    "  - Space before the title\n",
    "  - Captures one or more letters\n",
    "  - Followed by a period\n",
    "- **Why extract titles:**\n",
    "  - Captures social status (Mr., Mrs., Master, Dr., Rev.)\n",
    "  - Indicates age group (Master for boys, Miss for unmarried women)\n",
    "  - May correlate with survival (women and children first)\n",
    "- **Titles found:**\n",
    "  - Common: Mr (men), Miss (unmarried women), Mrs (married women), Master (boys)\n",
    "  - Rare: Dr, Rev, Col, Major, Countess, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Group rare titles into 'Other'\n",
    "# Keep only common titles: Mr, Miss, Mrs, Master\n",
    "# This prevents overfitting on rare categories\n",
    "common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n",
    "df['title_grouped'] = df['title'].apply(\n",
    "    lambda x: x if x in common_titles else 'Other'\n",
    ")\n",
    "\n",
    "print(\"\\nGrouped titles:\")\n",
    "print(df['title_grouped'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Why group rare titles:**\n",
    "  - Rare categories (e.g., Countess, Don) have few examples\n",
    "  - Models may overfit or fail to generalize from single examples\n",
    "  - Grouping increases sample size for 'Other' category\n",
    "- **Alternative approaches:**\n",
    "  - Could group by semantics: nobility (Sir, Lady, Countess), professional (Dr, Rev), military (Col, Major)\n",
    "  - Could drop rare titles entirely\n",
    "- **Trade-off:** Lose some information, but gain stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze survival by title\n",
    "title_survival = df.groupby('title_grouped')['survived'].mean().sort_values(ascending=False)\n",
    "fig = px.bar(x=title_survival.index, y=title_survival.values,\n",
    "             title='Survival Rate by Title',\n",
    "             labels={'x': 'Title', 'y': 'Survival Rate'})\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "- **Mrs (married women):** ~80% survival - highest rate, prioritized in \"women and children first\"\n",
    "- **Miss (unmarried women):** ~70% survival - also prioritized\n",
    "- **Master (boys):** ~60% survival - children were prioritized\n",
    "- **Mr (men):** ~16% survival - lowest rate, given last priority\n",
    "- **Other (rare titles):** Mixed results, often included wealthy/noble passengers\n",
    "\n",
    "**Why this feature is powerful:**\n",
    "- Captures both gender and social status in one feature\n",
    "- More predictive than gender alone\n",
    "- Shows clear pattern aligned with historical accounts of evacuation priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Age Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create age groups\n",
    "# Categories: Child (0-12), Teen (13-19), Adult (20-59), Senior (60+)\n",
    "# Binning continuous variables can capture non-linear relationships\n",
    "\n",
    "bins = [0, 12, 19, 59, 100]\n",
    "labels = ['Child', 'Teen', 'Adult', 'Senior']\n",
    "df['age_group'] = pd.cut(df['age_group_imputed'], bins=bins, labels=labels)\n",
    "\n",
    "print(\"Age group distribution:\")\n",
    "print(df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **pd.cut():** Converts continuous variable into categorical bins\n",
    "- **Bin choices:**\n",
    "  - Child (0-12): Young children, \"children first\" policy\n",
    "  - Teen (13-19): Adolescents, between child and adult\n",
    "  - Adult (20-59): Working-age adults\n",
    "  - Senior (60+): Elderly passengers\n",
    "- **Why bin age:**\n",
    "  - May have non-linear relationship with survival\n",
    "  - Different age groups may have been treated differently\n",
    "  - More interpretable than continuous age\n",
    "  - Can handle imputed ages without assuming exact precision\n",
    "- **Trade-offs:**\n",
    "  - Loses information within bins (e.g., 25 vs 35 treated same)\n",
    "  - Bin boundaries are somewhat arbitrary\n",
    "  - Creates categorical variable that needs encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival by age group\n",
    "age_group_survival = df.groupby('age_group')['survived'].mean()\n",
    "fig = px.bar(x=age_group_survival.index, y=age_group_survival.values,\n",
    "             title='Survival Rate by Age Group',\n",
    "             labels={'x': 'Age Group', 'y': 'Survival Rate'})\n",
    "fig.update_layout(yaxis_tickformat='.0%')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "- **Children:** ~60% survival - \"children first\" policy clearly visible\n",
    "- **Teens:** ~40% survival - lower priority than young children\n",
    "- **Adults:** ~40% survival - varied by gender (women higher, men lower)\n",
    "- **Seniors:** ~30% survival - may have had mobility challenges\n",
    "\n",
    "**Interpretation:**\n",
    "- Clear non-linear pattern - age groups capture this better than linear age\n",
    "- Validates our binning strategy\n",
    "- Interaction with gender likely important (adult women vs adult men)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4: Fare Per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Calculate fare per person (fare / family_size)\n",
    "# Some tickets were purchased for entire families\n",
    "# Fare per person better represents individual wealth/class\n",
    "df['fare_per_person'] = df['fare'] / df['family_size']\n",
    "\n",
    "print(\"Fare per person statistics:\")\n",
    "print(df['fare_per_person'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Problem with raw fare:**\n",
    "  - Families often purchased tickets together\n",
    "  - A family of 4 paying Â£80 (Â£20 each) is different from solo traveler paying Â£80\n",
    "  - Raw fare confounds family size with wealth/class\n",
    "- **fare_per_person solution:**\n",
    "  - Divides total fare by family size\n",
    "  - Better represents individual spending/class\n",
    "  - Normalizes for group ticket purchases\n",
    "- **Why this helps:**\n",
    "  - More accurate proxy for individual wealth\n",
    "  - Removes confounding with family size\n",
    "  - May be more predictive than raw fare\n",
    "- **Example:**\n",
    "  - Family of 4, fare Â£80 â†’ Â£20 per person (3rd class)\n",
    "  - Solo traveler, fare Â£80 â†’ Â£80 per person (1st class suite)\n",
    "\n",
    "**Additional insight:** This is an example of **interaction between features** - fare and family_size interact in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Creating the Final Cleaned Dataset (10 mins)\n",
    "\n",
    "### Exercise 8.1: Select and Prepare Final Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create final dataset with cleaned and engineered features\n",
    "# Select the most useful features for machine learning\n",
    "final_features = [\n",
    "    'survived',           # Target variable\n",
    "    'pclass',            # Original feature\n",
    "    'sex_encoded',       # Encoded feature\n",
    "    'age_group_imputed', # Imputed feature\n",
    "    'fare_per_person',   # Engineered feature\n",
    "    'family_size',       # Engineered feature\n",
    "    'is_alone',          # Engineered feature\n",
    "    'cabin_known'        # Engineered feature\n",
    "]\n",
    "\n",
    "df_final = df[final_features].copy()\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection Rationale:**\n",
    "\n",
    "1. **survived:** Target variable (what we're predicting)\n",
    "\n",
    "2. **pclass:** Original feature, strong predictor (1st class had better survival)\n",
    "\n",
    "3. **sex_encoded:** Encoded categorical, very strong predictor (women survived more)\n",
    "\n",
    "4. **age_group_imputed:** Imputed with group-based strategy, preserves age patterns\n",
    "\n",
    "5. **fare_per_person:** Engineered feature, better than raw fare\n",
    "\n",
    "6. **family_size:** Engineered feature combining sibsp + parch\n",
    "\n",
    "7. **is_alone:** Binary version of family_size, captures important pattern\n",
    "\n",
    "8. **cabin_known:** Engineered from missing data pattern, surprisingly predictive\n",
    "\n",
    "**Features NOT included:**\n",
    "- **name:** Too unique, extracted title instead\n",
    "- **ticket:** Mostly unique, not generalizable\n",
    "- **cabin:** Too many missing values, used cabin_known instead\n",
    "- **embarked:** Weaker predictor (could add if desired)\n",
    "- **sibsp, parch:** Replaced by family_size\n",
    "- **fare:** Replaced by fare_per_person\n",
    "- **age:** Replaced by age_group_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Check for any remaining missing values\n",
    "# After all our preprocessing, we should have no missing data\n",
    "print(\"Missing values in final dataset:\")\n",
    "print(df_final.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success Check:**\n",
    "- All missing values should be 0\n",
    "- If any remain, we need to go back and handle them\n",
    "- Machine learning algorithms generally cannot handle missing values\n",
    "- Our preprocessing pipeline successfully handled all missing data through:\n",
    "  - Age: Group-based imputation\n",
    "  - Cabin: Created cabin_known binary feature\n",
    "  - Other features: Either had no missing values or were excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nFinal Dataset Summary:\")\n",
    "print(df_final.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Quality Check:**\n",
    "- **Count:** All features should have 891 values (no missing data)\n",
    "- **Survived:** 38% survival rate (historically accurate)\n",
    "- **Pclass:** Mean ~2.3 (more 3rd class than 1st class)\n",
    "- **Sex_encoded:** Mean ~0.65 (65% male, 35% female)\n",
    "- **Age_group_imputed:** Mean ~30 years, reasonable range\n",
    "- **Fare_per_person:** Wide range, some outliers (expensive first-class suites)\n",
    "- **Family_size:** Mean ~1.9, most traveling alone or in small groups\n",
    "- **Is_alone:** ~60% traveling alone\n",
    "- **Cabin_known:** ~23% have cabin information\n",
    "\n",
    "**This dataset is now ready for machine learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Reflection\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "Today we learned:\n",
    "- âœ“ How to assess data quality using the knowledge hierarchy\n",
    "- âœ“ Strategies for handling missing data (mean, median, group-based imputation)\n",
    "- âœ“ Multiple methods for detecting outliers (IQR, Z-score, Isolation Forest)\n",
    "- âœ“ Normalization techniques (Min-Max, Z-score standardization)\n",
    "- âœ“ Categorical encoding (one-hot encoding, binary encoding)\n",
    "- âœ“ Feature engineering to create meaningful new features\n",
    "\n",
    "### Data Preparation Pipeline Summary\n",
    "\n",
    "```\n",
    "Raw Data â†’ Missing Data Handling â†’ Outlier Detection â†’ \n",
    "Normalization â†’ Encoding â†’ Feature Engineering â†’ Clean Dataset\n",
    "```\n",
    "\n",
    "### Reflection Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Which imputation strategy worked best for the age variable and why?**\n",
    "\n",
    "**Answer:** **Group-based imputation** (by passenger class and sex) worked best because:\n",
    "\n",
    "- **Preserves relationships:** Age varies significantly by passenger class and gender. First-class male passengers were typically older (~40 years median) than third-class female passengers (~21 years median).\n",
    "\n",
    "- **Maintains distribution:** Unlike mean/median imputation which creates artificial peaks, group-based imputation spreads values more naturally across the distribution.\n",
    "\n",
    "- **Reflects reality:** Using the median age of similar passengers (same class and gender) produces more realistic imputed values than using the overall population mean.\n",
    "\n",
    "- **Better for MAR data:** Since age missingness is related to other variables (likely MAR, not MCAR), group-based imputation accounts for these relationships.\n",
    "\n",
    "- **Improved model performance:** By preserving the relationship between age, class, and gender, we provide more informative features to ML models.\n",
    "\n",
    "**Example:** A missing age for a 1st class female is imputed as ~35 (median for that group) rather than ~30 (overall median), which is more accurate and preserves the class-age relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Why is feature engineering important for machine learning?**\n",
    "\n",
    "**Answer:** Feature engineering is crucial because:\n",
    "\n",
    "**A. Captures domain knowledge:**\n",
    "- Incorporates human understanding into the model\n",
    "- Example: Creating `family_size` reflects the intuition that families may stay together during evacuation\n",
    "- Example: Extracting `title` captures social status and evacuation priority\n",
    "\n",
    "**B. Reveals hidden patterns:**\n",
    "- Combines features to expose non-obvious relationships\n",
    "- Example: `fare_per_person` removes confounding between fare and family size\n",
    "- Example: `cabin_known` reveals that presence of data is itself predictive\n",
    "\n",
    "**C. Improves model performance:**\n",
    "- Good features can dramatically increase accuracy\n",
    "- May allow simpler models to achieve better results\n",
    "- Example: `title` may be more predictive than gender alone\n",
    "\n",
    "**D. Handles non-linearity:**\n",
    "- Creates features that capture non-linear relationships\n",
    "- Example: Binning age into groups captures the non-linear effect (children survived more, but not linearly with age)\n",
    "\n",
    "**E. Reduces dimensionality:**\n",
    "- Combines correlated features into single meaningful features\n",
    "- Example: `family_size` replaces `sibsp` + `parch` with one feature\n",
    "- Simpler models, less overfitting\n",
    "\n",
    "**Quote from Andrew Ng:** \"Applied machine learning is basically feature engineering.\" Good features often matter more than algorithm choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What new feature did you find most insightful?**\n",
    "\n",
    "**Answer:** The most insightful engineered feature is **`cabin_known`** because:\n",
    "\n",
    "**A. Counter-intuitive discovery:**\n",
    "- We typically think of missing data as a problem to fix\n",
    "- Here, the missingness itself is highly informative\n",
    "- Passengers with known cabins had ~67% survival vs ~30% for unknown\n",
    "\n",
    "**B. Reveals hidden proxy:**\n",
    "- `cabin_known` is a proxy for passenger class and wealth\n",
    "- First-class passengers had assigned cabins, third-class often did not\n",
    "- Having a cabin may also indicate proximity to lifeboats on upper decks\n",
    "\n",
    "**C. Lesson in creative thinking:**\n",
    "- Instead of trying to impute 77% missing cabin data (unreliable)\n",
    "- We extracted signal from the pattern of missingness\n",
    "- Demonstrates that \"how you handle missing data\" can be feature engineering\n",
    "\n",
    "**D. Generalizable principle:**\n",
    "- Look for patterns in missingness across your datasets\n",
    "- Missing data is not always missing at random\n",
    "- The fact that data is recorded/missing can be informative\n",
    "\n",
    "**Alternative answers:** `title` (extracts social status from text), `fare_per_person` (removes confounding), or `family_size` (combines related features) are all excellent answers with strong justification!\n",
    "\n",
    "**The key insight:** Feature engineering requires creativity, domain knowledge, and thinking beyond the obvious transformations. The best features often come from understanding the context and story behind the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus Challenge (Optional)\n",
    "\n",
    "### Create Your Own Feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create a new feature that might be useful for predicting survival\n",
    "# Here are several examples:\n",
    "\n",
    "# Example 1: Wealth indicator (combining class and fare)\n",
    "# Hypothesis: Within each class, higher fare indicates better accommodations (closer to lifeboats)\n",
    "df['wealth_score'] = df['fare_per_person'] / df['pclass']\n",
    "# Higher score = more wealth (high fare, low class number)\n",
    "\n",
    "# Example 2: Young female indicator\n",
    "# Hypothesis: Young women (\"women and children first\") had highest survival\n",
    "df['young_female'] = ((df['sex'] == 'female') & (df['age_group_imputed'] < 40)).astype(int)\n",
    "\n",
    "# Example 3: Deck level (from cabin letter)\n",
    "# Hypothesis: Passengers on higher decks (A, B, C) were closer to lifeboats\n",
    "df['deck_level'] = df['cabin'].str[0]  # First letter of cabin\n",
    "# Map deck letters to numbers (A=1 is highest/best, T=20 is lowest)\n",
    "deck_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
    "df['deck_numeric'] = df['deck_level'].map(deck_mapping)\n",
    "df['deck_numeric'] = df['deck_numeric'].fillna(9)  # Unknown deck gets worst value\n",
    "\n",
    "# Example 4: Ticket sharing (detect group tickets)\n",
    "# Hypothesis: People with shared tickets may have stayed together\n",
    "df['ticket_count'] = df.groupby('ticket')['ticket'].transform('count')\n",
    "df['shared_ticket'] = (df['ticket_count'] > 1).astype(int)\n",
    "\n",
    "print(\"New features created!\")\n",
    "print(\"\\nFeature correlations with survival:\")\n",
    "print(df[['survived', 'wealth_score', 'young_female', 'deck_numeric', 'shared_ticket']].corr()['survived'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze survival by your new feature\n",
    "# Example: Analyze young_female feature\n",
    "print(\"Survival rate by young female status:\")\n",
    "print(df.groupby('young_female')['survived'].agg(['mean', 'count']))\n",
    "\n",
    "fig = px.bar(df.groupby('young_female')['survived'].mean(),\n",
    "             title='Survival Rate: Young Females vs Others',\n",
    "             labels={'value': 'Survival Rate', 'young_female': 'Young Female (1) vs Others (0)'})\n",
    "fig.update_layout(yaxis_tickformat='.0%', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Feature Analysis:**\n",
    "\n",
    "**1. wealth_score (fare_per_person / pclass):**\n",
    "- Combines two indicators of wealth\n",
    "- Higher score indicates more resources and better accommodations\n",
    "- May capture within-class variation (luxury vs. standard first-class)\n",
    "\n",
    "**2. young_female:**\n",
    "- Combines gender and age into single indicator\n",
    "- Captures the \"women and children first\" policy precisely\n",
    "- Shows very high survival rate (~75%) vs others (~25%)\n",
    "\n",
    "**3. deck_numeric:**\n",
    "- Extracts deck information from cabin strings\n",
    "- Higher decks (A, B) were closer to lifeboats\n",
    "- Deals with missing data by assigning lowest value\n",
    "\n",
    "**4. shared_ticket:**\n",
    "- Identifies passengers traveling on group tickets\n",
    "- May indicate families or groups that stayed together\n",
    "- Different from family_size (could be friends, colleagues)\n",
    "\n",
    "**Feature Engineering Process:**\n",
    "1. **Start with hypothesis:** What factors might affect survival?\n",
    "2. **Test correlation:** Does the feature relate to the target?\n",
    "3. **Validate with domain knowledge:** Does it make sense?\n",
    "4. **Compare with existing features:** Does it add new information?\n",
    "5. **Check practical utility:** Is it worth the complexity?\n",
    "\n",
    "**Great feature engineering balances:**\n",
    "- Domain knowledge with data-driven discovery\n",
    "- Complexity with interpretability\n",
    "- Novelty with reliability\n",
    "\n",
    "**Remember:** Not every feature will be useful! The goal is to experiment and test which features actually improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Data Preparation Pipeline\n",
    "\n",
    "### Summary of Our Journey:\n",
    "\n",
    "**1. Raw Data Assessment:**\n",
    "- Identified missing values (age: 20%, cabin: 77%)\n",
    "- Categorized data types (numerical vs categorical)\n",
    "- Understood the knowledge hierarchy (Data â†’ Information â†’ Knowledge â†’ Wisdom)\n",
    "\n",
    "**2. Missing Data Handling:**\n",
    "- Age: Group-based imputation (by class and sex)\n",
    "- Cabin: Created cabin_known binary feature\n",
    "- Learned about MCAR, MAR, and MNAR mechanisms\n",
    "\n",
    "**3. Outlier Detection:**\n",
    "- IQR method: Simple, robust, box plot visualization\n",
    "- Z-score method: Assumes normality, good for univariate analysis\n",
    "- Isolation Forest: Advanced, multivariate, no distribution assumptions\n",
    "- Decided to keep outliers (legitimate expensive tickets)\n",
    "\n",
    "**4. Normalization & Standardization:**\n",
    "- Min-Max scaling: [0, 1] range, preserves relationships\n",
    "- Z-score standardization: Mean=0, std=1, less sensitive to outliers\n",
    "- Prepared data for ML algorithms\n",
    "\n",
    "**5. Categorical Encoding:**\n",
    "- Binary encoding: sex â†’ 0/1\n",
    "- One-hot encoding: embarked â†’ multiple binary columns\n",
    "- Converted text to numbers for ML models\n",
    "\n",
    "**6. Feature Engineering:**\n",
    "- family_size: Combined sibsp + parch + 1\n",
    "- is_alone: Binary indicator for solo travelers\n",
    "- title: Extracted from name (Mr, Mrs, Miss, Master)\n",
    "- age_group: Binned age into categories\n",
    "- fare_per_person: Normalized fare by family size\n",
    "- cabin_known: Binary indicator from missing data pattern\n",
    "\n",
    "**7. Final Dataset:**\n",
    "- 891 rows, 8 features\n",
    "- No missing values\n",
    "- Ready for machine learning!\n",
    "\n",
    "### Key Principles Learned:\n",
    "\n",
    "1. **Understand before transforming:** Always explore data first\n",
    "2. **Context matters:** Domain knowledge guides better decisions\n",
    "3. **Missing data tells a story:** Sometimes absence is information\n",
    "4. **Not all outliers are errors:** Understand before removing\n",
    "5. **Feature engineering is creative:** Best features come from insight\n",
    "6. **Pipeline thinking:** Each step builds on the previous\n",
    "7. **Documentation is crucial:** Explain your choices and reasoning\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "With our clean, engineered dataset, we're ready for:\n",
    "- **Day 6:** Machine Learning (training models on this prepared data)\n",
    "- Building classifiers to predict survival\n",
    "- Evaluating feature importance\n",
    "- Understanding which of our engineered features are most valuable\n",
    "\n",
    "**Great job completing Day 4!** You now have the essential skills for data preparation and feature engineering - often considered the most important (and time-consuming) parts of the machine learning pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "### Documentation:\n",
    "- **Scikit-learn Preprocessing:** https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- **Missing Data Handling:** https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "- **Feature Engineering Guide:** https://www.kaggle.com/learn/feature-engineering\n",
    "\n",
    "### Further Reading:\n",
    "- **Feature Engineering for Machine Learning** by Alice Zheng & Amanda Casari (O'Reilly)\n",
    "- **Isolation Forest Paper:** Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. ICDM.\n",
    "- **Missing Data Mechanisms:** Rubin, D. B. (1976). Inference and missing data. Biometrika.\n",
    "\n",
    "### Practice:\n",
    "- Try this pipeline on other datasets (house prices, customer churn, etc.)\n",
    "- Experiment with different imputation strategies\n",
    "- Create your own engineered features\n",
    "- Compare model performance with/without feature engineering\n",
    "\n",
    "**See you on Day 6 for Machine Learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
